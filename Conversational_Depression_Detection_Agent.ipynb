{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfa30f5-5e63-4f10-aaa9-a942e4fcb296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from typing import List, Dict\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import re\n",
    "import gradio as gr\n",
    "import hashlib\n",
    "import traceback\n",
    "\n",
    "# ==============================\n",
    "# CONFIGURATION\n",
    "# ==============================\n",
    "BASE_MODEL_PATH = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "LORA_ADAPTER_PATH = \"32_V5_Model_V4/checkpoint-450\"  # LoRA checkpoint dir\n",
    "MERGED_MODEL_PATH = \"./qwen3_merged_model_5090\"\n",
    "\n",
    "INITIAL_MESSAGE = \"hi i'm ellie thanks for coming in today i was created to talk to people in a safe and secure environment i'm not a therapist but i'm here to learn about people and would love to learn about you i'll ask a few questions to get us started and please feel free to tell me anything your answers are totally confidential are you ok with this\"\n",
    "\n",
    "class TherapeuticChatbot:\n",
    "    def __init__(self, use_merged: bool = False):\n",
    "        \"\"\"\n",
    "        Load fine-tuned Qwen model (LoRA adapters or merged model).\n",
    "        \"\"\"\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        if self.device == \"cpu\":\n",
    "            print(\"Warning: Running on CPU, which is slow. Use a GPU for better performance.\")\n",
    "\n",
    "        # Always load tokenizer from the base model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        )\n",
    "\n",
    "        # Advanced optimization: Enable Flash Attention 2 if supported\n",
    "        attn_impl = \"flash_attention_2\" if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else \"eager\"\n",
    "\n",
    "        if use_merged:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                MERGED_MODEL_PATH,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\",\n",
    "                attn_implementation=attn_impl\n",
    "            )\n",
    "            print(f\"Loaded merged 4-bit model from {MERGED_MODEL_PATH} with attn_impl={attn_impl}\")\n",
    "        else:\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                BASE_MODEL_PATH,\n",
    "                #quantization_config=bnb_config,\n",
    "                torch_dtype=torch.float16, \n",
    "                device_map=\"auto\",\n",
    "                attn_implementation=attn_impl\n",
    "            )\n",
    "            self.model = PeftModel.from_pretrained(\n",
    "                base_model,\n",
    "                LORA_ADAPTER_PATH,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "            print(f\"Loaded base model {BASE_MODEL_PATH} in 4-bit with LoRA adapters from {LORA_ADAPTER_PATH} and attn_impl={attn_impl}\")\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        # Caching for reused computations\n",
    "        self.cache = {}\n",
    "\n",
    "        self.system_prompts = {\n",
    "            \"Summary\": (\n",
    "                \"You are a cumulative summary AI for therapeutic conversations.\\n\\n\"\n",
    "                \"Task: Generate a concise cumulative summary of the full conversation so far.\\n\\n\"\n",
    "                \"Rules:\\n\"\n",
    "                \"1. Use only direct participant evidence—no inference or assumptions.\\n\"\n",
    "                \"2. Always include the most recent participant response.\\n\"\n",
    "                \"3. Capture:\\n\"\n",
    "                \" - Explicit emotional indicators (with quotes).\\n\"\n",
    "                \" - Depression symptoms mapped to PHQ-8 domains if mentioned.\\n\"\n",
    "                \" - Current participant state/mood based on evidence.\\n\"\n",
    "                \" - Therapist’s assessment approach if evident.\\n\"\n",
    "                \"4. Keep to 2–4 sentences (max 6–8), avoiding repetition.\\n\"\n",
    "                \"5. Exclude casual or irrelevant small talk.\\n\"\n",
    "            ),\n",
    "            \"Classification\": (\n",
    "                \"You are a PHQ-8 classification AI for therapeutic conversations.\\n\"\n",
    "                \"Task: Using the most recent participant response, prior 10 turns, and the cumulative summary, classify PHQ-8 symptom severities.\\n\\n\"\n",
    "                \"Rules:\\n\"\n",
    "                \"1. Use only explicit participant statements as evidence. Ignore therapist text unless quoting the participant.\\n\"\n",
    "                \"2. Be conservative: if no direct evidence exists, assign 'Not explored'.\\n\"\n",
    "                \"3. Allowed severity values: 'Not explored', 'Not at all', 'Several days', 'More than half the days', 'Nearly every day'.\\n\"\n",
    "                \"4. Always provide an evidence mapping (quote or 'no evidence').\\n\"\n",
    "                \"5. Depression classification is based only on mapped severities:\\n\"\n",
    "                \" - If multiple symptoms are rated at higher severities (≥ 'More than half the days'), label as 'Depressed'.\\n\"\n",
    "                \" - Otherwise, 'Not depressed'.\\n\"\n",
    "                \"6. Always incorporate the most recent participant response into classification.\\n\\n\"\n",
    "                \"Output only valid JSON in this exact format:\\n\"\n",
    "                \"{\\n\"\n",
    "                \" \\\"evidence_mapping\\\": {\\\"PHQ1\\\": \\\"evidence or 'no evidence'\\\", ..., \\\"PHQ8\\\": \\\"...\\\"},\\n\"\n",
    "                \" \\\"phq8_scores\\\": {\\\"PHQ1\\\": \\\"severity\\\", ..., \\\"PHQ8\\\": \\\"severity\\\"},\\n\"\n",
    "                \" \\\"depression_classification\\\": \\\"Depressed\\\" or \\\"Not depressed\\\"\\n\"\n",
    "                \"}\"\n",
    "            ),\n",
    "            \"Response\": (\n",
    "                \"You are a therapeutic response generator AI conducting a depression screening interview.\\n\\n\"\n",
    "                \"Task: Based on the last 10 turns, the cumulative summary, and PHQ-8 results, generate a natural therapist reply.\\n\\n\"\n",
    "                \"Guidelines:\\n\"\n",
    "                \"1. Responses must be clinically appropriate, and advance assessment.\\n\"\n",
    "                \"2. Incorporate therapeutic strategy (technique applied), response intent (1-sentence purpose), and emotion tag (tone).\\n\"\n",
    "                \"3. If symptoms are unclear, probe gently. If distress is explicit, respond with validation and support.\\n\\n\"\n",
    "                \"Output only valid JSON in this exact format:\\n\"\n",
    "                \"{\\n\"\n",
    "                \" \\\"strategy_used\\\": \\\"Therapeutic strategy applied\\\",\\n\"\n",
    "                \" \\\"response_intent\\\": \\\"1-sentence purpose\\\",\\n\"\n",
    "                \" \\\"emotion_tag\\\": \\\"Emotional tone\\\",\\n\"\n",
    "                \" \\\"therapist_response\\\": \\\"Response text\\\"\\n\"\n",
    "                \"}\"\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        self.phq8_questions = [\n",
    "            \"Little interest or pleasure in doing things\",\n",
    "            \"Feeling down, depressed, or hopeless\",\n",
    "            \"Trouble falling or staying asleep, or sleeping too much\",\n",
    "            \"Feeling tired or having little energy\",\n",
    "            \"Poor appetite or overeating\",\n",
    "            \"Feeling bad about yourself or that you are a failure\",\n",
    "            \"Trouble concentrating on things\",\n",
    "            \"Moving or speaking slowly or being fidgety/restless\"\n",
    "        ]\n",
    "\n",
    "    def _get_cache_key(self, func_name: str, input_data: str) -> str:\n",
    "        \"\"\"Generate a hash key for caching based on function and input.\"\"\"\n",
    "        key_str = f\"{func_name}:{input_data}\"\n",
    "        return hashlib.sha256(key_str.encode()).hexdigest()\n",
    "\n",
    "    def _generate(self, messages: List[Dict[str, str]], max_new_tokens: int = 512) -> str:\n",
    "        \"\"\"\n",
    "        Generate response using Qwen's chat template with improved error handling.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            input_text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            inputs = self.tokenizer(input_text, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "            # Clear GPU cache before generation\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    temperature=0.2,  # Increased for more varied responses\n",
    "                    top_p=0.8,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,  # Explicitly set pad token\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    repetition_penalty=1.2  # Prevent repetition\n",
    "                )\n",
    "\n",
    "            generated_text = self.tokenizer.decode(\n",
    "                outputs[0][len(inputs[\"input_ids\"][0]):],\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            \n",
    "            # Clear GPU cache after generation\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            return generated_text.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in _generate: {str(e)}\")\n",
    "            print(f\"Traceback: {traceback.format_exc()}\")\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "\n",
    "    def summarize_conversation(self, conversation_history: List[Dict[str, str]]) -> str:\n",
    "        \"\"\"\n",
    "        Generate a cumulative summary of the conversation with caching.\n",
    "        \"\"\"\n",
    "        print(f\"Summarizing conversation with {len(conversation_history)} messages\")\n",
    "        \n",
    "        input_dict = {\"conversation_history\": conversation_history}\n",
    "        input_data = json.dumps(input_dict)\n",
    "        cache_key = self._get_cache_key(\"summarize\", input_data)\n",
    "        \n",
    "        if cache_key in self.cache:\n",
    "            print(\"Using cached summary\")\n",
    "            return self.cache[cache_key]\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompts[\"Summary\"]},\n",
    "            {\"role\": \"user\", \"content\": input_data}\n",
    "        ]\n",
    "        \n",
    "        print(\"Generating new summary...\")\n",
    "        output = self._generate(messages, max_new_tokens=1000)\n",
    "        print(f\"Raw summary output: {output[:300]}...\")\n",
    "        \n",
    "        json_str = self._extract_json_from_output(output)\n",
    "        \n",
    "        try:\n",
    "            parsed = json.loads(json_str)\n",
    "            summary = parsed.get(\"cumulative_summary\", \"No summary available\")\n",
    "        except json.JSONDecodeError:\n",
    "            summary = self._remove_xml_tags(output).strip()\n",
    "        \n",
    "        self.cache[cache_key] = summary\n",
    "        print(f\"Final summary: {summary[:300]}...\")\n",
    "        return summary\n",
    "\n",
    "    def _extract_json_from_output(self, output: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract the JSON string from the output by removing XML tags and finding the JSON object.\n",
    "        \"\"\"\n",
    "        if not output:\n",
    "            return \"{}\"\n",
    "            \n",
    "        cleaned = self._remove_xml_tags(output).strip()\n",
    "        \n",
    "        start_idx = cleaned.find('{')\n",
    "        end_idx = cleaned.rfind('}') + 1\n",
    "        if start_idx != -1 and end_idx != -1 and start_idx < end_idx:\n",
    "            json_str = cleaned[start_idx:end_idx]\n",
    "        else:\n",
    "            json_str = cleaned\n",
    "        \n",
    "        return json_str.strip() if json_str.strip() else \"{}\"\n",
    "\n",
    "    def _remove_xml_tags(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Remove XML-like tags such as <think>, <tool_call>, etc.\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        cleaned = re.sub(r'<[^>]+>', '', text)\n",
    "        cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "        return cleaned\n",
    "\n",
    "    def classify_phq8(self, conversation_history: List[Dict[str, str]], cumulative_summary: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Classify PHQ-8 based on the recent context and cumulative summary with caching.\n",
    "        \"\"\"\n",
    "        print(\"Classifying PHQ-8...\")\n",
    "        \n",
    "        if not cumulative_summary or cumulative_summary.strip() == \"\":\n",
    "            return {\n",
    "                \"raw_output\": \"No summary provided\",\n",
    "                \"phq8_scores\": {f\"PHQ{i+1}\": \"Not explored\" for i in range(8)},\n",
    "                \"depression_classification\": \"Not assessed\",\n",
    "                \"evidence_mapping\": {f\"PHQ{i+1}\": \"no evidence\" for i in range(8)}\n",
    "            }\n",
    "        \n",
    "        # Use recent context (last 10 turns)\n",
    "        recent_context = conversation_history[-10:] if len(conversation_history) > 10 else conversation_history\n",
    "        \n",
    "        input_dict = {\n",
    "            \"recent_context\": recent_context,\n",
    "            \"cumulative_summary\": cumulative_summary\n",
    "        }\n",
    "        input_data = json.dumps(input_dict)\n",
    "        cache_key = self._get_cache_key(\"classify\", input_data)\n",
    "        \n",
    "        if cache_key in self.cache:\n",
    "            print(\"Using cached classification\")\n",
    "            return self.cache[cache_key]\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompts[\"Classification\"]},\n",
    "            {\"role\": \"user\", \"content\": input_data}\n",
    "        ]\n",
    "        \n",
    "        output = self._generate(messages, max_new_tokens=1000)\n",
    "        print(f\"Raw classification output: {output[:300]}...\")\n",
    "        \n",
    "        json_str = self._extract_json_from_output(output)\n",
    "        \n",
    "        try:\n",
    "            parsed = json.loads(json_str)\n",
    "            result = {\n",
    "                \"raw_output\": output,\n",
    "                \"phq8_scores\": parsed.get(\"phq8_scores\", {f\"PHQ{i+1}\": \"Not explored\" for i in range(8)}),\n",
    "                \"depression_classification\": parsed.get(\"depression_classification\", \"Not assessed\"),\n",
    "                \"evidence_mapping\": parsed.get(\"evidence_mapping\", {f\"PHQ{i+1}\": \"no evidence\" for i in range(8)})\n",
    "            }\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON decode error in classification: {e}\")\n",
    "            result = {\n",
    "                \"raw_output\": output,\n",
    "                \"phq8_scores\": {f\"PHQ{i+1}\": \"Not explored\" for i in range(8)},\n",
    "                \"depression_classification\": \"Not assessed\",\n",
    "                \"evidence_mapping\": {f\"PHQ{i+1}\": \"no evidence\" for i in range(8)}\n",
    "            }\n",
    "        \n",
    "        self.cache[cache_key] = result\n",
    "        return result\n",
    "\n",
    "    def generate_response(self, conversation_history: List[Dict[str, str]], cumulative_summary: str, \n",
    "                         classification_results: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Generate an empathetic therapeutic response.\n",
    "        \"\"\"\n",
    "        print(f\"Generating response for conversation with {len(conversation_history)} messages\")\n",
    "        \n",
    "        # Use recent context (last 10 turns to avoid token limits)\n",
    "        recent_context = conversation_history[-10:] if len(conversation_history) > 10 else conversation_history\n",
    "        \n",
    "        input_dict = {\n",
    "            \"recent_context\": recent_context,\n",
    "            \"cumulative_summary\": cumulative_summary,\n",
    "            \"classification_results\": classification_results\n",
    "        }\n",
    "        input_data = json.dumps(input_dict)\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompts[\"Response\"]},\n",
    "            {\"role\": \"user\", \"content\": input_data}\n",
    "        ]\n",
    "        \n",
    "        print(\"Generating therapeutic response...\")\n",
    "        output = self._generate(messages, max_new_tokens=512)\n",
    "        print(f\"Raw response output: {output[:300]}...\")\n",
    "        \n",
    "        # Try to parse as JSON first, fallback to raw output\n",
    "        json_str = self._extract_json_from_output(output)\n",
    "        try:\n",
    "            parsed = json.loads(json_str)\n",
    "            response = parsed.get('therapist_response', output)\n",
    "        except json.JSONDecodeError:\n",
    "            response = output\n",
    "        \n",
    "        # Clean up the response\n",
    "        response = self._remove_xml_tags(response).strip()\n",
    "        \n",
    "        if not response or response == \"\":\n",
    "            response = \"I understand you're sharing something important with me. Can you tell me more about how you're feeling?\"\n",
    "        \n",
    "        print(f\"Final response: {response[:200]}...\")\n",
    "        return response\n",
    "\n",
    "# Load the chatbot model once\n",
    "print(\"Loading therapeutic chatbot...\")\n",
    "chatbot = TherapeuticChatbot(use_merged=False)\n",
    "print(\"Chatbot loaded successfully!\")\n",
    "\n",
    "def respond(message, history, conversation_state, current_summary, current_classification):\n",
    "    try:\n",
    "        print(f\"\\n=== Processing message: {message[:50]}... ===\")\n",
    "        print(f\"Current conversation state length: {len(conversation_state)}\")\n",
    "        \n",
    "        if not message or message.strip() == \"\":\n",
    "            return history, conversation_state, current_summary, current_classification\n",
    "        \n",
    "        # Append user message to conversation state\n",
    "        conversation_state.append({\"speaker_role\": \"participant\", \"text\": message.strip()})\n",
    "        print(f\"Added participant message. New state length: {len(conversation_state)}\")\n",
    "        \n",
    "        # Generate summary\n",
    "        print(\"Step 1: Generating summary...\")\n",
    "        summary = chatbot.summarize_conversation(conversation_state)\n",
    "        \n",
    "        # Generate PHQ-8 classification\n",
    "        print(\"Step 2: Generating classification...\")\n",
    "        phq8_classification = chatbot.classify_phq8(conversation_state, summary)\n",
    "        \n",
    "        # Prepare classification results\n",
    "        classification_results = {\n",
    "            \"phq8_scores\": phq8_classification[\"phq8_scores\"],\n",
    "            \"depression_classification\": phq8_classification[\"depression_classification\"]\n",
    "        }\n",
    "        \n",
    "        # Generate response\n",
    "        print(\"Step 3: Generating therapeutic response...\")\n",
    "        response = chatbot.generate_response(conversation_state, summary, classification_results)\n",
    "        \n",
    "        if not response or response.strip() == \"\":\n",
    "            print(\"no response detected\")\n",
    "        \n",
    "        # Append bot response to conversation state\n",
    "        conversation_state.append({\"speaker_role\": \"therapist\", \"text\": response})\n",
    "        print(f\"Added therapist response. Final state length: {len(conversation_state)}\")\n",
    "        \n",
    "        # Update chat history for display\n",
    "        history.append((message, response))\n",
    "        \n",
    "        # Format displays\n",
    "        display_summary = summary if summary else \"No summary available\"\n",
    "        display_classification = f\"Depression Classification: {phq8_classification['depression_classification']}\\n\\nPHQ-8 Scores:\\n\" + \\\n",
    "                               \"\\n\".join([f\"{k}: {v}\" for k, v in phq8_classification[\"phq8_scores\"].items()])\n",
    "        \n",
    "        print(\"=== Processing completed successfully ===\\n\")\n",
    "        return history, conversation_state, display_summary, display_classification\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error during processing: {str(e)}\\n{traceback.format_exc()}\"\n",
    "        print(error_msg)\n",
    "        \n",
    "        # Provide a fallback response instead of breaking\n",
    "        fallback_response = \"I apologize, but I'm having trouble processing your message right now. Could you please try rephrasing or asking again?\"\n",
    "        history.append((message, fallback_response))\n",
    "        \n",
    "        return history, conversation_state, current_summary, current_classification\n",
    "\n",
    "def clear_chat():\n",
    "    print(\"Clearing chat...\")\n",
    "    return [[None, INITIAL_MESSAGE]], [{\"speaker_role\": \"therapist\", \"text\": INITIAL_MESSAGE}], \"\", \"\"\n",
    "\n",
    "# Create Gradio interface\n",
    "with gr.Blocks(title=\"Therapeutic Chatbot\") as demo:\n",
    "    gr.Markdown(\"# Therapeutic Chatbot\")\n",
    "    gr.Markdown(\"Engage in a therapeutic conversation. The bot will respond empathetically based on your inputs.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3):\n",
    "            chatbot_display = gr.Chatbot(label=\"Conversation\", height=500, value=[[None, INITIAL_MESSAGE]])\n",
    "            with gr.Row():\n",
    "                textbox = gr.Textbox(\n",
    "                    placeholder=\"Type your message here and press Enter...\", \n",
    "                    label=\"Your Message\",\n",
    "                    scale=4\n",
    "                )\n",
    "                send_btn = gr.Button(\"Send\", scale=1)\n",
    "        with gr.Column(scale=1):\n",
    "            summary_display = gr.Textbox(label=\"Cumulative Summary\", lines=10, interactive=False)\n",
    "            classification_display = gr.Textbox(label=\"Classification Results\", lines=10, interactive=False)\n",
    "    \n",
    "    # State management\n",
    "    state = gr.State([{\"speaker_role\": \"therapist\", \"text\": INITIAL_MESSAGE}])\n",
    "    \n",
    "    # Event handlers\n",
    "    def handle_submit(message, history, conversation_state, summary, classification):\n",
    "        # Clear the textbox by returning empty string as first output\n",
    "        result = respond(message, history, conversation_state, summary, classification)\n",
    "        return \"\", result[0], result[1], result[2], result[3]\n",
    "    \n",
    "    # Bind events\n",
    "    textbox.submit(\n",
    "        handle_submit,\n",
    "        inputs=[textbox, chatbot_display, state, summary_display, classification_display],\n",
    "        outputs=[textbox, chatbot_display, state, summary_display, classification_display]\n",
    "    )\n",
    "    \n",
    "    send_btn.click(\n",
    "        handle_submit,\n",
    "        inputs=[textbox, chatbot_display, state, summary_display, classification_display],\n",
    "        outputs=[textbox, chatbot_display, state, summary_display, classification_display]\n",
    "    )\n",
    "    \n",
    "    clear_btn = gr.Button(\"Clear Chat\")\n",
    "    clear_btn.click(\n",
    "        clear_chat, \n",
    "        outputs=[chatbot_display, state, summary_display, classification_display]\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
