{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c56472f-500e-4434-90c1-4b0a8ad9fda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import warnings\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# ============================================================\n",
    "# ENV + SILENCE NOISE\n",
    "# ============================================================\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings(\"ignore\", message=\"torch.utils.checkpoint\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"MatMul8bitLt\")\n",
    "\n",
    "# ============================================================\n",
    "# CUDA/GPU AVAILABILITY CHECK\n",
    "# ============================================================\n",
    "def check_device_setup():\n",
    "    \"\"\"Check CUDA availability and return appropriate device configuration\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DEVICE SETUP CHECK\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check PyTorch CUDA support\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    print(f\"CUDA Available: {cuda_available}\")\n",
    "    \n",
    "    if cuda_available:\n",
    "        try:\n",
    "            device_count = torch.cuda.device_count()\n",
    "            print(f\"CUDA Devices: {device_count}\")\n",
    "            for i in range(device_count):\n",
    "                props = torch.cuda.get_device_properties(i)\n",
    "                print(f\"  Device {i}: {props.name} ({props.total_memory / 1024**3:.1f} GB)\")\n",
    "            \n",
    "            # Test CUDA functionality\n",
    "            test_tensor = torch.randn(2, 2).cuda()\n",
    "            print(f\"CUDA Test: SUCCESS\")\n",
    "            return True, \"auto\"\n",
    "        except Exception as e:\n",
    "            print(f\"CUDA Test Failed: {e}\")\n",
    "            return False, \"cpu\"\n",
    "    else:\n",
    "        print(\"CUDA not available - using CPU mode\")\n",
    "        return False, \"cpu\"\n",
    "\n",
    "# ============================================================\n",
    "# PATHS (edit if your layout differs)\n",
    "# ============================================================\n",
    "train_glob = r\"Train2/**/*.jsonl\"  # your 16k multi-turn set\n",
    "test_glob = r\"Eval2/**/*.jsonl\"    # totally different transcripts\n",
    "cache_dir = \"./cache_qwen3_smallset_v4\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# DEVICE SETUP\n",
    "# ============================================================\n",
    "has_cuda, device_map = check_device_setup()\n",
    "\n",
    "# ============================================================\n",
    "# MODEL + TOKENIZER (FIXED VERSION)\n",
    "# ============================================================\n",
    "model_id = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "print(f\"\\nLoading tokenizer from: {model_id}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Fix for Qwen3 tokenizer configuration\n",
    "# Set PAD to EOS (standard for Qwen to avoid resizing or using UNK)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(f\"Set pad_token to eos_token: {tokenizer.pad_token}\")\n",
    "\n",
    "# Set padding side to left for Flash Attention compatibility\n",
    "tokenizer.padding_side = \"left\"\n",
    "print(\"Set padding_side to 'left' for Flash Attention\")\n",
    "\n",
    "# BOS token is None by design in Qwen3\n",
    "tokenizer.bos_token = None\n",
    "print(\"BOS token set to None (Qwen3 design)\")\n",
    "\n",
    "# Verify token configuration\n",
    "print(f\"EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"BOS token: {tokenizer.bos_token}\")\n",
    "\n",
    "# Function to align model config with tokenizer\n",
    "def fix_model_config_tokens(model, tokenizer):\n",
    "    \"\"\"Align model config with tokenizer to prevent warnings\"\"\"\n",
    "    if hasattr(model.config, 'pad_token_id'):\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    if hasattr(model.config, 'bos_token_id'):\n",
    "        model.config.bos_token_id = None  # Qwen3 doesn't use BOS\n",
    "    if hasattr(model.config, 'eos_token_id'):\n",
    "        model.config.eos_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    # Also fix generation config if it exists\n",
    "    if hasattr(model, 'generation_config'):\n",
    "        if model.generation_config is not None:\n",
    "            model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "            model.generation_config.bos_token_id = None\n",
    "            model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "    print(\"Model config aligned with tokenizer\")\n",
    "\n",
    "# Configure quantization and model loading based on device availability\n",
    "model_kwargs = {\n",
    "    \"device_map\": device_map,\n",
    "    \"low_cpu_mem_usage\": True,\n",
    "}\n",
    "\n",
    "if has_cuda:\n",
    "    print(\"Using CUDA configuration with 8-bit quantization...\")\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        bnb_8bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "    model_kwargs.update({\n",
    "        \"quantization_config\": quant_config,\n",
    "        \"torch_dtype\": torch.float16,\n",
    "        \"attn_implementation\": \"flash_attention_2\",\n",
    "    })\n",
    "else:\n",
    "    print(\"Using CPU configuration (no quantization)...\")\n",
    "    quant_config = None\n",
    "    model_kwargs.update({\n",
    "        \"torch_dtype\": torch.float32,  # Use float32 for CPU\n",
    "        # Remove flash_attention_2 for CPU compatibility\n",
    "    })\n",
    "\n",
    "print(f\"Loading model from: {model_id}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Apply the tokenizer config fix to the model\n",
    "fix_model_config_tokens(model, tokenizer)\n",
    "\n",
    "# No resize needed since PAD = EOS\n",
    "\n",
    "if has_cuda:\n",
    "    model.gradient_checkpointing_enable()\n",
    "    if quant_config is not None:\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# ============================================================\n",
    "# LoRA CONFIG\n",
    "# ============================================================\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.2,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# TRAINING ARGS\n",
    "# ============================================================\n",
    "output_dir = \"./32_V5_Model_V4\"\n",
    "\n",
    "# Adjust training arguments based on device\n",
    "if has_cuda:\n",
    "    print(\"Using GPU-optimized training configuration...\")\n",
    "    training_args = SFTConfig(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=5e-5,\n",
    "        weight_decay=0.1,\n",
    "        warmup_ratio=0.1,\n",
    "        fp16=True,\n",
    "        max_grad_norm=1.0,\n",
    "        logging_steps=25,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=150,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=150,\n",
    "        save_total_limit=7,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        report_to=\"tensorboard\",\n",
    "        seed=42,\n",
    "        max_length=2048,\n",
    "        packing=False,\n",
    "        dataset_text_field=\"text\",\n",
    "        dataloader_num_workers=4,\n",
    "        dataloader_pin_memory=True,\n",
    "        eval_accumulation_steps=2,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        group_by_length=True,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "    )\n",
    "else:\n",
    "    print(\"Using CPU-optimized training configuration...\")\n",
    "    training_args = SFTConfig(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=1,  # Reduce epochs for CPU\n",
    "        per_device_train_batch_size=1,  # Smaller batch size for CPU\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=8,  # Increase to maintain effective batch size\n",
    "        learning_rate=5e-5,  # Slightly lower learning rate\n",
    "        weight_decay=0.05,\n",
    "        warmup_ratio=0.1,\n",
    "        fp16=False,  # Disable FP16 for CPU\n",
    "        max_grad_norm=1.0,\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        save_total_limit=10,\n",
    "        optim=\"adamw_torch\",  # Use standard AdamW for CPU\n",
    "        report_to=\"tensorboard\",\n",
    "        seed=42,\n",
    "        max_length=1024,  # Shorter sequences for CPU\n",
    "        packing=False,\n",
    "        dataset_text_field=\"text\",\n",
    "        dataloader_num_workers=2,  # Fewer workers for CPU\n",
    "        dataloader_pin_memory=False,  # Disable for CPU\n",
    "        eval_accumulation_steps=4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        group_by_length=True,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "    )\n",
    "\n",
    "# ============================================================\n",
    "# THINK-TAG CLEANER\n",
    "# ============================================================\n",
    "def clean_think_tags(text: str) -> str:\n",
    "    cleaned = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL)\n",
    "    cleaned = re.sub(r\"\\n\\s*\\n\", \"\\n\", cleaned).strip()\n",
    "    return cleaned\n",
    "\n",
    "# ============================================================\n",
    "# CHAT TEMPLATE FORMATTER\n",
    "# ============================================================\n",
    "def formatting_func_batched(batch):\n",
    "    texts = []\n",
    "    for messages in batch[\"messages\"]:\n",
    "        try:\n",
    "            text = tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=False,\n",
    "            )\n",
    "            text = clean_think_tags(text)\n",
    "            # Ensure ends with EOS (required for Qwen3 fine-tuning)\n",
    "            if not text.endswith(tokenizer.eos_token):\n",
    "                text += tokenizer.eos_token\n",
    "            texts.append(text.strip())\n",
    "        except Exception as e:\n",
    "            print(f\"Error formatting example: {e}\")\n",
    "            texts.append(\"\")\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# ============================================================\n",
    "# FIXED MODULE CLASSIFICATION HELPER (clean + direct)\n",
    "# ============================================================\n",
    "def classify_module_type(example):\n",
    "    \"\"\"\n",
    "    Classify module type (classification, response, summary) \n",
    "    based only on system prompt signatures or expected JSON keys.\n",
    "    \"\"\"\n",
    "    if not example:\n",
    "        return \"unknown\"\n",
    "\n",
    "    # --- Method 1: Messages with system role ---\n",
    "    if \"messages\" in example and isinstance(example[\"messages\"], list):\n",
    "        for message in example[\"messages\"]:\n",
    "            if message.get(\"role\") == \"system\":\n",
    "                content = message.get(\"content\", \"\")\n",
    "                if \"PHQ-8 classification AI for therapeutic conversations\" in content:\n",
    "                    return \"classification\"\n",
    "                elif \"therapeutic response generator AI conducting a depression screening interview\" in content:\n",
    "                    return \"response\"\n",
    "                elif \"cumulative summary AI for therapeutic conversations\" in content:\n",
    "                    return \"summary\"\n",
    "\n",
    "    # --- Method 2: Direct text content fallback ---\n",
    "    text = example.get(\"text\", \"\")\n",
    "    if text:\n",
    "        if '\"evidence_mapping\"' in text and '\"phq8_scores\"' in text:\n",
    "            return \"classification\"\n",
    "        elif '\"therapist_response\"' in text and '\"strategy_used\"' in text:\n",
    "            return \"response\"\n",
    "        elif '\"cumulative_summary\"' in text:\n",
    "            return \"summary\"\n",
    "\n",
    "    return \"unknown\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# IMPROVED INSTANCE PRINTER\n",
    "# ============================================================\n",
    "def print_dataset_instances(dataset, dataset_name, num_instances=3):\n",
    "    \"\"\"Print sample instances grouped by module type\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"DATASET INSTANCES - {dataset_name.upper()}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Group instances\n",
    "    module_instances = {mt: [] for mt in [\"classification\", \"response\", \"summary\", \"unknown\"]}\n",
    "\n",
    "    for i, example in enumerate(dataset):\n",
    "        try:\n",
    "            mt = classify_module_type(example)\n",
    "            module_instances[mt].append((i, example))\n",
    "        except Exception as e:\n",
    "            print(f\"Error classifying instance {i}: {e}\")\n",
    "            module_instances[\"unknown\"].append((i, example))\n",
    "\n",
    "    # Print examples\n",
    "    for mt in [\"classification\", \"response\", \"summary\"]:\n",
    "        instances = module_instances[mt]\n",
    "        print(f\"\\n{'-' * 60}\")\n",
    "        print(f\"{mt.upper()} MODULE - {len(instances)} total instances\")\n",
    "        print(f\"{'-' * 60}\")\n",
    "\n",
    "        if not instances:\n",
    "            print(f\"No {mt} instances found in {dataset_name}\")\n",
    "            continue\n",
    "\n",
    "        for j, (idx, ex) in enumerate(instances[:num_instances]):\n",
    "            text = ex.get(\"text\", \"\")\n",
    "            print(f\"\\n▶ Instance {j+1} (Index {idx}) - {len(text)} chars\")\n",
    "            print(\"=\" * 40)\n",
    "            preview = text + \"...\" if len(text) > 2000 else text\n",
    "            print(preview)\n",
    "            print(\"=\" * 40)\n",
    "\n",
    "    # Summary\n",
    "    total = len(dataset)\n",
    "    print(f\"\\nTotal instances in {dataset_name}: {total}\")\n",
    "    for mt, inst in module_instances.items():\n",
    "        print(f\"  {mt.capitalize()}: {len(inst)}\")\n",
    "\n",
    "    return module_instances\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# DATA LOADING HELPERS\n",
    "# ============================================================\n",
    "def load_and_format_jsonl(file):\n",
    "    try:\n",
    "        ds = load_dataset(\"json\", data_files=file, split=\"train\")\n",
    "        if len(ds) == 0:\n",
    "            return None\n",
    "        \n",
    "        ds_formatted = ds.map(\n",
    "            formatting_func_batched,\n",
    "            remove_columns=ds.column_names,\n",
    "            batched=True,\n",
    "            batch_size=64,\n",
    "            num_proc=1,\n",
    "        )\n",
    "        ds_formatted = ds_formatted.filter(lambda x: x[\"text\"].strip() != \"\", num_proc=1)\n",
    "        return ds_formatted\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_corpus(glob_pattern, label=\"train\"):\n",
    "    files = glob.glob(glob_pattern, recursive=True)\n",
    "    if not files:\n",
    "        raise ValueError(f\"No JSONL files found for {label}: {glob_pattern}\")\n",
    "    \n",
    "    print(f\"Found {len(files)} {label} files\")\n",
    "    formatted, n = [], 0\n",
    "    \n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=min(4, os.cpu_count())) as ex:\n",
    "        futures = [ex.submit(load_and_format_jsonl, f) for f in files]\n",
    "        for fut in concurrent.futures.as_completed(futures):\n",
    "            ds = fut.result()\n",
    "            if ds is not None:\n",
    "                formatted.append(ds)\n",
    "                n += len(ds)\n",
    "    \n",
    "    if not formatted:\n",
    "        raise RuntimeError(f\"Failed to load any valid {label} data.\")\n",
    "    \n",
    "    merged = concatenate_datasets(formatted)\n",
    "    \n",
    "    # Shuffle the dataset using the dataset's shuffle method\n",
    "    print(f\"Shuffling {label} dataset...\")\n",
    "    merged = merged.shuffle(seed=42)\n",
    "    \n",
    "    print(f\"{label.capitalize()} examples after formatting: {len(merged)}\")\n",
    "    return merged\n",
    "\n",
    "# ============================================================\n",
    "# LOAD TRAIN + EXTERNAL TEST\n",
    "# ============================================================\n",
    "print(\"Loading training corpus…\")\n",
    "train_dataset = load_corpus(train_glob, label=\"train\")\n",
    "\n",
    "print(\"Loading external test corpus…\")\n",
    "eval_dataset = load_corpus(test_glob, label=\"test\")\n",
    "\n",
    "# ============================================================\n",
    "# PRINT DATASET INSTANCES\n",
    "# ============================================================\n",
    "print_dataset_instances(train_dataset, \"TRAINING\", num_instances=3)\n",
    "print_dataset_instances(eval_dataset, \"EVALUATION\", num_instances=3)\n",
    "\n",
    "# ============================================================\n",
    "# MEMORY MONITOR\n",
    "# ============================================================\n",
    "def print_memory_usage():\n",
    "    if has_cuda and torch.cuda.is_available():\n",
    "        try:\n",
    "            a = torch.cuda.memory_allocated(0) / 1024**3\n",
    "            r = torch.cuda.memory_reserved(0) / 1024**3\n",
    "            print(f\"GPU Memory Allocated: {a:.2f} GB | Reserved: {r:.2f} GB\")\n",
    "        except:\n",
    "            print(\"GPU memory info not available\")\n",
    "    else:\n",
    "        # For CPU, we can check system memory if needed\n",
    "        try:\n",
    "            import psutil\n",
    "            mem = psutil.virtual_memory()\n",
    "            print(f\"System Memory Usage: {mem.percent:.1f}% ({mem.used / 1024**3:.1f} GB / {mem.total / 1024**3:.1f} GB)\")\n",
    "        except ImportError:\n",
    "            print(\"psutil not available for memory monitoring\")\n",
    "\n",
    "print_memory_usage()\n",
    "\n",
    "# ============================================================\n",
    "# TRAINER SETUP\n",
    "# ============================================================\n",
    "print(\"Initializing SFTTrainer...\")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=lora_config,\n",
    "    processing_class=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3 if has_cuda else 2)],\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# AUTO-RESUME FROM LAST CHECKPOINT\n",
    "# ============================================================\n",
    "latest_checkpoint = None\n",
    "if os.path.isdir(output_dir):\n",
    "    checkpoints = [\n",
    "        os.path.join(output_dir, d) for d in os.listdir(output_dir)\n",
    "        if d.startswith(\"checkpoint-\") and os.path.isdir(os.path.join(output_dir, d))\n",
    "    ]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(checkpoints, key=lambda x: int(x.split(\"-\")[-1]))\n",
    "        print(f\"Resuming training from: {latest_checkpoint}\")\n",
    "\n",
    "print_memory_usage()\n",
    "print(\"Starting fine-tuning…\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    trainer.train(resume_from_checkpoint=latest_checkpoint)\n",
    "    print(\"Training completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Training failed with error: {e}\")\n",
    "    raise\n",
    "\n",
    "# Save best adapter + tokenizer\n",
    "final_dir = \"./qwen3_lora_finetuned_final_32_V5_Fourth\"\n",
    "trainer.save_model(final_dir)\n",
    "tokenizer.save_pretrained(final_dir)\n",
    "print(f\"Fine-tuning complete! Model saved to: {final_dir}\")\n",
    "\n",
    "# ============================================================\n",
    "# EVALUATE ON EXTERNAL TEST\n",
    "# ============================================================\n",
    "print(\"Evaluating on external test set…\")\n",
    "try:\n",
    "    eval_results = trainer.evaluate()\n",
    "    perplexity = float(np.exp(eval_results[\"eval_loss\"]))\n",
    "    print(f\"Evaluation Loss: {eval_results['eval_loss']:.4f}\")\n",
    "    print(f\"Perplexity: {perplexity:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Evaluation failed: {e}\")\n",
    "\n",
    "print_memory_usage()\n",
    "\n",
    "# ============================================================\n",
    "# MERGE ADAPTER INTO FULL MODEL (optional)\n",
    "# ============================================================\n",
    "if has_cuda:  # Only try merging on GPU\n",
    "    print(\"Attempting to merge LoRA adapter into full model...\")\n",
    "    try:\n",
    "        from peft import AutoPeftModelForCausalLM\n",
    "        merged = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            final_dir,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "        merged = merged.merge_and_unload()\n",
    "        \n",
    "        merged_dir = \"./qwen3_merged_model_final_v4\"\n",
    "        merged.save_pretrained(merged_dir)\n",
    "        tokenizer.save_pretrained(merged_dir)\n",
    "        print(f\"Merged full model saved to: {merged_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Merge failed (adapters still saved). Error: {e}\")\n",
    "else:\n",
    "    print(\"Skipping model merge on CPU (adapters saved separately)\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d8b039-db5f-452c-b8c4-4d3a864e72b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import warnings\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# ============================================================\n",
    "# ENV + SILENCE NOISE\n",
    "# ============================================================\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings(\"ignore\", message=\"torch.utils.checkpoint\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"MatMul8bitLt\")\n",
    "\n",
    "# ============================================================\n",
    "# CUDA/GPU AVAILABILITY CHECK\n",
    "# ============================================================\n",
    "def check_device_setup():\n",
    "    \"\"\"Check CUDA availability and return appropriate device configuration\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DEVICE SETUP CHECK\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check PyTorch CUDA support\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    print(f\"CUDA Available: {cuda_available}\")\n",
    "    \n",
    "    if cuda_available:\n",
    "        try:\n",
    "            device_count = torch.cuda.device_count()\n",
    "            print(f\"CUDA Devices: {device_count}\")\n",
    "            for i in range(device_count):\n",
    "                props = torch.cuda.get_device_properties(i)\n",
    "                print(f\"  Device {i}: {props.name} ({props.total_memory / 1024**3:.1f} GB)\")\n",
    "            \n",
    "            # Test CUDA functionality\n",
    "            test_tensor = torch.randn(2, 2).cuda()\n",
    "            print(f\"CUDA Test: SUCCESS\")\n",
    "            return True, \"auto\"\n",
    "        except Exception as e:\n",
    "            print(f\"CUDA Test Failed: {e}\")\n",
    "            return False, \"cpu\"\n",
    "    else:\n",
    "        print(\"CUDA not available - using CPU mode\")\n",
    "        return False, \"cpu\"\n",
    "\n",
    "# ============================================================\n",
    "# PATHS (edit if your layout differs)\n",
    "# ============================================================\n",
    "train_glob = r\"Train4/**/*.jsonl\"  # your 16k multi-turn set\n",
    "test_glob = r\"Eval4/**/*.jsonl\"    # totally different transcripts\n",
    "cache_dir = \"./cache_qwen3_smallset_v4\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# DEVICE SETUP\n",
    "# ============================================================\n",
    "has_cuda, device_map = check_device_setup()\n",
    "\n",
    "# ============================================================\n",
    "# MODEL + TOKENIZER (FIXED VERSION)\n",
    "# ============================================================\n",
    "model_id = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "print(f\"\\nLoading tokenizer from: {model_id}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# Configure quantization and model loading based on device availability\n",
    "model_kwargs = {\n",
    "    \"device_map\": device_map,\n",
    "    \"low_cpu_mem_usage\": True,\n",
    "}\n",
    "\n",
    "if has_cuda:\n",
    "    print(\"Using CUDA configuration with 8-bit quantization...\")\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        bnb_8bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "    model_kwargs.update({\n",
    "        \"quantization_config\": quant_config,\n",
    "        \"torch_dtype\": torch.float16,\n",
    "        \"attn_implementation\": \"flash_attention_2\",\n",
    "    })\n",
    "else:\n",
    "    print(\"Using CPU configuration (no quantization)...\")\n",
    "    quant_config = None\n",
    "    model_kwargs.update({\n",
    "        \"torch_dtype\": torch.float32,  # Use float32 for CPU\n",
    "        # Remove flash_attention_2 for CPU compatibility\n",
    "    })\n",
    "\n",
    "print(f\"Loading model from: {model_id}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)\n",
    "model.config.use_cache = False\n",
    "\n",
    "# No resize needed since PAD = EOS\n",
    "\n",
    "if has_cuda:\n",
    "    model.gradient_checkpointing_enable()\n",
    "    if quant_config is not None:\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# ============================================================\n",
    "# LoRA CONFIG\n",
    "# ============================================================\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.2,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# TRAINING ARGS\n",
    "# ============================================================\n",
    "output_dir = \"./32_V5_Model_V4\"\n",
    "\n",
    "# Adjust training arguments based on device\n",
    "if has_cuda:\n",
    "    print(\"Using GPU-optimized training configuration...\")\n",
    "    training_args = SFTConfig(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=5e-5,\n",
    "        weight_decay=0.15,\n",
    "        warmup_ratio=0.1,\n",
    "        fp16=True,\n",
    "        max_grad_norm=1.0,\n",
    "        logging_steps=25,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=150,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=150,\n",
    "        save_total_limit=7,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        report_to=\"tensorboard\",\n",
    "        seed=42,\n",
    "        max_length=2048,\n",
    "        packing=False,\n",
    "        dataset_text_field=\"text\",\n",
    "        dataloader_num_workers=4,\n",
    "        dataloader_pin_memory=True,\n",
    "        eval_accumulation_steps=2,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        group_by_length=True,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "    )\n",
    "else:\n",
    "    print(\"Using CPU-optimized training configuration...\")\n",
    "    training_args = SFTConfig(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=1,  # Reduce epochs for CPU\n",
    "        per_device_train_batch_size=1,  # Smaller batch size for CPU\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=8,  # Increase to maintain effective batch size\n",
    "        learning_rate=5e-5,  # Slightly lower learning rate\n",
    "        weight_decay=0.05,\n",
    "        warmup_ratio=0.1,\n",
    "        fp16=False,  # Disable FP16 for CPU\n",
    "        max_grad_norm=1.0,\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        save_total_limit=10,\n",
    "        optim=\"adamw_torch\",  # Use standard AdamW for CPU\n",
    "        report_to=\"tensorboard\",\n",
    "        seed=42,\n",
    "        max_length=1024,  # Shorter sequences for CPU\n",
    "        packing=False,\n",
    "        dataset_text_field=\"text\",\n",
    "        dataloader_num_workers=2,  # Fewer workers for CPU\n",
    "        dataloader_pin_memory=False,  # Disable for CPU\n",
    "        eval_accumulation_steps=4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        group_by_length=True,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "    )\n",
    "\n",
    "# ============================================================\n",
    "# THINK-TAG CLEANER\n",
    "# ============================================================\n",
    "def clean_think_tags(text: str) -> str:\n",
    "    cleaned = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL)\n",
    "    cleaned = re.sub(r\"\\n\\s*\\n\", \"\\n\", cleaned).strip()\n",
    "    return cleaned\n",
    "\n",
    "# ============================================================\n",
    "# CHAT TEMPLATE FORMATTER\n",
    "# ============================================================\n",
    "def formatting_func_batched(batch):\n",
    "    texts = []\n",
    "    for messages in batch[\"messages\"]:\n",
    "        try:\n",
    "            text = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False,\n",
    "                enable_thinking=False,  # This should prevent think tags, but we'll clean them anyway\n",
    "            )\n",
    "            if text.endswith(tokenizer.eos_token):\n",
    "                text = text[:-len(tokenizer.eos_token)]\n",
    "            \n",
    "            # CLEAN THINK TAGS - This is the key addition\n",
    "            text = clean_think_tags(text)\n",
    "            \n",
    "            texts.append(text.strip())\n",
    "        except Exception as e:\n",
    "            print(f\"Error formatting example: {e}\")\n",
    "            texts.append(\"\")\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# ============================================================\n",
    "# FIXED MODULE CLASSIFICATION HELPER (clean + direct)\n",
    "# ============================================================\n",
    "def classify_module_type(example):\n",
    "    \"\"\"\n",
    "    Classify module type (classification, response, summary) \n",
    "    based only on system prompt signatures or expected JSON keys.\n",
    "    \"\"\"\n",
    "    if not example:\n",
    "        return \"unknown\"\n",
    "\n",
    "    # --- Method 1: Messages with system role ---\n",
    "    if \"messages\" in example and isinstance(example[\"messages\"], list):\n",
    "        for message in example[\"messages\"]:\n",
    "            if message.get(\"role\") == \"system\":\n",
    "                content = message.get(\"content\", \"\")\n",
    "                if \"PHQ-8 classification AI for therapeutic conversations\" in content:\n",
    "                    return \"classification\"\n",
    "                elif \"therapeutic response generator AI conducting a depression screening interview\" in content:\n",
    "                    return \"response\"\n",
    "                elif \"cumulative summary AI for therapeutic conversations\" in content:\n",
    "                    return \"summary\"\n",
    "\n",
    "    # --- Method 2: Direct text content fallback ---\n",
    "    text = example.get(\"text\", \"\")\n",
    "    if text:\n",
    "        if '\"evidence_mapping\"' in text and '\"phq8_scores\"' in text:\n",
    "            return \"classification\"\n",
    "        elif '\"therapist_response\"' in text and '\"strategy_used\"' in text:\n",
    "            return \"response\"\n",
    "        elif '\"cumulative_summary\"' in text:\n",
    "            return \"summary\"\n",
    "\n",
    "    return \"unknown\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# IMPROVED INSTANCE PRINTER\n",
    "# ============================================================\n",
    "def print_dataset_instances(dataset, dataset_name, num_instances=3):\n",
    "    \"\"\"Print sample instances grouped by module type\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"DATASET INSTANCES - {dataset_name.upper()}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Group instances\n",
    "    module_instances = {mt: [] for mt in [\"classification\", \"response\", \"summary\", \"unknown\"]}\n",
    "\n",
    "    for i, example in enumerate(dataset):\n",
    "        try:\n",
    "            mt = classify_module_type(example)\n",
    "            module_instances[mt].append((i, example))\n",
    "        except Exception as e:\n",
    "            print(f\"Error classifying instance {i}: {e}\")\n",
    "            module_instances[\"unknown\"].append((i, example))\n",
    "\n",
    "    # Print examples\n",
    "    for mt in [\"classification\", \"response\", \"summary\"]:\n",
    "        instances = module_instances[mt]\n",
    "        print(f\"\\n{'-' * 60}\")\n",
    "        print(f\"{mt.upper()} MODULE - {len(instances)} total instances\")\n",
    "        print(f\"{'-' * 60}\")\n",
    "\n",
    "        if not instances:\n",
    "            print(f\"No {mt} instances found in {dataset_name}\")\n",
    "            continue\n",
    "\n",
    "        for j, (idx, ex) in enumerate(instances[:num_instances]):\n",
    "            text = ex.get(\"text\", \"\")\n",
    "            print(f\"\\n▶ Instance {j+1} (Index {idx}) - {len(text)} chars\")\n",
    "            print(\"=\" * 40)\n",
    "            preview = text + \"...\" if len(text) > 2000 else text\n",
    "            print(preview)\n",
    "            print(\"=\" * 40)\n",
    "\n",
    "    # Summary\n",
    "    total = len(dataset)\n",
    "    print(f\"\\nTotal instances in {dataset_name}: {total}\")\n",
    "    for mt, inst in module_instances.items():\n",
    "        print(f\"  {mt.capitalize()}: {len(inst)}\")\n",
    "\n",
    "    return module_instances\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# DATA LOADING HELPERS\n",
    "# ============================================================\n",
    "def load_and_format_jsonl(file):\n",
    "    try:\n",
    "        ds = load_dataset(\"json\", data_files=file, split=\"train\")\n",
    "        if len(ds) == 0:\n",
    "            return None\n",
    "        \n",
    "        ds_formatted = ds.map(\n",
    "            formatting_func_batched,\n",
    "            remove_columns=ds.column_names,\n",
    "            batched=True,\n",
    "            batch_size=64,\n",
    "            num_proc=1,\n",
    "        )\n",
    "        ds_formatted = ds_formatted.filter(lambda x: x[\"text\"].strip() != \"\", num_proc=1)\n",
    "        return ds_formatted\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_corpus(glob_pattern, label=\"train\"):\n",
    "    files = glob.glob(glob_pattern, recursive=True)\n",
    "    if not files:\n",
    "        raise ValueError(f\"No JSONL files found for {label}: {glob_pattern}\")\n",
    "    print(f\"Found {len(files)} {label} files\")\n",
    "    formatted, n = [], 0\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=min(4, os.cpu_count())) as ex:\n",
    "        futures = [ex.submit(load_and_format_jsonl, f) for f in files]\n",
    "        for fut in concurrent.futures.as_completed(futures):\n",
    "            ds = fut.result()\n",
    "            if ds is not None:\n",
    "                formatted.append(ds)\n",
    "                n += len(ds)\n",
    "    if not formatted:\n",
    "        raise RuntimeError(f\"Failed to load any valid {label} data.\")\n",
    "    merged = concatenate_datasets(formatted)\n",
    "    print(f\"{label.capitalize()} examples after formatting: {len(merged)}\")\n",
    "    return merged\n",
    "\n",
    "# ============================================================\n",
    "# LOAD TRAIN + EXTERNAL TEST\n",
    "# ============================================================\n",
    "print(\"Loading training corpus…\")\n",
    "train_dataset = load_corpus(train_glob, label=\"train\")\n",
    "\n",
    "print(\"Loading external test corpus…\")\n",
    "eval_dataset = load_corpus(test_glob, label=\"test\")\n",
    "\n",
    "# ============================================================\n",
    "# PRINT DATASET INSTANCES\n",
    "# ============================================================\n",
    "print_dataset_instances(train_dataset, \"TRAINING\", num_instances=1)\n",
    "print_dataset_instances(eval_dataset, \"EVALUATION\", num_instances=1)\n",
    "\n",
    "# ============================================================\n",
    "# MEMORY MONITOR\n",
    "# ============================================================\n",
    "def print_memory_usage():\n",
    "    if has_cuda and torch.cuda.is_available():\n",
    "        try:\n",
    "            a = torch.cuda.memory_allocated(0) / 1024**3\n",
    "            r = torch.cuda.memory_reserved(0) / 1024**3\n",
    "            print(f\"GPU Memory Allocated: {a:.2f} GB | Reserved: {r:.2f} GB\")\n",
    "        except:\n",
    "            print(\"GPU memory info not available\")\n",
    "    else:\n",
    "        # For CPU, we can check system memory if needed\n",
    "        try:\n",
    "            import psutil\n",
    "            mem = psutil.virtual_memory()\n",
    "            print(f\"System Memory Usage: {mem.percent:.1f}% ({mem.used / 1024**3:.1f} GB / {mem.total / 1024**3:.1f} GB)\")\n",
    "        except ImportError:\n",
    "            print(\"psutil not available for memory monitoring\")\n",
    "\n",
    "print_memory_usage()\n",
    "\n",
    "# ============================================================\n",
    "# TRAINER SETUP\n",
    "# ============================================================\n",
    "print(\"Initializing SFTTrainer...\")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=lora_config,\n",
    "    processing_class=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3 if has_cuda else 2)],\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# AUTO-RESUME FROM LAST CHECKPOINT\n",
    "# ============================================================\n",
    "latest_checkpoint = None\n",
    "if os.path.isdir(output_dir):\n",
    "    checkpoints = [\n",
    "        os.path.join(output_dir, d) for d in os.listdir(output_dir)\n",
    "        if d.startswith(\"checkpoint-\") and os.path.isdir(os.path.join(output_dir, d))\n",
    "    ]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(checkpoints, key=lambda x: int(x.split(\"-\")[-1]))\n",
    "        print(f\"Resuming training from: {latest_checkpoint}\")\n",
    "\n",
    "print_memory_usage()\n",
    "print(\"Starting fine-tuning…\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    trainer.train(resume_from_checkpoint=latest_checkpoint)\n",
    "    print(\"Training completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Training failed with error: {e}\")\n",
    "    raise\n",
    "\n",
    "# Save best adapter + tokenizer\n",
    "final_dir = \"./qwen3_lora_finetuned_final_32_V5_Fourth\"\n",
    "trainer.save_model(final_dir)\n",
    "tokenizer.save_pretrained(final_dir)\n",
    "print(f\"Fine-tuning complete! Model saved to: {final_dir}\")\n",
    "\n",
    "# ============================================================\n",
    "# EVALUATE ON EXTERNAL TEST\n",
    "# ============================================================\n",
    "print(\"Evaluating on external test set…\")\n",
    "try:\n",
    "    eval_results = trainer.evaluate()\n",
    "    perplexity = float(np.exp(eval_results[\"eval_loss\"]))\n",
    "    print(f\"Evaluation Loss: {eval_results['eval_loss']:.4f}\")\n",
    "    print(f\"Perplexity: {perplexity:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Evaluation failed: {e}\")\n",
    "\n",
    "print_memory_usage()\n",
    "\n",
    "# ============================================================\n",
    "# MERGE ADAPTER INTO FULL MODEL (optional)\n",
    "# ============================================================\n",
    "if has_cuda:  # Only try merging on GPU\n",
    "    print(\"Attempting to merge LoRA adapter into full model...\")\n",
    "    try:\n",
    "        from peft import AutoPeftModelForCausalLM\n",
    "        merged = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            final_dir,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "        merged = merged.merge_and_unload()\n",
    "        \n",
    "        merged_dir = \"./qwen3_merged_model_final_v5\"\n",
    "        merged.save_pretrained(merged_dir)\n",
    "        tokenizer.save_pretrained(merged_dir)\n",
    "        print(f\"Merged full model saved to: {merged_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Merge failed (adapters still saved). Error: {e}\")\n",
    "else:\n",
    "    print(\"Skipping model merge on CPU (adapters saved separately)\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
