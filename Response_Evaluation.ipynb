{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7ee8ef-61d4-4a54-9333-2b51316c28af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, BertTokenizer, BertModel\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.util import ngrams\n",
    "from nltk import download\n",
    "from rouge_score import rouge_scorer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =========================\n",
    "# Optional metric libraries\n",
    "# =========================\n",
    "_HAS_BERTSCORE = True\n",
    "_HAS_SENTENCE_TRANSFORMERS = True\n",
    "\n",
    "try:\n",
    "    from bert_score import score as bertscore\n",
    "    _HAS_BERTSCORE = True\n",
    "except Exception as e:\n",
    "    print(\"[Info] bert-score not available; official BERTScore will be skipped.\", e)\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer, util as sbert_util\n",
    "    _HAS_SENTENCE_TRANSFORMERS = True\n",
    "except Exception as e:\n",
    "    print(\"[Info] sentence-transformers not available; SBERT Similarity will be skipped.\", e)\n",
    "\n",
    "# ------------------------------------\n",
    "# Speed/parallelism friendly settings\n",
    "# ------------------------------------\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# Download NLTK data if needed (may require adjustment if no internet)\n",
    "download('wordnet', quiet=True)\n",
    "download('omw-1.4', quiet=True)  # For multilingual, but assuming English\n",
    "\n",
    "# Set up paths (adjust if needed)\n",
    "model_id = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "final_dir = \"checkpoint-400\"\n",
    "test_glob = r\"Eval/**/*.jsonl\"\n",
    "results_file = \"metrics_results.json\"\n",
    "plot_file = \"metrics_plot.png\"\n",
    "\n",
    "# -------------------------\n",
    "# Device & batch utilities\n",
    "# -------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# You can override with env var, otherwise pick a sensible default\n",
    "DEFAULT_BATCH_SIZE = int(os.environ.get(\"BATCH_SIZE\", 24 if DEVICE == \"cuda\" else 16))\n",
    "\n",
    "print(f\"[Info] Using device: {DEVICE}, default batch size: {DEFAULT_BATCH_SIZE}\")\n",
    "\n",
    "# ============================================================\n",
    "# FIXED TOKENIZER LOADING (MATCHING TRAINING CODE EXACTLY)\n",
    "# ============================================================\n",
    "print(f\"Loading tokenizer from: {final_dir}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(final_dir)\n",
    "\n",
    "# Apply the EXACT same tokenizer configuration as training\n",
    "# Training used: tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(f\"Set pad_token to eos_token: {tokenizer.pad_token}\")\n",
    "\n",
    "# Set padding side to left (matches training)\n",
    "tokenizer.padding_side = \"left\"\n",
    "print(\"Set padding_side to 'left' (matches training)\")\n",
    "\n",
    "# BOS token is None by design in Qwen3 (matches training)\n",
    "tokenizer.bos_token = None\n",
    "print(\"BOS token set to None (Qwen3 design)\")\n",
    "\n",
    "# Verify token configuration matches training\n",
    "print(f\"EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"BOS token: {tokenizer.bos_token}\")\n",
    "\n",
    "# Verify PAD and EOS are the same (critical for consistency with training)\n",
    "assert tokenizer.pad_token_id == tokenizer.eos_token_id, \"PAD and EOS token IDs must match training setup\"\n",
    "print(\"âœ“ Tokenizer configuration verified: PAD = EOS\")\n",
    "\n",
    "# ============================================================\n",
    "# FIXED MODEL LOADING WITH PROPER CONFIG ALIGNMENT\n",
    "# ============================================================\n",
    "print(f\"Loading model from: {final_dir}\")\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    final_dir,\n",
    "    device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "if DEVICE != \"cuda\":\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "# Function to align model config with tokenizer (simplified to match training)\n",
    "def fix_model_config_tokens(model, tokenizer):\n",
    "    \"\"\"Align model config with tokenizer exactly as in training\"\"\"\n",
    "    # Set model config to match tokenizer\n",
    "    if hasattr(model.config, 'pad_token_id'):\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    if hasattr(model.config, 'bos_token_id'):\n",
    "        model.config.bos_token_id = None  # Qwen3 doesn't use BOS\n",
    "    if hasattr(model.config, 'eos_token_id'):\n",
    "        model.config.eos_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    # Also fix generation config if it exists\n",
    "    if hasattr(model, 'generation_config'):\n",
    "        if model.generation_config is not None:\n",
    "            model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "            model.generation_config.bos_token_id = None\n",
    "            model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    print(\"Model config aligned with tokenizer (matches training)\")\n",
    "\n",
    "# Apply the tokenizer config fix to the model\n",
    "fix_model_config_tokens(model, tokenizer)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Load BERT for custom BERTScore (mean-pooled cosine)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased').to(DEVICE).eval()\n",
    "\n",
    "# Optionally load SBERT\n",
    "sbert_model = None\n",
    "if _HAS_SENTENCE_TRANSFORMERS:\n",
    "    # strong general-purpose sentence model\n",
    "    sbert_name = os.environ.get(\"SBERT_MODEL\", \"sentence-transformers/all-mpnet-base-v2\")\n",
    "    try:\n",
    "        sbert_model = SentenceTransformer(sbert_name, device=DEVICE)\n",
    "        print(f\"[Info] Loaded SBERT model: {sbert_name}\")\n",
    "    except Exception as e:\n",
    "        print(\"[Info] Could not load SBERT model; SBERT Similarity will be skipped.\", e)\n",
    "        _HAS_SENTENCE_TRANSFORMERS = False\n",
    "\n",
    "# Load test examples\n",
    "test_files = glob.glob(test_glob, recursive=True)\n",
    "examples = []\n",
    "for file in test_files:\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            examples.append(data[\"messages\"])\n",
    "\n",
    "# Function to determine module type\n",
    "def get_module_type(system_content):\n",
    "    lower_content = system_content.lower()\n",
    "    if \"therapeutic response generator\" in lower_content:\n",
    "        return \"response\"\n",
    "    elif \"cumulative summary ai\" in lower_content:\n",
    "        return \"summary\"\n",
    "    elif \"phq-8 classification ai\" in lower_content:\n",
    "        return \"classify\"\n",
    "    return \"unknown\"\n",
    "\n",
    "# Filter examples\n",
    "response_examples = [ex for ex in examples if get_module_type(ex[0][\"content\"]) == \"response\"]\n",
    "summary_examples = [ex for ex in examples if get_module_type(ex[0][\"content\"]) == \"summary\"]\n",
    "classify_examples = [ex for ex in examples if get_module_type(ex[0][\"content\"]) == \"classify\"]\n",
    "\n",
    "# -----------------------------------\n",
    "# Batched generation (with fallback)\n",
    "# -----------------------------------\n",
    "def build_prompt(messages):\n",
    "    return tokenizer.apply_chat_template(messages[:-1], tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "def generate_outputs(examples_list, batch_size=DEFAULT_BATCH_SIZE, max_new_tokens=512, min_batch_size=1, force_single_batch=False):\n",
    "    \"\"\"\n",
    "    Batched generation with tokenizer settings matching training exactly.\n",
    "    \"\"\"\n",
    "    if force_single_batch:\n",
    "        batch_size = 1\n",
    "        print(\"[Info] Force single batch mode enabled.\")\n",
    "    \n",
    "    if batch_size < min_batch_size:\n",
    "        batch_size = min_batch_size\n",
    "\n",
    "    outputs_all = []\n",
    "    try:\n",
    "        prompts = [build_prompt(msgs) for msgs in examples_list]\n",
    "        \n",
    "        if DEVICE == \"cuda\":\n",
    "            print(f\"[Mem] Pre-batch max allocated: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
    "        \n",
    "        for i in range(0, len(prompts), batch_size):\n",
    "            batch_prompts = prompts[i:i+batch_size]\n",
    "            inputs = tokenizer(\n",
    "                batch_prompts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=tokenizer.model_max_length if hasattr(tokenizer, \"model_max_length\") else 2048\n",
    "            ).to(model.device if DEVICE == \"cuda\" else DEVICE)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                gen_out = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    do_sample=False,\n",
    "                    temperature=0.0,\n",
    "                    top_p=1.0,\n",
    "                    # Use the same token IDs as training\n",
    "                    pad_token_id=tokenizer.pad_token_id,  # This is now EOS token ID\n",
    "                    eos_token_id=tokenizer.eos_token_id,  # Same as pad_token_id\n",
    "                    bos_token_id=None,  # Qwen3 doesn't use BOS\n",
    "                )\n",
    "\n",
    "            input_length = inputs[\"input_ids\"].shape[1]\n",
    "            for j, out_ids in enumerate(gen_out):\n",
    "                text = tokenizer.decode(out_ids[input_length:], skip_special_tokens=True).strip()\n",
    "                outputs_all.append(text)\n",
    "        \n",
    "        if DEVICE == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "            print(f\"[Mem] Post-batch max allocated: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
    "        \n",
    "        return outputs_all\n",
    "\n",
    "    except Exception as e:\n",
    "        if DEVICE == \"cuda\" and \"out of memory\" in str(e).lower():\n",
    "            print(f\"[Warn] Batch size {batch_size} failed (likely OOM), clearing cache and retrying smaller batch.\", e)\n",
    "            if DEVICE == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "            new_batch = max(batch_size // 2, min_batch_size)\n",
    "            if new_batch < batch_size and not force_single_batch:\n",
    "                return generate_outputs(examples_list, batch_size=new_batch, max_new_tokens=max_new_tokens, min_batch_size=min_batch_size)\n",
    "        else:\n",
    "            print(\"[Warn] Batched generation failed, falling back to single-example generation.\", e)\n",
    "        \n",
    "        # Fallback with consistent token settings\n",
    "        def generate_output(messages):\n",
    "            prompt = tokenizer.apply_chat_template(messages[:-1], tokenize=False, add_generation_prompt=True)\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device if DEVICE == \"cuda\" else DEVICE)\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=512,\n",
    "                    do_sample=False,\n",
    "                    temperature=0.0,\n",
    "                    top_p=1.0,\n",
    "                    pad_token_id=tokenizer.pad_token_id,  # EOS token ID\n",
    "                    eos_token_id=tokenizer.eos_token_id,  # Same as pad_token_id\n",
    "                    bos_token_id=None,  # Qwen3 doesn't use BOS\n",
    "                )\n",
    "            generated = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n",
    "            return generated\n",
    "        \n",
    "        singles = [generate_output(msgs) for msgs in examples_list]\n",
    "        if DEVICE == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "        return singles\n",
    "        \n",
    "# -------------------------------\n",
    "# Custom BERTScore (yours, kept)\n",
    "# -------------------------------\n",
    "def compute_bertscore(hyps, refs, batch_size=DEFAULT_BATCH_SIZE):\n",
    "    if not hyps:\n",
    "        return 0.0\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(hyps), batch_size):\n",
    "            h_batch = hyps[i:i+batch_size]\n",
    "            r_batch = refs[i:i+batch_size]\n",
    "            h_inputs = bert_tokenizer(h_batch, return_tensors=\"pt\", truncation=True, max_length=512, padding=True).to(DEVICE)\n",
    "            r_inputs = bert_tokenizer(r_batch, return_tensors=\"pt\", truncation=True, max_length=512, padding=True).to(DEVICE)\n",
    "            h_last = bert_model(**h_inputs).last_hidden_state.mean(dim=1)  # [B, H]\n",
    "            r_last = bert_model(**r_inputs).last_hidden_state.mean(dim=1)\n",
    "            batch_scores = F.cosine_similarity(h_last, r_last, dim=1)  # [B]\n",
    "            scores.extend(batch_scores.detach().cpu().tolist())\n",
    "    return float(np.mean(scores)) if scores else 0.0\n",
    "\n",
    "# ---------------------------------\n",
    "# Official BERTScore (new, added)\n",
    "# ---------------------------------\n",
    "def compute_official_bertscore(hyps, refs, lang=\"en\", batch_size=DEFAULT_BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Returns (Precision, Recall, F1). If bert-score is unavailable, returns (None, None, None).\n",
    "    \"\"\"\n",
    "    if not _HAS_BERTSCORE or not hyps:\n",
    "        return None, None, None\n",
    "    try:\n",
    "        P, R, F1 = bertscore(\n",
    "            hyps, refs,\n",
    "            lang=lang,\n",
    "            rescale_with_baseline=True,   # stabilized 0..1ish range\n",
    "            batch_size=batch_size,\n",
    "            # model_type can be set via env if you like:\n",
    "            # model_type=os.environ.get(\"BERTSCORE_MODEL\", None)\n",
    "        )\n",
    "        return float(P.mean()), float(R.mean()), float(F1.mean())\n",
    "    except Exception as e:\n",
    "        print(\"[Info] Official BERTScore failed; skipping.\", e)\n",
    "        return None, None, None\n",
    "\n",
    "# -----------------------------------------\n",
    "# SBERT Similarity (new, added, sentence-level)\n",
    "# -----------------------------------------\n",
    "def compute_sbert_similarity(hyps, refs, batch_size=DEFAULT_BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Mean diagonal cosine similarity of SBERT embeddings (hyp vs ref).\n",
    "    Returns float or None if SBERT unavailable.\n",
    "    \"\"\"\n",
    "    if not _HAS_SENTENCE_TRANSFORMERS or not hyps:\n",
    "        return None\n",
    "    try:\n",
    "        # Encode in batches under-the-hood; SentenceTransformer handles batching\n",
    "        hyp_embs = sbert_model.encode(hyps, convert_to_tensor=True, normalize_embeddings=True, batch_size=batch_size)\n",
    "        ref_embs = sbert_model.encode(refs, convert_to_tensor=True, normalize_embeddings=True, batch_size=batch_size)\n",
    "        sims = sbert_util.cos_sim(hyp_embs, ref_embs).diagonal().detach().cpu().numpy()\n",
    "        return float(np.mean(sims))\n",
    "    except Exception as e:\n",
    "        print(\"[Info] SBERT similarity failed; skipping.\", e)\n",
    "        return None\n",
    "\n",
    "# -------------------\n",
    "# Perplexity function\n",
    "# -------------------\n",
    "def compute_perplexity(texts, batch_size=DEFAULT_BATCH_SIZE):\n",
    "    if not texts:\n",
    "        return 0.0\n",
    "    ppl_vals = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            inputs = tokenizer(batch, return_tensors=\"pt\", truncation=True, max_length=512, padding=True).to(model.device if DEVICE == \"cuda\" else DEVICE)\n",
    "            # Shift labels are handled by transformers when labels=input_ids\n",
    "            loss = model(**inputs, labels=inputs[\"input_ids\"]).loss\n",
    "            ppl = torch.exp(loss).item()\n",
    "            ppl_vals.append(ppl)\n",
    "    return float(np.mean(ppl_vals)) if ppl_vals else 0.0\n",
    "\n",
    "# -------------------------\n",
    "# Function to compute scores\n",
    "# -------------------------\n",
    "def compute_scores(hypotheses, references, module_name, results_dict):\n",
    "    if not hypotheses:\n",
    "        print(f\"No {module_name} module examples found.\")\n",
    "        return results_dict\n",
    "\n",
    "    # BLEU\n",
    "    ref_tokens = [[ref.split()] for ref in references]\n",
    "    hyp_tokens = [hyp.split() for hyp in hypotheses]\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu_score = corpus_bleu(ref_tokens, hyp_tokens, smoothing_function=smoothing)\n",
    "\n",
    "    # ROUGE\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        scores = scorer.score(ref, hyp)\n",
    "        for key in rouge_scores:\n",
    "            rouge_scores[key].append(scores[key].fmeasure)\n",
    "    avg_rouge1 = float(np.mean(rouge_scores['rouge1']))\n",
    "    avg_rouge2 = float(np.mean(rouge_scores['rouge2']))\n",
    "    avg_rougeL = float(np.mean(rouge_scores['rougeL']))\n",
    "\n",
    "    # METEOR\n",
    "    meteor_scores = [meteor_score([ref.split()], hyp.split()) for ref, hyp in zip(references, hypotheses)]\n",
    "    avg_meteor = float(np.mean(meteor_scores)) if meteor_scores else 0.0\n",
    "\n",
    "    # Custom BERTScore (yours)\n",
    "    avg_bertscore_custom = compute_bertscore(hypotheses, references)\n",
    "\n",
    "    # Official BERTScore (new)\n",
    "    bP, bR, bF1 = compute_official_bertscore(hypotheses, references)\n",
    "\n",
    "    # SBERT Similarity (new)\n",
    "    sbert_sim = compute_sbert_similarity(hypotheses, references)\n",
    "\n",
    "    # Perplexity (on references; preserved)\n",
    "    avg_ppl = compute_perplexity(references)\n",
    "\n",
    "    # Length Ratio\n",
    "    len_ratio_vals = [len(h) / len(r) for h, r in zip(hypotheses, references) if len(r) > 0]\n",
    "    len_ratio = float(np.mean(len_ratio_vals)) if len_ratio_vals else 0.0\n",
    "\n",
    "    # Bigram Diversity\n",
    "    all_bigrams = set()\n",
    "    total_bigrams = 0\n",
    "    for hyp in hypotheses:\n",
    "        hyp_words = hyp.split()\n",
    "        hyp_bigrams = list(ngrams(hyp_words, 2))\n",
    "        all_bigrams.update(hyp_bigrams)\n",
    "        total_bigrams += len(hyp_bigrams)\n",
    "    diversity = float(len(all_bigrams) / total_bigrams) if total_bigrams > 0 else 0.0\n",
    "\n",
    "    # Save results\n",
    "    res = {\n",
    "        \"BLEU\": bleu_score,\n",
    "        \"ROUGE-1\": avg_rouge1,\n",
    "        \"ROUGE-2\": avg_rouge2,\n",
    "        \"ROUGE-L\": avg_rougeL,\n",
    "        \"METEOR\": avg_meteor,\n",
    "        \"BERTScore (Custom)\": avg_bertscore_custom,\n",
    "        \"Perplexity\": avg_ppl,\n",
    "        \"Length Ratio\": len_ratio,\n",
    "        \"Bigram Diversity\": diversity\n",
    "    }\n",
    "    # Append new metrics if available\n",
    "    if bP is not None:\n",
    "        res[\"BERTScore-P\"] = bP\n",
    "    if bR is not None:\n",
    "        res[\"BERTScore-R\"] = bR\n",
    "    if bF1 is not None:\n",
    "        res[\"BERTScore-F1\"] = bF1\n",
    "    if sbert_sim is not None:\n",
    "        res[\"SBERT Similarity\"] = sbert_sim\n",
    "\n",
    "    results_dict[module_name] = res\n",
    "\n",
    "    print(f\"{module_name.capitalize()} BLEU: {bleu_score:.4f}\")\n",
    "    print(f\"{module_name.capitalize()} ROUGE-1: {avg_rouge1:.4f}\")\n",
    "    print(f\"{module_name.capitalize()} ROUGE-2: {avg_rouge2:.4f}\")\n",
    "    print(f\"{module_name.capitalize()} ROUGE-L: {avg_rougeL:.4f}\")\n",
    "    print(f\"{module_name.capitalize()} METEOR: {avg_meteor:.4f}\")\n",
    "    print(f\"{module_name.capitalize()} BERTScore (Custom): {avg_bertscore_custom:.4f}\")\n",
    "    if bF1 is not None:\n",
    "        print(f\"{module_name.capitalize()} BERTScore-P (official): {bP:.4f}\")\n",
    "        print(f\"{module_name.capitalize()} BERTScore-R (official): {bR:.4f}\")\n",
    "        print(f\"{module_name.capitalize()} BERTScore-F1 (official): {bF1:.4f}\")\n",
    "    else:\n",
    "        print(f\"{module_name.capitalize()} BERTScore (official): [skipped]\")\n",
    "    if sbert_sim is not None:\n",
    "        print(f\"{module_name.capitalize()} SBERT Similarity: {sbert_sim:.4f}\")\n",
    "    else:\n",
    "        print(f\"{module_name.capitalize()} SBERT Similarity: [skipped]\")\n",
    "    print(f\"{module_name.capitalize()} Perplexity: {avg_ppl:.4f}\")\n",
    "    print(f\"{module_name.capitalize()} Length Ratio: {len_ratio:.4f}\")\n",
    "    print(f\"{module_name.capitalize()} Bigram Diversity: {diversity:.4f}\")\n",
    "\n",
    "    return results_dict\n",
    "\n",
    "# =================\n",
    "# Run evaluations in new order: Response -> Classification -> Summary\n",
    "# =================\n",
    "results = {}\n",
    "\n",
    "# 1. RESPONSE MODULE EVALUATION\n",
    "print(\"Evaluating Response Module...\")\n",
    "response_hypotheses = generate_outputs(response_examples, batch_size=DEFAULT_BATCH_SIZE)\n",
    "response_references = [ex[-1][\"content\"] for ex in response_examples]\n",
    "\n",
    "# Additional evaluation for therapist_response only (preserved)\n",
    "print(\"\\nEvaluating Response Therapist Only...\")\n",
    "response_hyps_therapist = []\n",
    "response_refs_therapist = []\n",
    "for hyp, ref in zip(response_hypotheses, response_references):\n",
    "    try:\n",
    "        hyp_json = json.loads(hyp)\n",
    "        ref_json = json.loads(ref)\n",
    "        response_hyps_therapist.append(hyp_json.get('therapist_response', ''))\n",
    "        response_refs_therapist.append(ref_json.get('therapist_response', ''))\n",
    "    except json.JSONDecodeError:\n",
    "        response_hyps_therapist.append('')\n",
    "        response_refs_therapist.append('')\n",
    "\n",
    "# Print first 3 generated and true therapist responses (preserved)\n",
    "print(\"First 3 generated and true therapist responses:\")\n",
    "for i in range(min(3, len(response_hyps_therapist))):\n",
    "    print(f\"Generated {i+1}: {response_hyps_therapist[i]}\")\n",
    "    print(f\"True {i+1}: {response_refs_therapist[i]}\")\n",
    "    print(\"---\")\n",
    "\n",
    "results = compute_scores(response_hyps_therapist, response_refs_therapist, \"response_therapist\", results)\n",
    "print(results)\n",
    "\n",
    "# Original full response evaluation (preserved)\n",
    "print(\"\\nEvaluating Response Full...\")\n",
    "results = compute_scores(response_hypotheses, response_references, \"response\", results)\n",
    "\n",
    "# 2. CLASSIFICATION MODULE EVALUATION\n",
    "print(\"\\nEvaluating Classify Module...\")\n",
    "classify_hypotheses = generate_outputs(classify_examples, batch_size=DEFAULT_BATCH_SIZE)\n",
    "classify_references = [ex[-1][\"content\"] for ex in classify_examples]\n",
    "results = compute_scores(classify_hypotheses, classify_references, \"classify\", results)\n",
    "\n",
    "# 3. SUMMARY MODULE EVALUATION (with single batch fallback)\n",
    "print(\"\\nEvaluating Summary Module...\")\n",
    "summary_hypotheses = generate_outputs(summary_examples, batch_size=DEFAULT_BATCH_SIZE, force_single_batch=True)\n",
    "summary_references = [ex[-1][\"content\"] for ex in summary_examples]\n",
    "results = compute_scores(summary_hypotheses, summary_references, \"summary\", results)\n",
    "\n",
    "# 4. OVERALL EVALUATION\n",
    "print(\"\\nEvaluating Overall...\")\n",
    "overall_hypotheses = response_hypotheses + classify_hypotheses + summary_hypotheses\n",
    "overall_references = response_references + classify_references + summary_references\n",
    "results = compute_scores(overall_hypotheses, overall_references, \"overall\", results)\n",
    "\n",
    "# Save results to JSON\n",
    "with open(results_file, \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "print(f\"\\nSaved results to {results_file}\")\n",
    "\n",
    "# Plot results\n",
    "modules = list(results.keys())\n",
    "# Extend metric list with new ones (while preserving originals)\n",
    "metrics = [\n",
    "    \"BLEU\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"METEOR\",\n",
    "    \"BERTScore (Custom)\",\n",
    "    \"BERTScore-P\", \"BERTScore-R\", \"BERTScore-F1\",\n",
    "    \"SBERT Similarity\",\n",
    "    \"Perplexity\", \"Length Ratio\", \"Bigram Diversity\"\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "bar_width = 0.06  # narrower since we have more metrics now\n",
    "x = np.arange(len(modules))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    values = [results[m].get(metric, 0) for m in modules]  # Use 0 if metric not present\n",
    "    plt.bar(x + i * bar_width, values, width=bar_width, label=metric)\n",
    "\n",
    "plt.xticks(x + bar_width * (len(metrics) - 1) / 2, modules)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Evaluation Metrics per Module\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_file)\n",
    "print(f\"Saved plot to {plot_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c46c59-ef67-4572-affb-ea171360f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, BertTokenizer, BertModel\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.util import ngrams\n",
    "from nltk import download\n",
    "from rouge_score import rouge_scorer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Download NLTK data if needed (may require adjustment if no internet)\n",
    "download('wordnet', quiet=True)\n",
    "download('omw-1.4', quiet=True)  # For multilingual, but assuming English\n",
    "\n",
    "# Set up paths (adjust if needed)\n",
    "model_id = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "final_dir = \"./qwen3_lora_finetuned_final\"\n",
    "test_glob = r\"Eval/**/*.jsonl\"\n",
    "results_file = \"metrics_results.json\"\n",
    "plot_file = \"metrics_plot.png\"\n",
    "\n",
    "# Load tokenizer and model once\n",
    "tokenizer = AutoTokenizer.from_pretrained(final_dir)\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    final_dir,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Load BERT for custom BERTScore\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased').eval()\n",
    "\n",
    "# Load test examples\n",
    "test_files = glob.glob(test_glob, recursive=True)\n",
    "examples = []\n",
    "for file in test_files:\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            examples.append(data[\"messages\"])\n",
    "\n",
    "# Function to determine module type\n",
    "def get_module_type(system_content):\n",
    "    lower_content = system_content.lower()\n",
    "    if \"therapeutic response generator\" in lower_content:\n",
    "        return \"response\"\n",
    "    elif \"cumulative summary ai\" in lower_content:\n",
    "        return \"summary\"\n",
    "    elif \"phq-8 classification ai\" in lower_content:\n",
    "        return \"classify\"\n",
    "    return \"unknown\"\n",
    "\n",
    "# Filter examples\n",
    "response_examples = [ex for ex in examples if get_module_type(ex[0][\"content\"]) == \"response\"]\n",
    "summary_examples = [ex for ex in examples if get_module_type(ex[0][\"content\"]) == \"summary\"]\n",
    "classify_examples = [ex for ex in examples if get_module_type(ex[0][\"content\"]) == \"classify\"]\n",
    "\n",
    "# Generation function\n",
    "def generate_output(messages):\n",
    "    prompt = tokenizer.apply_chat_template(messages[:-1], tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            top_p=1.0,\n",
    "        )\n",
    "    generated = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n",
    "    return generated\n",
    "\n",
    "# Custom BERTScore function (sentence-level average embedding cosine)\n",
    "def compute_bertscore(hyps, refs):\n",
    "    if not hyps:\n",
    "        return 0.0\n",
    "    hyp_embs = []\n",
    "    ref_embs = []\n",
    "    for h, r in zip(hyps, refs):\n",
    "        hyp_inputs = bert_tokenizer(h, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        ref_inputs = bert_tokenizer(r, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            hyp_emb = bert_model(**hyp_inputs).last_hidden_state.mean(dim=1)\n",
    "            ref_emb = bert_model(**ref_inputs).last_hidden_state.mean(dim=1)\n",
    "        hyp_embs.append(hyp_emb)\n",
    "        ref_embs.append(ref_emb)\n",
    "    scores = [F.cosine_similarity(h, r).item() for h, r in zip(hyp_embs, ref_embs)]\n",
    "    return np.mean(scores)\n",
    "\n",
    "# Perplexity function\n",
    "def compute_perplexity(texts):\n",
    "    if not texts:\n",
    "        return 0.0\n",
    "    ppl = []\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            loss = model(**inputs, labels=inputs[\"input_ids\"]).loss\n",
    "        ppl.append(torch.exp(loss).item())\n",
    "    return np.mean(ppl)\n",
    "\n",
    "# Function to compute scores\n",
    "def compute_scores(hypotheses, references, module_name, results_dict):\n",
    "    if not hypotheses:\n",
    "        print(f\"No {module_name} module examples found.\")\n",
    "        return results_dict\n",
    "\n",
    "    # BLEU\n",
    "    ref_tokens = [[ref.split()] for ref in references]\n",
    "    hyp_tokens = [hyp.split() for hyp in hypotheses]\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu_score = corpus_bleu(ref_tokens, hyp_tokens, smoothing_function=smoothing)\n",
    "\n",
    "    # ROUGE\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        scores = scorer.score(ref, hyp)\n",
    "        for key in rouge_scores:\n",
    "            rouge_scores[key].append(scores[key].fmeasure)\n",
    "    avg_rouge1 = np.mean(rouge_scores['rouge1'])\n",
    "    avg_rouge2 = np.mean(rouge_scores['rouge2'])\n",
    "    avg_rougeL = np.mean(rouge_scores['rougeL'])\n",
    "\n",
    "    # METEOR\n",
    "    meteor_scores = [meteor_score([ref.split()], hyp.split()) for ref, hyp in zip(references, hypotheses)]\n",
    "    avg_meteor = np.mean(meteor_scores)\n",
    "\n",
    "    # Custom BERTScore\n",
    "    avg_bertscore = compute_bertscore(hypotheses, references)\n",
    "\n",
    "    # Perplexity (on references)\n",
    "    avg_ppl = compute_perplexity(references)\n",
    "\n",
    "    # Length Ratio\n",
    "    len_ratio = np.mean([len(h) / len(r) for h, r in zip(hypotheses, references) if len(r) > 0])\n",
    "\n",
    "    # Bigram Diversity\n",
    "    all_bigrams = set()\n",
    "    total_bigrams = 0\n",
    "    for hyp in hypotheses:\n",
    "        hyp_words = hyp.split()\n",
    "        hyp_bigrams = list(ngrams(hyp_words, 2))\n",
    "        all_bigrams.update(hyp_bigrams)\n",
    "        total_bigrams += len(hyp_bigrams)\n",
    "    diversity = len(all_bigrams) / total_bigrams if total_bigrams > 0 else 0.0\n",
    "\n",
    "    # Save results\n",
    "    results_dict[module_name] = {\n",
    "        \"BLEU\": bleu_score,\n",
    "        \"ROUGE-1\": avg_rouge1,\n",
    "        \"ROUGE-2\": avg_rouge2,\n",
    "        \"ROUGE-L\": avg_rougeL,\n",
    "        \"METEOR\": avg_meteor,\n",
    "        \"BERTScore (Custom)\": avg_bertscore,\n",
    "        \"Perplexity\": avg_ppl,\n",
    "        \"Length Ratio\": len_ratio,\n",
    "        \"Bigram Diversity\": diversity\n",
    "    }\n",
    "    print(f\"{module_name.capitalize()} BLEU: {bleu_score:.4f}\")\n",
    "    print(f\"{module_name.capitalize()} ROUGE-1: {avg_rouge1:.4f}\")\n",
    "    print(f\"{module_name.capitalize()} ROUGE-2: {avg_rouge2:.4f}\")\n",
    "    print(f\"{module_name.capitalize()} ROUGE-L: {avg_rougeL:.4f}\")\n",
    "    print(f\"{module_name.capitalize()} METEOR: {avg_meteor:.4f}\")\n",
    "    print(f\"{module_name.capitalize()} BERTScore (Custom): {avg_bertscore:.4f}\")\n",
    "    print(f\"{module_name.capitalize()} Perplexity: {avg_ppl:.4f}\")\n",
    "    print(f\"{module_name.capitalize()} Length Ratio: {len_ratio:.4f}\")\n",
    "    print(f\"{module_name.capitalize()} Bigram Diversity: {diversity:.4f}\")\n",
    "    return results_dict\n",
    "\n",
    "# Run evaluations\n",
    "results = {}\n",
    "\n",
    "print(\"Evaluating Response Module...\")\n",
    "response_hypotheses = [generate_output(ex) for ex in response_examples]\n",
    "response_references = [ex[-1][\"content\"] for ex in response_examples]\n",
    "results = compute_scores(response_hypotheses, response_references, \"response\", results)\n",
    "\n",
    "print(\"\\nEvaluating Summary Module...\")\n",
    "summary_hypotheses = [generate_output(ex) for ex in summary_examples]\n",
    "summary_references = [ex[-1][\"content\"] for ex in summary_examples]\n",
    "results = compute_scores(summary_hypotheses, summary_references, \"summary\", results)\n",
    "\n",
    "print(\"\\nEvaluating Classify Module...\")\n",
    "classify_hypotheses = [generate_output(ex) for ex in classify_examples]\n",
    "classify_references = [ex[-1][\"content\"] for ex in classify_examples]\n",
    "results = compute_scores(classify_hypotheses, classify_references, \"classify\", results)\n",
    "\n",
    "print(\"\\nEvaluating Overall...\")\n",
    "overall_hypotheses = response_hypotheses + summary_hypotheses + classify_hypotheses\n",
    "overall_references = response_references + summary_references + classify_references\n",
    "results = compute_scores(overall_hypotheses, overall_references, \"overall\", results)\n",
    "\n",
    "# Save results to JSON\n",
    "with open(results_file, \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "print(f\"\\nSaved results to {results_file}\")\n",
    "\n",
    "# Plot results\n",
    "modules = list(results.keys())\n",
    "metrics = [\"BLEU\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"METEOR\", \"BERTScore (Custom)\", \"Perplexity\", \"Length Ratio\", \"Bigram Diversity\"]\n",
    "\n",
    "plt.figure(figsize=(14,8))\n",
    "bar_width = 0.1\n",
    "x = np.arange(len(modules))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    values = [results[m].get(metric, 0) for m in modules]  # Use 0 if metric not present\n",
    "    plt.bar(x + i*bar_width, values, width=bar_width, label=metric)\n",
    "\n",
    "plt.xticks(x + bar_width*(len(metrics)-1)/2, modules)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Evaluation Metrics per Module\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_file)\n",
    "print(f\"Saved plot to {plot_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
