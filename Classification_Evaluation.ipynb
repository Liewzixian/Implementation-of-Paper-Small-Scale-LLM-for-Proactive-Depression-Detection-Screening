{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55633517-c30f-423c-b018-1d1de3909d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from typing import List, Dict\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import re\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ==============================\n",
    "# CONFIGURATION\n",
    "# ==============================\n",
    "BASE_MODEL_PATH = \"Qwen/Qwen3-4B-Instruct-2507\"  # Base Qwen model\n",
    "LORA_ADAPTER_PATH = \"32_V5_Model/checkpoint-400\"\n",
    "MERGED_MODEL_PATH = \"./qwen3_merged_model_5090\"  # Optional merged model dir\n",
    "\n",
    "\n",
    "\n",
    "class TherapeuticChatbot:\n",
    "    def __init__(self, use_merged: bool = False):\n",
    "        \"\"\"\n",
    "        Load fine-tuned Qwen model (LoRA adapters or merged model).\n",
    "        \"\"\"\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        if self.device == \"cpu\":\n",
    "            print(\"Warning: Running on CPU, which is slow. Use a GPU for better performance.\")\n",
    "\n",
    "        # ============================================================\n",
    "        # FIXED TOKENIZER LOADING (MATCHING TRAINING CODE EXACTLY)\n",
    "        # ============================================================\n",
    "        print(f\"Loading tokenizer from: {BASE_MODEL_PATH}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "        \n",
    "        # Apply the EXACT SAME tokenizer configuration as in training\n",
    "        # Set PAD to EOS (standard for Qwen to avoid resizing or using UNK)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        print(f\"Set pad_token to eos_token: {self.tokenizer.pad_token}\")\n",
    "        \n",
    "        # Set padding side to left for Flash Attention compatibility\n",
    "        self.tokenizer.padding_side = \"left\"\n",
    "        print(\"Set padding_side to 'left' for Flash Attention\")\n",
    "        \n",
    "        # BOS token is None by design in Qwen3\n",
    "        self.tokenizer.bos_token = None\n",
    "        print(\"BOS token set to None (Qwen3 design)\")\n",
    "        \n",
    "        # Verify token configuration\n",
    "        print(f\"EOS token: {self.tokenizer.eos_token} (ID: {self.tokenizer.eos_token_id})\")\n",
    "        print(f\"PAD token: {self.tokenizer.pad_token} (ID: {self.tokenizer.pad_token_id})\")\n",
    "        print(f\"BOS token: {self.tokenizer.bos_token}\")\n",
    "\n",
    "        # ============================================================\n",
    "        # MODEL LOADING WITH PROPER CONFIG ALIGNMENT\n",
    "        # ============================================================\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        )\n",
    "\n",
    "        attn_impl = \"flash_attention_2\" if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else \"eager\"\n",
    "\n",
    "        if use_merged:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                MERGED_MODEL_PATH,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\",\n",
    "                attn_implementation=attn_impl\n",
    "            )\n",
    "            print(f\"Loaded merged 4-bit model from {MERGED_MODEL_PATH} with attn_impl={attn_impl}\")\n",
    "        else:\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                BASE_MODEL_PATH,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                attn_implementation=attn_impl\n",
    "            )\n",
    "            self.model = PeftModel.from_pretrained(\n",
    "                base_model,\n",
    "                LORA_ADAPTER_PATH,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "            print(f\"Loaded base model {BASE_MODEL_PATH} in 4-bit with LoRA adapters from {LORA_ADAPTER_PATH} and attn_impl={attn_impl}\")\n",
    "\n",
    "        # Function to align model config with tokenizer (from training code)\n",
    "        self._fix_model_config_tokens()\n",
    "        self.model.eval()\n",
    "\n",
    "        self.system_prompts = {\n",
    "            \"Summary\": (\n",
    "                \"You are a cumulative summary AI for therapeutic conversations.\\n\\n\"\n",
    "                \"Task: Generate a concise cumulative summary of the full conversation so far.\\n\\n\"\n",
    "                \"Rules:\\n\"\n",
    "                \"1. Use only direct participant evidence—no inference or assumptions.\\n\"\n",
    "                \"2. Always include the most recent participant response.\\n\"\n",
    "                \"3. Capture:\\n\"\n",
    "                \" - Explicit emotional indicators (with quotes).\\n\"\n",
    "                \" - Depression symptoms mapped to PHQ-8 domains if mentioned.\\n\"\n",
    "                \" - Current participant state/mood based on evidence.\\n\"\n",
    "                \" - Therapist's assessment approach if evident.\\n\"\n",
    "                \"4. Keep to 2–4 sentences (max 6–8), avoiding repetition.\\n\"\n",
    "                \"5. Exclude casual or irrelevant small talk.\\n\"\n",
    "            ),\n",
    "            \"Classification\": (\n",
    "                \"You are a PHQ-8 classification AI for therapeutic conversations.\\n\"\n",
    "                \"Task: Using the most recent participant response, prior 10 turns, and the cumulative summary, classify PHQ-8 symptom severities.\\n\\n\"\n",
    "                \"Rules:\\n\"\n",
    "                \"1. Use only explicit participant statements as evidence. Ignore therapist text unless quoting the participant.\\n\"\n",
    "                \"2. Be conservative: if no direct evidence exists, assign 'Not explored'.\\n\"\n",
    "                \"3. Allowed severity values: 'Not explored', 'Not at all', 'Several days', 'More than half the days', 'Nearly every day'.\\n\"\n",
    "                \"4. Always provide an evidence mapping (quote or 'no evidence').\\n\"\n",
    "                \"5. Depression classification is based only on mapped severities:\\n\"\n",
    "                \" - If multiple symptoms are rated at higher severities (≥ 'More than half the days'), label as 'Depressed'.\\n\"\n",
    "                \" - Otherwise, 'Not depressed'.\\n\"\n",
    "                \"6. Always incorporate the most recent participant response into classification.\\n\\n\"\n",
    "                \"Output only valid JSON in this exact format:\\n\"\n",
    "                \"{\\n\"\n",
    "                \" \\\"evidence_mapping\\\": {\\\"PHQ1\\\": \\\"evidence or 'no evidence'\\\", ..., \\\"PHQ8\\\": \\\"...\\\"},\\n\"\n",
    "                \" \\\"phq8_scores\\\": {\\\"PHQ1\\\": \\\"severity\\\", ..., \\\"PHQ8\\\": \\\"severity\\\"},\\n\"\n",
    "                \" \\\"depression_classification\\\": \\\"Depressed\\\" or \\\"Not depressed\\\"\\n\"\n",
    "                \"}\"\n",
    "            ),\n",
    "            \"Response\": (\n",
    "                \"You are a therapeutic response generator AI conducting a depression screening interview.\\n\\n\"\n",
    "                \"Task: Based on the last 10 turns, the cumulative summary, and PHQ-8 results, generate a natural therapist reply.\\n\\n\"\n",
    "                \"Guidelines:\\n\"\n",
    "                \"1. Responses must be clinically appropriate, and advance assessment.\\n\"\n",
    "                \"2. Incorporate therapeutic strategy (technique applied), response intent (1-sentence purpose), and emotion tag (tone).\\n\"\n",
    "                \"3. If symptoms are unclear, probe gently. If distress is explicit, respond with validation and support.\\n\\n\"\n",
    "                \"Output only valid JSON in this exact format:\\n\"\n",
    "                \"{\\n\"\n",
    "                \" \\\"strategy_used\\\": \\\"Therapeutic strategy applied\\\",\\n\"\n",
    "                \" \\\"response_intent\\\": \\\"1-sentence purpose\\\",\\n\"\n",
    "                \" \\\"emotion_tag\\\": \\\"Emotional tone\\\",\\n\"\n",
    "                \" \\\"therapist_response\\\": \\\"Response text\\\"\\n\"\n",
    "                \"}\"\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        self.phq8_questions = [\n",
    "            \"Little interest or pleasure in doing things\",\n",
    "            \"Feeling down, depressed, or hopeless\",\n",
    "            \"Trouble falling or staying asleep, or sleeping too much\",\n",
    "            \"Feeling tired or having little energy\",\n",
    "            \"Poor appetite or overeating\",\n",
    "            \"Feeling bad about yourself or that you are a failure\",\n",
    "            \"Trouble concentrating on things\",\n",
    "            \"Moving or speaking slowly or being fidgety/restless\"\n",
    "        ]\n",
    "\n",
    "    def _fix_model_config_tokens(self):\n",
    "        \"\"\"Align model config with tokenizer to prevent warnings\"\"\"\n",
    "        if hasattr(self.model.config, 'pad_token_id'):\n",
    "            self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
    "        if hasattr(self.model.config, 'bos_token_id'):\n",
    "            self.model.config.bos_token_id = None  # Qwen3 doesn't use BOS\n",
    "        if hasattr(self.model.config, 'eos_token_id'):\n",
    "            self.model.config.eos_token_id = self.tokenizer.eos_token_id\n",
    "        \n",
    "        # Also fix generation config if it exists\n",
    "        if hasattr(self.model, 'generation_config'):\n",
    "            if self.model.generation_config is not None:\n",
    "                self.model.generation_config.pad_token_id = self.tokenizer.pad_token_id\n",
    "                self.model.generation_config.bos_token_id = None\n",
    "                self.model.generation_config.eos_token_id = self.tokenizer.eos_token_id\n",
    "        print(\"Model config aligned with tokenizer\")\n",
    "\n",
    "    def _generate(self, messages: List[Dict[str, str]], max_new_tokens: int = 512) -> str:\n",
    "        input_text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = self.tokenizer(input_text, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                temperature=0.0,\n",
    "                top_p=1.0,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,  # Explicit pad token\n",
    "                eos_token_id=self.tokenizer.eos_token_id,  # Explicit eos token\n",
    "                bos_token_id=None,  # Qwen3 doesn't use BOS\n",
    "                repetition_penalty=1.2\n",
    "            )\n",
    "\n",
    "        generated_text = self.tokenizer.decode(\n",
    "            outputs[0][len(inputs[\"input_ids\"][0]):],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        return generated_text.strip()\n",
    "\n",
    "    def summarize_conversation(self, conversation_history: List[Dict[str, str]]) -> str:\n",
    "        input_data = json.dumps({\"conversation_history\": conversation_history})\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompts[\"Summary\"]},\n",
    "            {\"role\": \"user\", \"content\": input_data}\n",
    "        ]\n",
    "        output = self._generate(messages, max_new_tokens=1000)\n",
    "        return self._remove_xml_tags(output).strip()\n",
    "\n",
    "    def classify_phq8(self, conversation_history: List[Dict[str, str]], cumulative_summary: str) -> Dict:\n",
    "        recent_history = conversation_history[-10:] if len(conversation_history) > 10 else conversation_history\n",
    "        input_data = json.dumps({\"recent_context\": recent_history,\"cumulative_summary\": cumulative_summary})\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompts[\"Classification\"]},\n",
    "            {\"role\": \"user\", \"content\": input_data}\n",
    "        ]\n",
    "        output = self._generate(messages, max_new_tokens=1000)\n",
    "        json_str = self._extract_json_from_output(output)\n",
    "        try:\n",
    "            parsed = json.loads(json_str)\n",
    "            result = {\n",
    "                \"phq8_scores\": parsed.get(\"phq8_scores\", {f\"PHQ{i+1}\": \"Not explored\" for i in range(8)}),\n",
    "                \"depression_classification\": parsed.get(\"depression_classification\", \"Not depressed\"),\n",
    "                \"evidence_mapping\": parsed.get(\"evidence_mapping\", {f\"PHQ{i+1}\": \"no evidence\" for i in range(8)})\n",
    "            }\n",
    "        except json.JSONDecodeError:\n",
    "            result = {\n",
    "                \"phq8_scores\": {f\"PHQ{i+1}\": \"Not explored\" for i in range(8)},\n",
    "                \"depression_classification\": \"Not depressed\",\n",
    "                \"evidence_mapping\": {f\"PHQ{i+1}\": \"no evidence\" for i in range(8)}\n",
    "            }\n",
    "        return result\n",
    "\n",
    "    def _extract_json_from_output(self, output: str) -> str:\n",
    "        cleaned = self._remove_xml_tags(output).strip()\n",
    "        start_idx = cleaned.find('{')\n",
    "        end_idx = cleaned.rfind('}') + 1\n",
    "        return cleaned[start_idx:end_idx] if start_idx != -1 and end_idx != -1 else cleaned\n",
    "\n",
    "    def _remove_xml_tags(self, text: str) -> str:\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        cleaned = re.sub(r'<[^>]+>', '', text)\n",
    "        return re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "\n",
    "\n",
    "def parse_transcript(file_path: str) -> List[Dict[str, str]]:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    if 'speaker' not in df.columns or 'value' not in df.columns:\n",
    "        df['speaker'] = df.iloc[:, 0].str.extract(r'(\\w+)', expand=False)\n",
    "        df['value'] = df.iloc[:, 0].str.extract(r'\\w+ (.*)', expand=False)\n",
    "    history, current_text, current_speaker = [], [], None\n",
    "    for _, row in df.iterrows():\n",
    "        role = 'therapist' if 'ellie' in str(row['speaker']).lower() else 'participant'\n",
    "        if role != current_speaker:\n",
    "            if current_speaker:\n",
    "                history.append({'speaker_role': current_speaker, 'text': ' '.join(current_text)})\n",
    "            current_speaker, current_text = role, [str(row['value'])]\n",
    "        else:\n",
    "            current_text.append(str(row['value']))\n",
    "    if current_speaker:\n",
    "        history.append({'speaker_role': current_speaker, 'text': ' '.join(current_text)})\n",
    "    return history\n",
    "\n",
    "\n",
    "def map_severity_to_number(severity: str) -> int:\n",
    "    severity_map = {\n",
    "        \"Not explored\": 0,\n",
    "        \"Not at all\": 0,\n",
    "        \"Several days\": 1,\n",
    "        \"More than half the days\": 2,\n",
    "        \"Nearly every day\": 3\n",
    "    }\n",
    "    return severity_map.get(severity, 0)\n",
    "\n",
    "\n",
    "def evaluate_model(test_folder: str, label_csv: str, use_merged=False):\n",
    "    chatbot = TherapeuticChatbot(use_merged=use_merged)\n",
    "    labels_df = pd.read_csv(label_csv)\n",
    "    labels_df.columns = labels_df.columns.str.strip()\n",
    "    if 'PHQ_Binary' not in labels_df.columns or 'PHQ_Score' not in labels_df.columns:\n",
    "        labels_df.rename(columns={'PHQ8_Binary': 'PHQ_Binary', 'PHQ8_Score': 'PHQ_Score'}, inplace=True)\n",
    "\n",
    "    predictions_binary, true_binary, predictions_total, true_total, results = [], [], [], [], []\n",
    "\n",
    "    for file_name in os.listdir(test_folder):\n",
    "        if not file_name.lower().endswith('.csv'):\n",
    "            continue\n",
    "        try:\n",
    "            pid = int(file_name.split('_')[0])\n",
    "        except ValueError:\n",
    "            continue\n",
    "        label_row = labels_df[labels_df['Participant_ID'] == pid]\n",
    "        if label_row.empty:\n",
    "            continue\n",
    "\n",
    "        true_binary_num = label_row['PHQ_Binary'].values[0]\n",
    "        true_label = \"Depressed\" if true_binary_num == 1 else \"Not depressed\"\n",
    "        true_total_score = label_row['PHQ_Score'].values[0]\n",
    "\n",
    "        file_path = os.path.join(test_folder, file_name)\n",
    "        conversation_history = parse_transcript(file_path)\n",
    "\n",
    "        summary = chatbot.summarize_conversation(conversation_history)\n",
    "        classification = chatbot.classify_phq8(conversation_history, summary)\n",
    "\n",
    "        predicted_label = classification['depression_classification']\n",
    "        predicted_phq_list = [map_severity_to_number(classification['phq8_scores'].get(f\"PHQ{i+1}\", \"Not explored\")) for i in range(8)]\n",
    "        predicted_total_score = sum(predicted_phq_list)\n",
    "\n",
    "        predictions_binary.append(predicted_label)\n",
    "        true_binary.append(true_label)\n",
    "        predictions_total.append(predicted_total_score)\n",
    "        true_total.append(true_total_score)\n",
    "\n",
    "        results.append({\n",
    "            \"Participant_ID\": pid,\n",
    "            \"Predicted_Binary\": 1 if predicted_label == \"Depressed\" else 0,\n",
    "            \"True_Binary\": true_binary_num,\n",
    "            \"Predicted_Score\": predicted_total_score,\n",
    "            \"True_Score\": true_total_score,\n",
    "            \"Raw_Classification_Output\": json.dumps(classification)\n",
    "        })\n",
    "\n",
    "    if results:\n",
    "        pd.DataFrame(results).to_csv(\"evaluation_results.csv\", index=False)\n",
    "\n",
    "    # Metrics\n",
    "    pred_binary_num = [1 if p == \"Depressed\" else 0 for p in predictions_binary]\n",
    "    true_binary_num = [1 if t == \"Depressed\" else 0 for t in true_binary]\n",
    "    accuracy_bin = accuracy_score(true_binary_num, pred_binary_num)\n",
    "    precision_bin = precision_score(true_binary_num, pred_binary_num, zero_division=0)\n",
    "    recall_bin = recall_score(true_binary_num, pred_binary_num, zero_division=0)\n",
    "    f1_bin = f1_score(true_binary_num, pred_binary_num, zero_division=0)\n",
    "    cm_bin = confusion_matrix(true_binary_num, pred_binary_num)\n",
    "\n",
    "    total_mae = np.mean(np.abs(np.array(predictions_total) - np.array(true_total)))\n",
    "\n",
    "    print(\"\\nBinary Depression Classification Results:\")\n",
    "    print(f\"Accuracy: {accuracy_bin:.4f}\")\n",
    "    print(f\"Precision: {precision_bin:.4f}\")\n",
    "    print(f\"Recall: {recall_bin:.4f}\")\n",
    "    print(f\"F1 Score: {f1_bin:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm_bin)\n",
    "\n",
    "    print(\"\\nPHQ-8 Total Score Results:\")\n",
    "    print(f\"Total PHQ-8 Score MAE: {total_mae:.4f}\")\n",
    "\n",
    "    metrics = {\n",
    "        \"binary_classification\": {\n",
    "            \"accuracy\": accuracy_bin,\n",
    "            \"precision\": precision_bin,\n",
    "            \"recall\": recall_bin,\n",
    "            \"f1_score\": f1_bin,\n",
    "            \"confusion_matrix\": cm_bin.tolist()\n",
    "        },\n",
    "        \"phq8_score\": {\"MAE\": total_mae}\n",
    "    }\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    with open(f\"evaluation_metrics_{timestamp}.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    # --- Visualization Section ---\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm_bin, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=[\"Not Depressed\", \"Depressed\"],\n",
    "                yticklabels=[\"Not Depressed\", \"Depressed\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Binary Depression Classification Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"confusion_matrix_{timestamp}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(true_total, predictions_total, alpha=0.6)\n",
    "    plt.plot([0, max(true_total)], [0, max(true_total)], color=\"red\", linestyle=\"--\")\n",
    "    plt.xlabel(\"True PHQ-8 Score\")\n",
    "    plt.ylabel(\"Predicted PHQ-8 Score\")\n",
    "    plt.title(\"PHQ-8 True vs Predicted Scores\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"phq8_true_vs_pred_{timestamp}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"\\nMetrics and plots saved with timestamp {timestamp}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_model(\n",
    "        test_folder=\"Extracted_Text_Transcript_DAIC\",\n",
    "        label_csv=\"full_test_split.csv\",\n",
    "        use_merged=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992fe733-09b3-4e5b-9c33-2601f3c8f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from typing import List, Dict\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import re\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ==============================\n",
    "# CONFIGURATION\n",
    "# ==============================\n",
    "BASE_MODEL_PATH = \"Qwen/Qwen3-4B-Instruct-2507\"  # Base Qwen model\n",
    "LORA_ADAPTER_PATH = \"./checkpoint-400\"\n",
    "MERGED_MODEL_PATH = \"./qwen3_merged_model_5090\"  # Optional merged model dir\n",
    "\n",
    "\n",
    "class TherapeuticChatbot:\n",
    "    def __init__(self, use_merged: bool = False):\n",
    "        \"\"\"\n",
    "        Load fine-tuned Qwen model (LoRA adapters or merged model).\n",
    "        \"\"\"\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        if self.device == \"cpu\":\n",
    "            print(\"Warning: Running on CPU, which is slow. Use a GPU for better performance.\")\n",
    "\n",
    "        # Always load tokenizer from the base model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        )\n",
    "\n",
    "        attn_impl = \"flash_attention_2\" if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else \"eager\"\n",
    "\n",
    "        if use_merged:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                MERGED_MODEL_PATH,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\",\n",
    "                attn_implementation=attn_impl\n",
    "            )\n",
    "            print(f\"Loaded merged 4-bit model from {MERGED_MODEL_PATH} with attn_impl={attn_impl}\")\n",
    "        else:\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                BASE_MODEL_PATH,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                attn_implementation=attn_impl\n",
    "            )\n",
    "            self.model = PeftModel.from_pretrained(\n",
    "                base_model,\n",
    "                LORA_ADAPTER_PATH,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "            print(f\"Loaded base model {BASE_MODEL_PATH} in 4-bit with LoRA adapters from {LORA_ADAPTER_PATH} and attn_impl={attn_impl}\")\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        self.system_prompts = {\n",
    "            \"Summary\": (\n",
    "                \"You are a cumulative summary AI for therapeutic conversations.\\n\\n\"\n",
    "                \"Task: Generate a concise cumulative summary of the full conversation so far.\\n\\n\"\n",
    "                \"Rules:\\n\"\n",
    "                \"1. Use only direct participant evidence—no inference or assumptions.\\n\"\n",
    "                \"2. Always include the most recent participant response.\\n\"\n",
    "                \"3. Capture:\\n\"\n",
    "                \" - Explicit emotional indicators (with quotes).\\n\"\n",
    "                \" - Depression symptoms mapped to PHQ-8 domains if mentioned.\\n\"\n",
    "                \" - Current participant state/mood based on evidence.\\n\"\n",
    "                \" - Therapist’s assessment approach if evident.\\n\"\n",
    "                \"4. Keep to 2–4 sentences (max 6–8), avoiding repetition.\\n\"\n",
    "                \"5. Exclude casual or irrelevant small talk.\\n\"\n",
    "            ),\n",
    "            \"Classification\": (\n",
    "                \"You are a PHQ-8 classification AI for therapeutic conversations.\\n\"\n",
    "                \"Task: Using the most recent participant response, prior 10 turns, and the cumulative summary, classify PHQ-8 symptom severities.\\n\\n\"\n",
    "                \"Rules:\\n\"\n",
    "                \"1. Use only explicit participant statements as evidence. Ignore therapist text unless quoting the participant.\\n\"\n",
    "                \"2. Be conservative: if no direct evidence exists, assign 'Not explored'.\\n\"\n",
    "                \"3. Allowed severity values: 'Not explored', 'Not at all', 'Several days', 'More than half the days', 'Nearly every day'.\\n\"\n",
    "                \"4. Always provide an evidence mapping (quote or 'no evidence').\\n\"\n",
    "                \"5. Depression classification is based only on mapped severities:\\n\"\n",
    "                \" - If multiple symptoms are rated at higher severities (≥ 'More than half the days'), label as 'Depressed'.\\n\"\n",
    "                \" - Otherwise, 'Not depressed'.\\n\"\n",
    "                \"6. Always incorporate the most recent participant response into classification.\\n\\n\"\n",
    "                \"Output only valid JSON in this exact format:\\n\"\n",
    "                \"{\\n\"\n",
    "                \" \\\"evidence_mapping\\\": {\\\"PHQ1\\\": \\\"evidence or 'no evidence'\\\", ..., \\\"PHQ8\\\": \\\"...\\\"},\\n\"\n",
    "                \" \\\"phq8_scores\\\": {\\\"PHQ1\\\": \\\"severity\\\", ..., \\\"PHQ8\\\": \\\"severity\\\"},\\n\"\n",
    "                \" \\\"depression_classification\\\": \\\"Depressed\\\" or \\\"Not depressed\\\"\\n\"\n",
    "                \"}\"\n",
    "            ),\n",
    "            \"Response\": (\n",
    "                \"You are a therapeutic response generator AI conducting a depression screening interview.\\n\\n\"\n",
    "                \"Task: Based on the last 10 turns, the cumulative summary, and PHQ-8 results, generate a natural therapist reply.\\n\\n\"\n",
    "                \"Guidelines:\\n\"\n",
    "                \"1. Responses must be clinically appropriate, and advance assessment.\\n\"\n",
    "                \"2. Incorporate therapeutic strategy (technique applied), response intent (1-sentence purpose), and emotion tag (tone).\\n\"\n",
    "                \"3. If symptoms are unclear, probe gently. If distress is explicit, respond with validation and support.\\n\\n\"\n",
    "                \"Output only valid JSON in this exact format:\\n\"\n",
    "                \"{\\n\"\n",
    "                \" \\\"strategy_used\\\": \\\"Therapeutic strategy applied\\\",\\n\"\n",
    "                \" \\\"response_intent\\\": \\\"1-sentence purpose\\\",\\n\"\n",
    "                \" \\\"emotion_tag\\\": \\\"Emotional tone\\\",\\n\"\n",
    "                \" \\\"therapist_response\\\": \\\"Response text\\\"\\n\"\n",
    "                \"}\"\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        self.phq8_questions = [\n",
    "            \"Little interest or pleasure in doing things\",\n",
    "            \"Feeling down, depressed, or hopeless\",\n",
    "            \"Trouble falling or staying asleep, or sleeping too much\",\n",
    "            \"Feeling tired or having little energy\",\n",
    "            \"Poor appetite or overeating\",\n",
    "            \"Feeling bad about yourself or that you are a failure\",\n",
    "            \"Trouble concentrating on things\",\n",
    "            \"Moving or speaking slowly or being fidgety/restless\"\n",
    "        ]\n",
    "\n",
    "    def _generate(self, messages: List[Dict[str, str]], max_new_tokens: int = 512) -> str:\n",
    "        input_text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = self.tokenizer(input_text, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                temperature=0.0,\n",
    "                top_p=1.0,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.2\n",
    "            )\n",
    "\n",
    "        generated_text = self.tokenizer.decode(\n",
    "            outputs[0][len(inputs[\"input_ids\"][0]):],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        return generated_text.strip()\n",
    "\n",
    "    def summarize_conversation(self, conversation_history: List[Dict[str, str]]) -> str:\n",
    "        input_data = json.dumps({\"conversation_history\": conversation_history})\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompts[\"Summary\"]},\n",
    "            {\"role\": \"user\", \"content\": input_data}\n",
    "        ]\n",
    "        output = self._generate(messages, max_new_tokens=1000)\n",
    "        return self._remove_xml_tags(output).strip()\n",
    "\n",
    "    def classify_phq8(self, conversation_history: List[Dict[str, str]], cumulative_summary: str) -> Dict:\n",
    "        recent_history = conversation_history[-10:] if len(conversation_history) > 10 else conversation_history\n",
    "        input_data = json.dumps({\"recent_context\": recent_history,\"cumulative_summary\": cumulative_summary})\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompts[\"Classification\"]},\n",
    "            {\"role\": \"user\", \"content\": input_data}\n",
    "        ]\n",
    "        output = self._generate(messages, max_new_tokens=1000)\n",
    "        json_str = self._extract_json_from_output(output)\n",
    "        try:\n",
    "            parsed = json.loads(json_str)\n",
    "            result = {\n",
    "                \"phq8_scores\": parsed.get(\"phq8_scores\", {f\"PHQ{i+1}\": \"Not explored\" for i in range(8)}),\n",
    "                \"depression_classification\": parsed.get(\"depression_classification\", \"Not depressed\"),\n",
    "                \"evidence_mapping\": parsed.get(\"evidence_mapping\", {f\"PHQ{i+1}\": \"no evidence\" for i in range(8)})\n",
    "            }\n",
    "        except json.JSONDecodeError:\n",
    "            result = {\n",
    "                \"phq8_scores\": {f\"PHQ{i+1}\": \"Not explored\" for i in range(8)},\n",
    "                \"depression_classification\": \"Not depressed\",\n",
    "                \"evidence_mapping\": {f\"PHQ{i+1}\": \"no evidence\" for i in range(8)}\n",
    "            }\n",
    "        return result\n",
    "\n",
    "    def _extract_json_from_output(self, output: str) -> str:\n",
    "        cleaned = self._remove_xml_tags(output).strip()\n",
    "        start_idx = cleaned.find('{')\n",
    "        end_idx = cleaned.rfind('}') + 1\n",
    "        return cleaned[start_idx:end_idx] if start_idx != -1 and end_idx != -1 else cleaned\n",
    "\n",
    "    def _remove_xml_tags(self, text: str) -> str:\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        cleaned = re.sub(r'<[^>]+>', '', text)\n",
    "        return re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "\n",
    "\n",
    "def parse_transcript(file_path: str) -> List[Dict[str, str]]:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    if 'speaker' not in df.columns or 'value' not in df.columns:\n",
    "        df['speaker'] = df.iloc[:, 0].str.extract(r'(\\w+)', expand=False)\n",
    "        df['value'] = df.iloc[:, 0].str.extract(r'\\w+ (.*)', expand=False)\n",
    "    history, current_text, current_speaker = [], [], None\n",
    "    for _, row in df.iterrows():\n",
    "        role = 'therapist' if 'ellie' in str(row['speaker']).lower() else 'participant'\n",
    "        if role != current_speaker:\n",
    "            if current_speaker:\n",
    "                history.append({'speaker_role': current_speaker, 'text': ' '.join(current_text)})\n",
    "            current_speaker, current_text = role, [str(row['value'])]\n",
    "        else:\n",
    "            current_text.append(str(row['value']))\n",
    "    if current_speaker:\n",
    "        history.append({'speaker_role': current_speaker, 'text': ' '.join(current_text)})\n",
    "    return history\n",
    "\n",
    "\n",
    "def map_severity_to_number(severity: str) -> int:\n",
    "    severity_map = {\n",
    "        \"Not explored\": 0,\n",
    "        \"Not at all\": 0,\n",
    "        \"Several days\": 1,\n",
    "        \"More than half the days\": 2,\n",
    "        \"Nearly every day\": 3\n",
    "    }\n",
    "    return severity_map.get(severity, 0)\n",
    "\n",
    "\n",
    "def evaluate_model(test_folder: str, label_csv: str, use_merged=False):\n",
    "    chatbot = TherapeuticChatbot(use_merged=use_merged)\n",
    "    labels_df = pd.read_csv(label_csv)\n",
    "    labels_df.columns = labels_df.columns.str.strip()\n",
    "    if 'PHQ_Binary' not in labels_df.columns or 'PHQ_Score' not in labels_df.columns:\n",
    "        labels_df.rename(columns={'PHQ8_Binary': 'PHQ_Binary', 'PHQ8_Score': 'PHQ_Score'}, inplace=True)\n",
    "\n",
    "    predictions_binary, true_binary, predictions_total, true_total, results = [], [], [], [], []\n",
    "\n",
    "    for file_name in os.listdir(test_folder):\n",
    "        if not file_name.lower().endswith('.csv'):\n",
    "            continue\n",
    "        try:\n",
    "            pid = int(file_name.split('_')[0])\n",
    "        except ValueError:\n",
    "            continue\n",
    "        label_row = labels_df[labels_df['Participant_ID'] == pid]\n",
    "        if label_row.empty:\n",
    "            continue\n",
    "\n",
    "        true_binary_num = label_row['PHQ_Binary'].values[0]\n",
    "        true_label = \"Depressed\" if true_binary_num == 1 else \"Not depressed\"\n",
    "        true_total_score = label_row['PHQ_Score'].values[0]\n",
    "\n",
    "        file_path = os.path.join(test_folder, file_name)\n",
    "        conversation_history = parse_transcript(file_path)\n",
    "\n",
    "        summary = chatbot.summarize_conversation(conversation_history)\n",
    "        classification = chatbot.classify_phq8(conversation_history, summary)\n",
    "\n",
    "        predicted_label = classification['depression_classification']\n",
    "        predicted_phq_list = [map_severity_to_number(classification['phq8_scores'].get(f\"PHQ{i+1}\", \"Not explored\")) for i in range(8)]\n",
    "        predicted_total_score = sum(predicted_phq_list)\n",
    "\n",
    "        predictions_binary.append(predicted_label)\n",
    "        true_binary.append(true_label)\n",
    "        predictions_total.append(predicted_total_score)\n",
    "        true_total.append(true_total_score)\n",
    "\n",
    "        results.append({\n",
    "            \"Participant_ID\": pid,\n",
    "            \"Predicted_Binary\": 1 if predicted_label == \"Depressed\" else 0,\n",
    "            \"True_Binary\": true_binary_num,\n",
    "            \"Predicted_Score\": predicted_total_score,\n",
    "            \"True_Score\": true_total_score,\n",
    "            \"Raw_Classification_Output\": json.dumps(classification)\n",
    "        })\n",
    "\n",
    "    if results:\n",
    "        pd.DataFrame(results).to_csv(\"evaluation_results.csv\", index=False)\n",
    "\n",
    "    # Metrics\n",
    "    pred_binary_num = [1 if p == \"Depressed\" else 0 for p in predictions_binary]\n",
    "    true_binary_num = [1 if t == \"Depressed\" else 0 for t in true_binary]\n",
    "    accuracy_bin = accuracy_score(true_binary_num, pred_binary_num)\n",
    "    precision_bin = precision_score(true_binary_num, pred_binary_num, zero_division=0)\n",
    "    recall_bin = recall_score(true_binary_num, pred_binary_num, zero_division=0)\n",
    "    f1_bin = f1_score(true_binary_num, pred_binary_num, zero_division=0)\n",
    "    cm_bin = confusion_matrix(true_binary_num, pred_binary_num)\n",
    "\n",
    "    total_mae = np.mean(np.abs(np.array(predictions_total) - np.array(true_total)))\n",
    "\n",
    "    print(\"\\nBinary Depression Classification Results:\")\n",
    "    print(f\"Accuracy: {accuracy_bin:.4f}\")\n",
    "    print(f\"Precision: {precision_bin:.4f}\")\n",
    "    print(f\"Recall: {recall_bin:.4f}\")\n",
    "    print(f\"F1 Score: {f1_bin:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm_bin)\n",
    "\n",
    "    print(\"\\nPHQ-8 Total Score Results:\")\n",
    "    print(f\"Total PHQ-8 Score MAE: {total_mae:.4f}\")\n",
    "\n",
    "    metrics = {\n",
    "        \"binary_classification\": {\n",
    "            \"accuracy\": accuracy_bin,\n",
    "            \"precision\": precision_bin,\n",
    "            \"recall\": recall_bin,\n",
    "            \"f1_score\": f1_bin,\n",
    "            \"confusion_matrix\": cm_bin.tolist()\n",
    "        },\n",
    "        \"phq8_score\": {\"MAE\": total_mae}\n",
    "    }\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    with open(f\"evaluation_metrics_{timestamp}.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    # --- Visualization Section ---\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm_bin, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=[\"Not Depressed\", \"Depressed\"],\n",
    "                yticklabels=[\"Not Depressed\", \"Depressed\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Binary Depression Classification Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"confusion_matrix_{timestamp}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(true_total, predictions_total, alpha=0.6)\n",
    "    plt.plot([0, max(true_total)], [0, max(true_total)], color=\"red\", linestyle=\"--\")\n",
    "    plt.xlabel(\"True PHQ-8 Score\")\n",
    "    plt.ylabel(\"Predicted PHQ-8 Score\")\n",
    "    plt.title(\"PHQ-8 True vs Predicted Scores\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"phq8_true_vs_pred_{timestamp}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"\\nMetrics and plots saved with timestamp {timestamp}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_model(\n",
    "        test_folder=\"Extracted_Text_Transcript_DAIC\",\n",
    "        label_csv=\"full_test_split.csv\",\n",
    "        use_merged=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f8d0ad-2176-4867-a7fa-c081993fb364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from typing import List, Dict\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import re\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==============================\n",
    "# CONFIGURATION\n",
    "# ==============================\n",
    "BASE_MODEL_PATH = \"Qwen/Qwen3-4B-Instruct-2507\"  # Base Qwen model\n",
    "LORA_ADAPTER_PATH = \"32_V5_Model/checkpoint-400\"\n",
    "MERGED_MODEL_PATH = \"./qwen3_merged_model_5090\"  # Optional merged model dir\n",
    "\n",
    "\n",
    "\n",
    "class TherapeuticChatbot:\n",
    "    def __init__(self, use_merged: bool = False):\n",
    "        \"\"\"\n",
    "        Load fine-tuned Qwen model (LoRA adapters or merged model).\n",
    "        \"\"\"\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        if self.device == \"cpu\":\n",
    "            print(\"Warning: Running on CPU, which is slow. Use a GPU for better performance.\")\n",
    "\n",
    "        # ============================================================\n",
    "        # FIXED TOKENIZER LOADING (MATCHING TRAINING CODE EXACTLY)\n",
    "        # ============================================================\n",
    "        print(f\"Loading tokenizer from: {BASE_MODEL_PATH}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "        \n",
    "        # Apply the EXACT SAME tokenizer configuration as in training\n",
    "        # Set PAD to EOS (standard for Qwen to avoid resizing or using UNK)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        print(f\"Set pad_token to eos_token: {self.tokenizer.pad_token}\")\n",
    "        \n",
    "        # Set padding side to left for Flash Attention compatibility\n",
    "        self.tokenizer.padding_side = \"left\"\n",
    "        print(\"Set padding_side to 'left' for Flash Attention\")\n",
    "        \n",
    "        # BOS token is None by design in Qwen3\n",
    "        self.tokenizer.bos_token = None\n",
    "        print(\"BOS token set to None (Qwen3 design)\")\n",
    "        \n",
    "        # Verify token configuration\n",
    "        print(f\"EOS token: {self.tokenizer.eos_token} (ID: {self.tokenizer.eos_token_id})\")\n",
    "        print(f\"PAD token: {self.tokenizer.pad_token} (ID: {self.tokenizer.pad_token_id})\")\n",
    "        print(f\"BOS token: {self.tokenizer.bos_token}\")\n",
    "\n",
    "        # ============================================================\n",
    "        # MODEL LOADING WITH PROPER CONFIG ALIGNMENT\n",
    "        # ============================================================\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        )\n",
    "\n",
    "        attn_impl = \"flash_attention_2\" if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else \"eager\"\n",
    "\n",
    "        if use_merged:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                MERGED_MODEL_PATH,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\",\n",
    "                attn_implementation=attn_impl\n",
    "            )\n",
    "            print(f\"Loaded merged 4-bit model from {MERGED_MODEL_PATH} with attn_impl={attn_impl}\")\n",
    "        else:\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                BASE_MODEL_PATH,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                attn_implementation=attn_impl\n",
    "            )\n",
    "            self.model = PeftModel.from_pretrained(\n",
    "                base_model,\n",
    "                LORA_ADAPTER_PATH,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "            print(f\"Loaded base model {BASE_MODEL_PATH} in 4-bit with LoRA adapters from {LORA_ADAPTER_PATH} and attn_impl={attn_impl}\")\n",
    "\n",
    "        # Function to align model config with tokenizer (from training code)\n",
    "        self._fix_model_config_tokens()\n",
    "        self.model.eval()\n",
    "\n",
    "        self.system_prompts = {\n",
    "            \"Summary\": (\n",
    "                \"You are a cumulative summary AI for therapeutic conversations.\\n\\n\"\n",
    "                \"Task: Generate a concise cumulative summary of the full conversation so far.\\n\\n\"\n",
    "                \"Rules:\\n\"\n",
    "                \"1. Use only direct participant evidence—no inference or assumptions.\\n\"\n",
    "                \"2. Always include the most recent participant response.\\n\"\n",
    "                \"3. Capture:\\n\"\n",
    "                \" - Explicit emotional indicators (with quotes).\\n\"\n",
    "                \" - Depression symptoms mapped to PHQ-8 domains if mentioned.\\n\"\n",
    "                \" - Current participant state/mood based on evidence.\\n\"\n",
    "                \" - Therapist's assessment approach if evident.\\n\"\n",
    "                \"4. Keep to 2–4 sentences (max 6–8), avoiding repetition.\\n\"\n",
    "                \"5. Exclude casual or irrelevant small talk.\\n\"\n",
    "            ),\n",
    "            \"Classification\": (\n",
    "                \"You are a PHQ-8 classification AI for therapeutic conversations.\\n\"\n",
    "                \"Task: Using the most recent participant response, prior 10 turns, and the cumulative summary, classify PHQ-8 symptom severities.\\n\\n\"\n",
    "                \"Rules:\\n\"\n",
    "                \"1. Use only explicit participant statements as evidence. Ignore therapist text unless quoting the participant.\\n\"\n",
    "                \"2. Be conservative: if no direct evidence exists, assign 'Not explored'.\\n\"\n",
    "                \"3. Allowed severity values: 'Not explored', 'Not at all', 'Several days', 'More than half the days', 'Nearly every day'.\\n\"\n",
    "                \"4. Always provide an evidence mapping (quote or 'no evidence').\\n\"\n",
    "                \"5. Depression classification is based only on mapped severities:\\n\"\n",
    "                \" - If multiple symptoms are rated at higher severities (≥ 'More than half the days'), label as 'Depressed'.\\n\"\n",
    "                \" - Otherwise, 'Not depressed'.\\n\"\n",
    "                \"6. Always incorporate the most recent participant response into classification.\\n\\n\"\n",
    "                \"Output only valid JSON in this exact format:\\n\"\n",
    "                \"{\\n\"\n",
    "                \" \\\"evidence_mapping\\\": {\\\"PHQ1\\\": \\\"evidence or 'no evidence'\\\", ..., \\\"PHQ8\\\": \\\"...\\\"},\\n\"\n",
    "                \" \\\"phq8_scores\\\": {\\\"PHQ1\\\": \\\"severity\\\", ..., \\\"PHQ8\\\": \\\"severity\\\"},\\n\"\n",
    "                \" \\\"depression_classification\\\": \\\"Depressed\\\" or \\\"Not depressed\\\"\\n\"\n",
    "                \"}\"\n",
    "            ),\n",
    "            \"Response\": (\n",
    "                \"You are a therapeutic response generator AI conducting a depression screening interview.\\n\\n\"\n",
    "                \"Task: Based on the last 10 turns, the cumulative summary, and PHQ-8 results, generate a natural therapist reply.\\n\\n\"\n",
    "                \"Guidelines:\\n\"\n",
    "                \"1. Responses must be clinically appropriate, and advance assessment.\\n\"\n",
    "                \"2. Incorporate therapeutic strategy (technique applied), response intent (1-sentence purpose), and emotion tag (tone).\\n\"\n",
    "                \"3. If symptoms are unclear, probe gently. If distress is explicit, respond with validation and support.\\n\\n\"\n",
    "                \"Output only valid JSON in this exact format:\\n\"\n",
    "                \"{\\n\"\n",
    "                \" \\\"strategy_used\\\": \\\"Therapeutic strategy applied\\\",\\n\"\n",
    "                \" \\\"response_intent\\\": \\\"1-sentence purpose\\\",\\n\"\n",
    "                \" \\\"emotion_tag\\\": \\\"Emotional tone\\\",\\n\"\n",
    "                \" \\\"therapist_response\\\": \\\"Response text\\\"\\n\"\n",
    "                \"}\"\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        self.phq8_questions = [\n",
    "            \"Little interest or pleasure in doing things\",\n",
    "            \"Feeling down, depressed, or hopeless\",\n",
    "            \"Trouble falling or staying asleep, or sleeping too much\",\n",
    "            \"Feeling tired or having little energy\",\n",
    "            \"Poor appetite or overeating\",\n",
    "            \"Feeling bad about yourself or that you are a failure\",\n",
    "            \"Trouble concentrating on things\",\n",
    "            \"Moving or speaking slowly or being fidgety/restless\"\n",
    "        ]\n",
    "\n",
    "    def _fix_model_config_tokens(self):\n",
    "        \"\"\"Align model config with tokenizer to prevent warnings\"\"\"\n",
    "        if hasattr(self.model.config, 'pad_token_id'):\n",
    "            self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
    "        if hasattr(self.model.config, 'bos_token_id'):\n",
    "            self.model.config.bos_token_id = None  # Qwen3 doesn't use BOS\n",
    "        if hasattr(self.model.config, 'eos_token_id'):\n",
    "            self.model.config.eos_token_id = self.tokenizer.eos_token_id\n",
    "        \n",
    "        # Also fix generation config if it exists\n",
    "        if hasattr(self.model, 'generation_config'):\n",
    "            if self.model.generation_config is not None:\n",
    "                self.model.generation_config.pad_token_id = self.tokenizer.pad_token_id\n",
    "                self.model.generation_config.bos_token_id = None\n",
    "                self.model.generation_config.eos_token_id = self.tokenizer.eos_token_id\n",
    "        print(\"Model config aligned with tokenizer\")\n",
    "\n",
    "    def _generate(self, messages: List[Dict[str, str]], max_new_tokens: int = 512) -> str:\n",
    "        input_text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = self.tokenizer(input_text, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                temperature=0.0,\n",
    "                top_p=1.0,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,  # Explicit pad token\n",
    "                eos_token_id=self.tokenizer.eos_token_id,  # Explicit eos token\n",
    "                bos_token_id=None,  # Qwen3 doesn't use BOS\n",
    "                repetition_penalty=1.2\n",
    "            )\n",
    "\n",
    "        generated_text = self.tokenizer.decode(\n",
    "            outputs[0][len(inputs[\"input_ids\"][0]):],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        return generated_text.strip()\n",
    "\n",
    "    def summarize_conversation(self, conversation_history: List[Dict[str, str]]) -> str:\n",
    "        input_data = json.dumps({\"conversation_history\": conversation_history})\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompts[\"Summary\"]},\n",
    "            {\"role\": \"user\", \"content\": input_data}\n",
    "        ]\n",
    "        output = self._generate(messages, max_new_tokens=1000)\n",
    "        return self._remove_xml_tags(output).strip()\n",
    "\n",
    "    def classify_phq8(self, conversation_history: List[Dict[str, str]], cumulative_summary: str) -> Dict:\n",
    "        recent_history = conversation_history[-10:] if len(conversation_history) > 10 else conversation_history\n",
    "        input_data = json.dumps({\"recent_context\": recent_history,\"cumulative_summary\": cumulative_summary})\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompts[\"Classification\"]},\n",
    "            {\"role\": \"user\", \"content\": input_data}\n",
    "        ]\n",
    "        output = self._generate(messages, max_new_tokens=1000)\n",
    "        json_str = self._extract_json_from_output(output)\n",
    "        try:\n",
    "            parsed = json.loads(json_str)\n",
    "            result = {\n",
    "                \"phq8_scores\": parsed.get(\"phq8_scores\", {f\"PHQ{i+1}\": \"Not explored\" for i in range(8)}),\n",
    "                \"depression_classification\": parsed.get(\"depression_classification\", \"Not depressed\"),\n",
    "                \"evidence_mapping\": parsed.get(\"evidence_mapping\", {f\"PHQ{i+1}\": \"no evidence\" for i in range(8)})\n",
    "            }\n",
    "        except json.JSONDecodeError:\n",
    "            result = {\n",
    "                \"phq8_scores\": {f\"PHQ{i+1}\": \"Not explored\" for i in range(8)},\n",
    "                \"depression_classification\": \"Not depressed\",\n",
    "                \"evidence_mapping\": {f\"PHQ{i+1}\": \"no evidence\" for i in range(8)}\n",
    "            }\n",
    "        return result\n",
    "\n",
    "    def _extract_json_from_output(self, output: str) -> str:\n",
    "        cleaned = self._remove_xml_tags(output).strip()\n",
    "        start_idx = cleaned.find('{')\n",
    "        end_idx = cleaned.rfind('}') + 1\n",
    "        return cleaned[start_idx:end_idx] if start_idx != -1 and end_idx != -1 else cleaned\n",
    "\n",
    "    def _remove_xml_tags(self, text: str) -> str:\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        cleaned = re.sub(r'<[^>]+>', '', text)\n",
    "        return re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "\n",
    "\n",
    "def parse_transcript(file_path: str) -> List[Dict[str, str]]:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    if 'speaker' not in df.columns or 'value' not in df.columns:\n",
    "        df['speaker'] = df.iloc[:, 0].str.extract(r'(\\w+)', expand=False)\n",
    "        df['value'] = df.iloc[:, 0].str.extract(r'\\w+ (.*)', expand=False)\n",
    "    history, current_text, current_speaker = [], [], None\n",
    "    for _, row in df.iterrows():\n",
    "        role = 'therapist' if 'ellie' in str(row['speaker']).lower() else 'participant'\n",
    "        if role != current_speaker:\n",
    "            if current_speaker:\n",
    "                history.append({'speaker_role': current_speaker, 'text': ' '.join(current_text)})\n",
    "            current_speaker, current_text = role, [str(row['value'])]\n",
    "        else:\n",
    "            current_text.append(str(row['value']))\n",
    "    if current_speaker:\n",
    "        history.append({'speaker_role': current_speaker, 'text': ' '.join(current_text)})\n",
    "    return history\n",
    "\n",
    "\n",
    "def map_severity_to_number(severity: str) -> int:\n",
    "    severity_map = {\n",
    "        \"Not explored\": 0,\n",
    "        \"Not at all\": 0,\n",
    "        \"Several days\": 1,\n",
    "        \"More than half the days\": 2,\n",
    "        \"Nearly every day\": 3\n",
    "    }\n",
    "    return severity_map.get(severity, 0)\n",
    "\n",
    "\n",
    "def evaluate_model(test_folder: str, label_csv: str, use_merged=False):\n",
    "    chatbot = TherapeuticChatbot(use_merged=use_merged)\n",
    "    labels_df = pd.read_csv(label_csv)\n",
    "    labels_df.columns = labels_df.columns.str.strip()\n",
    "    if 'PHQ_Binary' not in labels_df.columns or 'PHQ_Score' not in labels_df.columns:\n",
    "        labels_df.rename(columns={'PHQ8_Binary': 'PHQ_Binary', 'PHQ8_Score': 'PHQ_Score'}, inplace=True)\n",
    "\n",
    "    predictions_binary, true_binary, predictions_total, true_total, results = [], [], [], [], []\n",
    "\n",
    "    csv_files = [f for f in os.listdir(test_folder) if f.lower().endswith('.csv')]\n",
    "    for file_name in tqdm(csv_files, desc=\"Processing transcripts\"):\n",
    "        try:\n",
    "            pid = int(file_name.split('_')[0])\n",
    "        except ValueError:\n",
    "            continue\n",
    "        label_row = labels_df[labels_df['Participant_ID'] == pid]\n",
    "        if label_row.empty:\n",
    "            continue\n",
    "\n",
    "        true_binary_num = label_row['PHQ_Binary'].values[0]\n",
    "        true_label = \"Depressed\" if true_binary_num == 1 else \"Not depressed\"\n",
    "        true_total_score = label_row['PHQ_Score'].values[0]\n",
    "\n",
    "        file_path = os.path.join(test_folder, file_name)\n",
    "        conversation_history = parse_transcript(file_path)\n",
    "\n",
    "        summary = chatbot.summarize_conversation(conversation_history)\n",
    "        classification = chatbot.classify_phq8(conversation_history, summary)\n",
    "\n",
    "        predicted_label = classification['depression_classification']\n",
    "        predicted_phq_list = [map_severity_to_number(classification['phq8_scores'].get(f\"PHQ{i+1}\", \"Not explored\")) for i in range(8)]\n",
    "        predicted_total_score = sum(predicted_phq_list)\n",
    "\n",
    "        predictions_binary.append(predicted_label)\n",
    "        true_binary.append(true_label)\n",
    "        predictions_total.append(predicted_total_score)\n",
    "        true_total.append(true_total_score)\n",
    "\n",
    "        results.append({\n",
    "            \"Participant_ID\": pid,\n",
    "            \"Predicted_Binary\": 1 if predicted_label == \"Depressed\" else 0,\n",
    "            \"True_Binary\": true_binary_num,\n",
    "            \"Predicted_Score\": predicted_total_score,\n",
    "            \"True_Score\": true_total_score,\n",
    "            \"Raw_Classification_Output\": json.dumps(classification)\n",
    "        })\n",
    "\n",
    "    if results:\n",
    "        pd.DataFrame(results).to_csv(\"evaluation_results.csv\", index=False)\n",
    "\n",
    "    # Metrics\n",
    "    pred_binary_num = [1 if p == \"Depressed\" else 0 for p in predictions_binary]\n",
    "    true_binary_num = [1 if t == \"Depressed\" else 0 for t in true_binary]\n",
    "    accuracy_bin = accuracy_score(true_binary_num, pred_binary_num)\n",
    "    precision_bin = precision_score(true_binary_num, pred_binary_num, zero_division=0)\n",
    "    recall_bin = recall_score(true_binary_num, pred_binary_num, zero_division=0)\n",
    "    f1_bin = f1_score(true_binary_num, pred_binary_num, zero_division=0)\n",
    "    cm_bin = confusion_matrix(true_binary_num, pred_binary_num)\n",
    "\n",
    "    total_mae = np.mean(np.abs(np.array(predictions_total) - np.array(true_total)))\n",
    "\n",
    "    print(\"\\nBinary Depression Classification Results:\")\n",
    "    print(f\"Accuracy: {accuracy_bin:.4f}\")\n",
    "    print(f\"Precision: {precision_bin:.4f}\")\n",
    "    print(f\"Recall: {recall_bin:.4f}\")\n",
    "    print(f\"F1 Score: {f1_bin:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm_bin)\n",
    "\n",
    "    print(\"\\nPHQ-8 Total Score Results:\")\n",
    "    print(f\"Total PHQ-8 Score MAE: {total_mae:.4f}\")\n",
    "\n",
    "    metrics = {\n",
    "        \"binary_classification\": {\n",
    "            \"accuracy\": accuracy_bin,\n",
    "            \"precision\": precision_bin,\n",
    "            \"recall\": recall_bin,\n",
    "            \"f1_score\": f1_bin,\n",
    "            \"confusion_matrix\": cm_bin.tolist()\n",
    "        },\n",
    "        \"phq8_score\": {\"MAE\": total_mae}\n",
    "    }\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    with open(f\"evaluation_metrics_{timestamp}.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    # --- Visualization Section ---\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm_bin, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=[\"Not Depressed\", \"Depressed\"],\n",
    "                yticklabels=[\"Not Depressed\", \"Depressed\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Binary Depression Classification Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"confusion_matrix_{timestamp}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(true_total, predictions_total, alpha=0.6)\n",
    "    plt.plot([0, max(true_total)], [0, max(true_total)], color=\"red\", linestyle=\"--\")\n",
    "    plt.xlabel(\"True PHQ-8 Score\")\n",
    "    plt.ylabel(\"Predicted PHQ-8 Score\")\n",
    "    plt.title(\"PHQ-8 True vs Predicted Scores\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"phq8_true_vs_pred_{timestamp}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"\\nMetrics and plots saved with timestamp {timestamp}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_model(\n",
    "        test_folder=\"Extracted_Text_Transcript_DAIC\",\n",
    "        label_csv=\"full_test_split.csv\",\n",
    "        use_merged=False\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
