{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff8fbba1-1479-4961-bd48-03c76fde6f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Analyzed strategy for transition at turn 175\n",
      "INFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Analyzed strategy for transition at turn 177\n",
      "INFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Analyzed strategy for transition at turn 179\n",
      "INFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Analyzed strategy for transition at turn 181\n",
      "INFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Analyzed strategy for transition at turn 183\n",
      "INFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Analyzed strategy for transition at turn 185\n",
      "INFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Analyzed strategy for transition at turn 187\n",
      "INFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Analyzed strategy for transition at turn 189\n",
      "INFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Analyzed strategy for transition at turn 191\n",
      "INFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Analyzed strategy for transition at turn 195\n",
      "INFO:__main__:Analyzed strategy for transition at turn 197\n",
      "INFO:__main__:Analyzed strategy for transition at turn 193\n",
      "INFO:__main__:Creating aligned training samples...\n",
      "INFO:__main__:Training data saved to: simplified_training_data_320.json\n",
      "INFO:__main__:Exported 99 samples to simplified_training_data_320\\summary_module_samples.jsonl\n",
      "INFO:__main__:Exported 99 samples to simplified_training_data_320\\classification_module_samples.jsonl\n",
      "INFO:__main__:Exported 99 samples to simplified_training_data_320\\response_generation_samples.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Processed participant 320\n",
      "Final depression label: Depressed\n",
      "Total transitions: 99\n",
      "\n",
      "ðŸŽ¯ All transcripts processed.\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 114\n",
      "Total transition points: 56\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 67\n",
      "Total transition points: 33\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 74\n",
      "Total transition points: 36\n",
      "Final depression classification: Depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 70\n",
      "Total transition points: 34\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 73\n",
      "Total transition points: 36\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 75\n",
      "Total transition points: 37\n",
      "Final depression classification: Depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 80\n",
      "Total transition points: 40\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 90\n",
      "Total transition points: 44\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 80\n",
      "Total transition points: 40\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 89\n",
      "Total transition points: 44\n",
      "Final depression classification: Depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 88\n",
      "Total transition points: 44\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 85\n",
      "Total transition points: 42\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 91\n",
      "Total transition points: 45\n",
      "Final depression classification: Depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 88\n",
      "Total transition points: 43\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 85\n",
      "Total transition points: 42\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 89\n",
      "Total transition points: 44\n",
      "Final depression classification: Depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 87\n",
      "Total transition points: 43\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 93\n",
      "Total transition points: 46\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 96\n",
      "Total transition points: 47\n",
      "Final depression classification: Depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 91\n",
      "Total transition points: 45\n",
      "Final depression classification: Depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 99\n",
      "Total transition points: 49\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 102\n",
      "Total transition points: 50\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 99\n",
      "Total transition points: 49\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 90\n",
      "Total transition points: 45\n",
      "Final depression classification: Depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 93\n",
      "Total transition points: 46\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 99\n",
      "Total transition points: 49\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 103\n",
      "Total transition points: 51\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 102\n",
      "Total transition points: 50\n",
      "Final depression classification: Depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 101\n",
      "Total transition points: 50\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 100\n",
      "Total transition points: 49\n",
      "Final depression classification: Depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 92\n",
      "Total transition points: 46\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 102\n",
      "Total transition points: 50\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 101\n",
      "Total transition points: 50\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 105\n",
      "Total transition points: 52\n",
      "Final depression classification: Depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 107\n",
      "Total transition points: 53\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 91\n",
      "Total transition points: 45\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 109\n",
      "Total transition points: 54\n",
      "Final depression classification: Depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 103\n",
      "Total transition points: 51\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 112\n",
      "Total transition points: 56\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 112\n",
      "Total transition points: 55\n",
      "Final depression classification: Depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 109\n",
      "Total transition points: 54\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 105\n",
      "Total transition points: 52\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 103\n",
      "Total transition points: 51\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 106\n",
      "Total transition points: 53\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 110\n",
      "Total transition points: 55\n",
      "Final depression classification: Depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 111\n",
      "Total transition points: 55\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 111\n",
      "Total transition points: 55\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 111\n",
      "Total transition points: 55\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 114\n",
      "Total transition points: 56\n",
      "Final depression classification: Depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 116\n",
      "Total transition points: 57\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 117\n",
      "Total transition points: 58\n",
      "Final depression classification: Depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 106\n",
      "Total transition points: 53\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 114\n",
      "Total transition points: 57\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 122\n",
      "Total transition points: 60\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 116\n",
      "Total transition points: 57\n",
      "Final depression classification: Depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 112\n",
      "Total transition points: 56\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 111\n",
      "Total transition points: 55\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 118\n",
      "Total transition points: 58\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 114\n",
      "Total transition points: 57\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 124\n",
      "Total transition points: 61\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 117\n",
      "Total transition points: 58\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 120\n",
      "Total transition points: 59\n",
      "Final depression classification: Depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 121\n",
      "Total transition points: 60\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 121\n",
      "Total transition points: 60\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 115\n",
      "Total transition points: 57\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 124\n",
      "Total transition points: 61\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 116\n",
      "Total transition points: 58\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 121\n",
      "Total transition points: 60\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 121\n",
      "Total transition points: 60\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 124\n",
      "Total transition points: 62\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 117\n",
      "Total transition points: 58\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 119\n",
      "Total transition points: 59\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 115\n",
      "Total transition points: 57\n",
      "Final depression classification: Depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 123\n",
      "Total transition points: 61\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 123\n",
      "Total transition points: 61\n",
      "Final depression classification: Depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 123\n",
      "Total transition points: 61\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 127\n",
      "Total transition points: 63\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 130\n",
      "Total transition points: 64\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 136\n",
      "Total transition points: 67\n",
      "Final depression classification: Depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 140\n",
      "Total transition points: 69\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 146\n",
      "Total transition points: 72\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 135\n",
      "Total transition points: 67\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 136\n",
      "Total transition points: 67\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 129\n",
      "Total transition points: 64\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 136\n",
      "Total transition points: 67\n",
      "Final depression classification: Depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 137\n",
      "Total transition points: 68\n",
      "Final depression classification: Depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 148\n",
      "Total transition points: 73\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 149\n",
      "Total transition points: 74\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 136\n",
      "Total transition points: 68\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 150\n",
      "Total transition points: 74\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 145\n",
      "Total transition points: 72\n",
      "Final depression classification: Depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 141\n",
      "Total transition points: 70\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 150\n",
      "Total transition points: 74\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 145\n",
      "Total transition points: 72\n",
      "Final depression classification: Depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 142\n",
      "Total transition points: 71\n",
      "Final depression classification: Depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 148\n",
      "Total transition points: 73\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 154\n",
      "Total transition points: 76\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 147\n",
      "Total transition points: 73\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 145\n",
      "Total transition points: 72\n",
      "Final depression classification: Depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 160\n",
      "Total transition points: 79\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 159\n",
      "Total transition points: 79\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 168\n",
      "Total transition points: 83\n",
      "Final depression classification: Depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 164\n",
      "Total transition points: 81\n",
      "Final depression classification: Depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 176\n",
      "Total transition points: 87\n",
      "Final depression classification: Not depressed\n",
      "\n",
      "Successfully generated simplified training data!\n",
      "Total turns processed: 199\n",
      "Total transition points: 99\n",
      "Final depression classification: Depressed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import concurrent.futures\n",
    "import pickle  # For checkpointing\n",
    "import openai  # Assuming DeepSeek uses OpenAI-compatible client\n",
    "import time\n",
    "import requests\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class APIConfig:\n",
    "    \"\"\"Configuration for different API providers\"\"\"\n",
    "    provider: str  # 'deepseek'\n",
    "    api_key: str\n",
    "    base_url: Optional[str] = None\n",
    "    model: str = \"DeepSeek-R1-0528\"  # Updated to your specified model\n",
    "    max_tokens: int = 1500\n",
    "    temperature: float = 0.2  # Low to reduce hallucinations\n",
    "    timeout: int = 30\n",
    "    rate_limit_delay: float = 0.5  # seconds between requests\n",
    "\n",
    "'''\n",
    "class APIClient:\n",
    "    \"\"\"Generic API client for DeepSeek\"\"\"\n",
    "    \n",
    "    def __init__(self, config: APIConfig):\n",
    "        self.config = config\n",
    "        self.client = openai.OpenAI(\n",
    "            api_key=config.api_key,\n",
    "            base_url=config.base_url or \"https://api.deepseek.com\"\n",
    "        )\n",
    "    \n",
    "    def generate_completion(self, prompt: str, max_tokens: Optional[int] = None) -> str:\n",
    "        \"\"\"Generate completion using the configured API\"\"\"\n",
    "        max_tokens = max_tokens or self.config.max_tokens\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.config.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant specialized in therapeutic conversation analysis. Stick strictly to the user's instructions without adding external knowledge or inferring unstated information.\"},  # Added to reduce hallucinations\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=self.config.temperature,\n",
    "                timeout=self.config.timeout\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"API request failed: {e}\")\n",
    "            time.sleep(self.config.rate_limit_delay * 2)\n",
    "            raise\n",
    "        \n",
    "        finally:\n",
    "            time.sleep(self.config.rate_limit_delay)\n",
    "'''\n",
    "\n",
    "class APIClient:\n",
    "    \"\"\"Generic API client for DeepSeek\"\"\"\n",
    "    \n",
    "    def __init__(self, config: APIConfig):\n",
    "        self.config = config\n",
    "        self.client = openai.OpenAI(\n",
    "            api_key=config.api_key,\n",
    "            base_url=config.base_url or \"https://api.deepseek.com\"\n",
    "        )\n",
    "    \n",
    "    def generate_completion(self, prompt: str, max_tokens: Optional[int] = None) -> str:\n",
    "        \"\"\"Generate completion using the configured API\"\"\"\n",
    "        max_tokens = max_tokens or self.config.max_tokens\n",
    "        \n",
    "        try:\n",
    "            # For deepseek-reasoner, remove temperature parameter\n",
    "            params = {\n",
    "                \"model\": self.config.model,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant specialized in therapeutic conversation analysis. Stick strictly to the user's instructions without adding external knowledge or inferring unstated information.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"timeout\": self.config.timeout\n",
    "            }\n",
    "            \n",
    "            # Only add temperature for non-reasoner models\n",
    "            if \"reasoner\" not in self.config.model.lower():\n",
    "                params[\"temperature\"] = self.config.temperature\n",
    "                \n",
    "            response = self.client.chat.completions.create(**params)\n",
    "            \n",
    "            # Handle different response structures\n",
    "            message = response.choices[0].message\n",
    "            \n",
    "            # For deepseek-reasoner, it has both reasoning_content and content\n",
    "            if hasattr(message, 'reasoning_content') and message.reasoning_content:\n",
    "                # You can choose to use reasoning_content, content, or both\n",
    "                # For debugging, you might want to log the reasoning_content\n",
    "                logger.info(f\"Reasoning: {message.reasoning_content[:200]}...\")\n",
    "                return message.content.strip()  # Return the final answer\n",
    "            else:\n",
    "                return message.content.strip()\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"API request failed: {e}\")\n",
    "            time.sleep(self.config.rate_limit_delay * 2)\n",
    "            raise\n",
    "        \n",
    "        finally:\n",
    "            time.sleep(self.config.rate_limit_delay)\n",
    "\n",
    "    def test_api_connection(self):\n",
    "        \"\"\"Test API connection and basic functionality\"\"\"\n",
    "        test_prompt = \"Say 'test successful' briefly.\"\n",
    "        \n",
    "        try:\n",
    "            response = self.generate_completion(test_prompt, max_tokens=20)\n",
    "            logger.info(f\"API Test Response: {response}\")\n",
    "            # More lenient check\n",
    "            if response and len(response.strip()) > 0:\n",
    "                logger.info(\"API connection validated successfully\")\n",
    "                return True\n",
    "            else:\n",
    "                logger.warning(\"API responded with empty content\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"API Test Failed: {e}\")\n",
    "            return False\n",
    "            \n",
    "class ImprovedTrainingDataGenerator:\n",
    "    def __init__(self, api_config: APIConfig):\n",
    "        self.api_client = APIClient(api_config)\n",
    "        self.phq8_questions = {\n",
    "            \"PHQ1\": \"Little interest or pleasure in doing things\",\n",
    "            \"PHQ2\": \"Feeling down, depressed, or hopeless\",\n",
    "            \"PHQ3\": \"Trouble falling or staying asleep, or sleeping too much\",\n",
    "            \"PHQ4\": \"Feeling tired or having little energy\",\n",
    "            \"PHQ5\": \"Poor appetite or overeating\",\n",
    "            \"PHQ6\": \"Feeling bad about yourself or that you are a failure\",\n",
    "            \"PHQ7\": \"Trouble concentrating on things\",\n",
    "            \"PHQ8\": \"Moving or speaking slowly or being fidgety/restless\"\n",
    "        }\n",
    "        self.phq8_severity_levels = [\n",
    "            \"Not explored\", \"Not at all\", \"Several days\", \"More than half the days\", \"Nearly every day\"\n",
    "        ]\n",
    "        self.depression_classifications = [\"Depressed\", \"Not depressed\"]\n",
    "        self.emotion_tags = [\n",
    "            \"Empathy\", \"Neutral/Supportive\", \"Probing\", \"Encouraging\", \"Reassuring\", \"Clarifying\"]\n",
    "        self.response_strategies = [\n",
    "            \"Continue PHQ Assessment\", \"Explore Depression Indicators\",\n",
    "            \"Provide Emotional Support\", \"Gather Context\", \"Redirect Conversation\",\n",
    "            \"Build Rapport\", \"Validate Feelings\", \"Ask Follow-up Questions\",\"Ending conversation\"\n",
    "        ]\n",
    "\n",
    "    def parse_single_column_csv(self, file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Parse single column CSV format with combined timestamp_speaker_text\"\"\"\n",
    "        logger.info(f\"Loading single-column CSV: {file_path}\")\n",
    "        \n",
    "        df = pd.read_csv(file_path, header=None)\n",
    "        \n",
    "        if df.iloc[0, 0] == 'start_timestop_timespeakervalue' or 'start_timestop' in str(df.iloc[0, 0]).lower():\n",
    "            df = df.iloc[1:].reset_index(drop=True)\n",
    "        \n",
    "        parsed_data = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            raw_text = str(row[0]).strip()\n",
    "            \n",
    "            if not raw_text or raw_text == 'nan':\n",
    "                continue\n",
    "                \n",
    "            pattern = r'^(\\d+\\.\\d+)(\\d+\\.\\d+)([A-Za-z]+)(.*)'\n",
    "            match = re.match(pattern, raw_text)\n",
    "            \n",
    "            if match:\n",
    "                start_time = float(match.group(1))\n",
    "                end_time = float(match.group(2))  \n",
    "                speaker = match.group(3)\n",
    "                text = match.group(4).strip()\n",
    "                timestamp = start_time\n",
    "                \n",
    "            else:\n",
    "                pattern2 = r'^(\\d+\\.\\d+)([A-Za-z]+)(.*)'\n",
    "                match2 = re.match(pattern2, raw_text)\n",
    "                \n",
    "                if match2:\n",
    "                    timestamp = float(match2.group(1))\n",
    "                    speaker = match2.group(2)\n",
    "                    text = match2.group(3).strip()\n",
    "                else:\n",
    "                    logger.warning(f\"Could not parse line {idx}: {raw_text[:50]}...\")\n",
    "                    timestamp = idx\n",
    "                    speaker = \"Unknown\"\n",
    "                    text = raw_text\n",
    "            \n",
    "            speaker = speaker.strip()\n",
    "            if speaker.lower() in ['ellie', 'elle', 'eli']:\n",
    "                speaker = 'Ellie'\n",
    "            elif speaker.lower() in ['participant', 'p', 'user']:\n",
    "                speaker = 'Participant'\n",
    "            \n",
    "            parsed_data.append({\n",
    "                'timestamp': timestamp,\n",
    "                'speaker': speaker,\n",
    "                'text': text\n",
    "            })\n",
    "        \n",
    "        parsed_df = pd.DataFrame(parsed_data)\n",
    "        parsed_df = parsed_df[parsed_df['text'].str.len() > 0]\n",
    "        parsed_df = parsed_df.sort_values('timestamp').reset_index(drop=True)\n",
    "        \n",
    "        logger.info(f\"Parsed {len(parsed_df)} individual utterances from single-column CSV\")\n",
    "        logger.info(f\"Speakers found: {parsed_df['speaker'].unique()}\")\n",
    "        \n",
    "        return parsed_df\n",
    "\n",
    "    def load_transcript_data(self, file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Load and preprocess transcript data with proper column handling for multiple formats\"\"\"\n",
    "        logger.info(f\"Detecting CSV format for: {file_path}\")\n",
    "        \n",
    "        separators = [',', '\\t', ';', '|']\n",
    "        df_test = None\n",
    "        separator_used = None\n",
    "        \n",
    "        for sep in separators:\n",
    "            try:\n",
    "                df_test = pd.read_csv(file_path, sep=sep, nrows=5)\n",
    "                if df_test.shape[1] >= 3:\n",
    "                    separator_used = sep\n",
    "                    logger.info(f\"Detected {df_test.shape[1]} columns with separator: '{sep}'\")\n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if df_test is None or df_test.shape[1] < 3:\n",
    "            logger.info(\"Falling back to single column parsing\")\n",
    "            df = self.parse_single_column_csv(file_path)\n",
    "        else:\n",
    "            logger.info(f\"Processing as multi-column format with separator '{separator_used}'\")\n",
    "            df = pd.read_csv(file_path, sep=separator_used)\n",
    "            \n",
    "            if df.shape[1] == 4:\n",
    "                df.columns = ['start_time', 'stop_time', 'speaker', 'text']\n",
    "                df['timestamp'] = df['start_time']\n",
    "            elif df.shape[1] >= 3:\n",
    "                df.columns = ['timestamp', 'speaker', 'text'] + [f'extra_{i}' for i in range(df.shape[1] - 3)]\n",
    "            \n",
    "            df['timestamp'] = pd.to_numeric(df['timestamp'], errors='coerce')\n",
    "            df['speaker'] = df['speaker'].astype(str).str.strip()\n",
    "            df['text'] = df['text'].astype(str).str.strip()\n",
    "            \n",
    "            df = df.dropna(subset=['timestamp', 'speaker', 'text'])\n",
    "            df = df[df['text'] != '']\n",
    "            \n",
    "            if df.iloc[0]['speaker'].lower() in ['speaker', 'ellie', 'participant']:\n",
    "                if any(word in str(df.iloc[0]['text']).lower() for word in ['value', 'text', 'transcript']):\n",
    "                    df = df.iloc[1:].reset_index(drop=True)\n",
    "            \n",
    "            df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "            df = df[['timestamp', 'speaker', 'text']].copy()\n",
    "        \n",
    "        logger.info(f\"Successfully loaded {len(df)} individual utterances\")\n",
    "        return df\n",
    "\n",
    "    def group_by_speaker_turns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Group consecutive utterances by the same speaker into single turns\"\"\"\n",
    "        logger.info(\"Grouping consecutive utterances by speaker into turns...\")\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            return df\n",
    "        \n",
    "        turns = []\n",
    "        current_speaker = None\n",
    "        current_texts = []\n",
    "        current_timestamps = []\n",
    "        turn_id = 0\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            speaker = row['speaker']\n",
    "            text = row['text']\n",
    "            timestamp = row['timestamp']\n",
    "            \n",
    "            if speaker != current_speaker:\n",
    "                if current_speaker is not None and current_texts:\n",
    "                    combined_text = ' '.join(current_texts)\n",
    "                    start_timestamp = min(current_timestamps)\n",
    "                    end_timestamp = max(current_timestamps)\n",
    "                    \n",
    "                    turns.append({\n",
    "                        'turn_id': turn_id,\n",
    "                        'timestamp': start_timestamp,\n",
    "                        'end_timestamp': end_timestamp,\n",
    "                        'speaker': current_speaker,\n",
    "                        'text': combined_text,\n",
    "                        'utterance_count': len(current_texts),\n",
    "                        'duration': end_timestamp - start_timestamp\n",
    "                    })\n",
    "                    turn_id += 1\n",
    "                \n",
    "                current_speaker = speaker\n",
    "                current_texts = [text]\n",
    "                current_timestamps = [timestamp]\n",
    "            else:\n",
    "                current_texts.append(text)\n",
    "                current_timestamps.append(timestamp)\n",
    "        \n",
    "        if current_speaker is not None and current_texts:\n",
    "            combined_text = ' '.join(current_texts)\n",
    "            start_timestamp = min(current_timestamps)\n",
    "            end_timestamp = max(current_timestamps)\n",
    "            \n",
    "            turns.append({\n",
    "                'turn_id': turn_id,\n",
    "                'timestamp': start_timestamp,\n",
    "                'end_timestamp': end_timestamp,\n",
    "                'speaker': current_speaker,\n",
    "                'text': combined_text,\n",
    "                'utterance_count': len(current_texts),\n",
    "                'duration': end_timestamp - start_timestamp\n",
    "            })\n",
    "        \n",
    "        turns_df = pd.DataFrame(turns)\n",
    "        logger.info(f\"Grouped {len(df)} utterances into {len(turns_df)} speaker turns\")\n",
    "        \n",
    "        if not turns_df.empty:\n",
    "            speaker_counts = turns_df['speaker'].value_counts()\n",
    "            logger.info(\"Turn distribution by speaker:\")\n",
    "            for speaker, count in speaker_counts.items():\n",
    "                logger.info(f\"  {speaker}: {count} turns\")\n",
    "        \n",
    "        return turns_df\n",
    "\n",
    "    def extract_conversation_turns(self, df: pd.DataFrame) -> List[Dict]:\n",
    "        \"\"\"Extract conversation turns with proper speaker identification and grouping\"\"\"\n",
    "        grouped_df = self.group_by_speaker_turns(df)\n",
    "        \n",
    "        turns = []\n",
    "        \n",
    "        for idx, row in grouped_df.iterrows():\n",
    "            speaker = row['speaker'].lower()\n",
    "            if 'ellie' in speaker:\n",
    "                speaker_role = 'therapist'\n",
    "            elif 'participant' in speaker:\n",
    "                speaker_role = 'participant'\n",
    "            else:\n",
    "                speaker_role = 'unknown'\n",
    "            \n",
    "            turn = {\n",
    "                'turn_id': row['turn_id'],\n",
    "                'timestamp': row['timestamp'],\n",
    "                'end_timestamp': row.get('end_timestamp', row['timestamp']),\n",
    "                'speaker': row['speaker'],\n",
    "                'speaker_role': speaker_role,\n",
    "                'text': row['text'],\n",
    "                'text_length': len(row['text'].split()),\n",
    "                'utterance_count': row.get('utterance_count', 1),\n",
    "                'duration': row.get('duration', 0.0)\n",
    "            }\n",
    "            turns.append(turn)\n",
    "        \n",
    "        logger.info(f\"Extracted {len(turns)} conversation turns with speaker-based switching\")\n",
    "        return turns\n",
    "\n",
    "    def identify_transition_points(self, turns: List[Dict]) -> List[int]:\n",
    "        \"\"\"Identify transition points only from participant to therapist\"\"\"\n",
    "        transition_points = []\n",
    "        \n",
    "        for i in range(len(turns) - 1):\n",
    "            current_speaker = turns[i]['speaker_role']\n",
    "            next_speaker = turns[i + 1]['speaker_role']\n",
    "            \n",
    "            if current_speaker == 'participant' and next_speaker == 'therapist':\n",
    "                transition_points.append(i + 1)  # Point to the therapist's turn\n",
    "        \n",
    "        logger.info(f\"Identified {len(transition_points)} participant-to-therapist transition points for training data generation\")\n",
    "        return transition_points\n",
    "\n",
    "    def generate_single_summary(self, turns: List[Dict], transition_idx: int) -> Dict:\n",
    "        \"\"\"Helper to generate a single summary at a transition point\"\"\"\n",
    "        context_turns = turns[:transition_idx]\n",
    "        \n",
    "        context_text = \"\\n\".join([\n",
    "            f\"{t['speaker_role']}: {t['text']}\" for t in context_turns\n",
    "        ])\n",
    "        #print('summary')\n",
    "        #print(context_text)\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "Task: Generate a cumulative summary of the therapeutic conversation up to this transition point.\n",
    "\n",
    "Step-by-Step Instructions:\n",
    "1. Read the conversation carefully.\n",
    "2. Identify only explicit emotional expressions, mood indicators, and depression-related symptoms mentioned.\n",
    "3. Note all PHQ-8 related content if directly discussed (e.g., interest, mood, sleep, energy, appetite, self-worth, concentration, psychomotor changes). List evidence for each explored symptom explicitly, even if minor. Do not infer or omit any.\n",
    "4. Focus solely on clinically relevant information; ignore casual talk.\n",
    "5. Build on previous information without repetition. Base ONLY on the provided conversationâ€”do not add external knowledge or assumptions.\n",
    "\n",
    "Conversation so far:\n",
    "{context_text}\n",
    "\n",
    "Output a concise yet comprehensive summary no more than 2 short paragraphs or 6â€“8 sentences , capturing:\n",
    "- Key emotional indicators (cite direct quotes).\n",
    "- All depression symptoms discussed, with explicit mapping to PHQ-8 questions (if any).\n",
    "- Participant's current state/mood (based on evidence).\n",
    "- Therapist's assessment approach (if evident).\n",
    "- Important quotes or expressions.\n",
    "\n",
    "Summary:\"\"\"  # Optimized: Added step-by-step, grounding, citation requirement\n",
    "\n",
    "        try:\n",
    "            response = self.api_client.generate_completion(prompt, max_tokens=1000)\n",
    "            summary_text = response.strip()\n",
    "            \n",
    "            return {\n",
    "                'transition_turn_id': transition_idx,\n",
    "                'cumulative_summary': summary_text,\n",
    "                'conversation_length': len(context_turns),\n",
    "                'context_turns_count': len(context_turns)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating summary for transition at turn {transition_idx}: {e}\")\n",
    "            return {\n",
    "                'transition_turn_id': transition_idx,\n",
    "                'cumulative_summary': \"Summary generation failed\",\n",
    "                'conversation_length': len(context_turns),\n",
    "                'context_turns_count': len(context_turns)\n",
    "            }\n",
    "\n",
    "    def generate_summaries_at_transitions(self, turns: List[Dict], transition_points: List[int]) -> List[Dict]:\n",
    "        \"\"\"Generate summaries only at speaker transition points, parallelized\"\"\"\n",
    "        summaries = []\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:  # Capped to 3 for rate limits\n",
    "            future_to_idx = {executor.submit(self.generate_single_summary, turns, idx): idx for idx in transition_points}\n",
    "            \n",
    "            for future in concurrent.futures.as_completed(future_to_idx):\n",
    "                idx = future_to_idx[future]\n",
    "                try:\n",
    "                    summary = future.result()\n",
    "                    summaries.append(summary)\n",
    "                    logger.info(f\"Generated summary for transition at turn {idx}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error in parallel summary generation for turn {idx}: {e}\")\n",
    "                    summaries.append({\n",
    "                        'transition_turn_id': idx,\n",
    "                        'cumulative_summary': \"Summary generation failed\",\n",
    "                        'conversation_length': 0,\n",
    "                        'context_turns_count': 0\n",
    "                    })\n",
    "        \n",
    "        summaries.sort(key=lambda x: x['transition_turn_id'])\n",
    "        \n",
    "        return summaries\n",
    "\n",
    "    def generate_single_strategy(self, turns: List[Dict], transition_idx: int, classifications: List[Dict], i: int, summaries: List[Dict]) -> Dict:\n",
    "        \"\"\"Helper to generate a single response strategy at a transition point\"\"\"\n",
    "        before_turn = turns[transition_idx - 1]  # Previous speaker's turn\n",
    "        after_turn = turns[transition_idx] if transition_idx < len(turns) else None\n",
    "        \n",
    "        if after_turn is None:\n",
    "            return None\n",
    "            \n",
    "        if (before_turn['speaker_role'] == 'participant' and \n",
    "            after_turn['speaker_role'] == 'therapist'):\n",
    "            \n",
    "            current_classification = classifications[i] if i < len(classifications) else {}\n",
    "            current_summary = summaries[i]['cumulative_summary'] if i < len(summaries) else \"\"\n",
    "            \n",
    "            # Use full conversation history up to the point (excluding current therapist response) for accuracy\n",
    "            full_history = \"\\n\".join([\n",
    "                f\"{t['speaker_role']}: {t['text']}\" for t in turns[:transition_idx]\n",
    "            ])\n",
    "\n",
    "            \n",
    "            prompt = f\"\"\"\n",
    "    Task: Analyze the therapeutic response strategy used at this speaker transition.\n",
    "    \n",
    "    Step-by-Step Instructions:\n",
    "    1. Read the full conversation history up to the participant's statement, therapist's response, and cumulative summary (derived from the full conversation history).\n",
    "    2. Identify the strategy based ONLY on the response's content (e.g., if asking questions, it's \"Ask Follow-up Questions\").\n",
    "    3. Choose the best emotion tag that matches the tone, using the suggestions as a starting point but generating a new one if the context requires a more appropriate fit.\n",
    "    4. Describe the intent concisely, based on evidence in the response and broader context from the summary and history. Do not infer unstated goals.\n",
    "    \n",
    "    Cumulative summary (derived from full conversation): {current_summary}\n",
    "    \n",
    "    Full conversation history up to participant: \n",
    "    {full_history}\n",
    "    \n",
    "    Participant said: \"{before_turn['text']}\"\n",
    "    \n",
    "    Therapist responded: \"{after_turn['text']}\"\n",
    "    \n",
    "    Current Assessment State: {json.dumps(current_classification.get('phq8_scores', {}), indent=2)}\n",
    "    \n",
    "    Reference Strategies (use as inspiration; generate a new concise 1-2 word description if none fit the context perfectly): \n",
    "    {self.response_strategies}\n",
    "    \n",
    "    Suggested Emotion Tags (choose one from the list or generate a new one based on context if more appropriate): {self.emotion_tags}\n",
    "    \n",
    "    Output ONLY the JSON object in this EXACT format. No additional text:\n",
    "    {{\n",
    "      \"strategy_used\": \"concise 1-2 word strategy description\",\n",
    "      \"emotion_tag\": \"emotion tag (from suggestions or context-based)\", \n",
    "      \"response_intent\": \"1-sentence description of what the therapist was trying to achieve, based on evidence\"\n",
    "    }}\"\"\"\n",
    "            \n",
    "            try:\n",
    "                response = self.api_client.generate_completion(prompt, max_tokens=300)\n",
    "                \n",
    "                json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "                if json_match:\n",
    "                    strategy_data = json.loads(json_match.group())\n",
    "                    strategy_data.update({\n",
    "                        'transition_turn_id': transition_idx,\n",
    "                        'participant_turn_id': transition_idx - 1,\n",
    "                        'therapist_turn_id': transition_idx,\n",
    "                        'participant_text': before_turn['text'],\n",
    "                        'therapist_response': after_turn['text']\n",
    "                    })\n",
    "                    return strategy_data\n",
    "                else:\n",
    "                    return self.create_default_strategy_at_transition(transition_idx, before_turn, after_turn)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error analyzing strategy for transition at turn {transition_idx}: {e}\")\n",
    "                return self.create_default_strategy_at_transition(transition_idx, before_turn, after_turn)\n",
    "        return None\n",
    "\n",
    "    def generate_classifications_at_transitions(self, turns: List[Dict], transition_points: List[int], final_phq_scores: Dict, final_depression_label: str, summaries: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Generate classifications only at speaker transition points with final label validation, parallelized\"\"\"\n",
    "        classifications = []\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "            future_to_idx = {executor.submit(self.generate_single_classification, turns, transition_points[i], final_phq_scores, final_depression_label, summaries[i]['cumulative_summary']): transition_points[i] for i in range(len(transition_points))}\n",
    "            \n",
    "            for future in concurrent.futures.as_completed(future_to_idx):\n",
    "                idx = future_to_idx[future]\n",
    "                try:\n",
    "                    classification = future.result()\n",
    "                    classifications.append(classification)\n",
    "                    logger.info(f\"Generated classification for transition at turn {idx}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error in parallel classification generation for turn {idx}: {e}\")\n",
    "                    classifications.append(self.create_default_classification_at_transition({'transition_turn_id': idx}))\n",
    "        \n",
    "        classifications.sort(key=lambda x: x['transition_turn_id'])\n",
    "        \n",
    "        if final_phq_scores and final_depression_label and classifications:\n",
    "            classifications = self.align_with_ground_truth_flexible(\n",
    "                classifications, final_phq_scores, final_depression_label\n",
    "            )\n",
    "        \n",
    "        return classifications\n",
    "\n",
    "    def generate_single_classification(self, turns: List[Dict], transition_idx: int, final_phq_scores: Dict, final_depression_label: str, current_summary: str) -> Dict:\n",
    "        \"\"\"Helper to generate a single classification at a transition point using summary + full history up to point\"\"\"\n",
    "        full_turns = turns[:transition_idx]  # Full up to transition, excluding response\n",
    "        full_text = \"\\n\".join([f\"{t['speaker_role']}: {t['text']}\" for t in full_turns])\n",
    "        #print('classify')\n",
    "        #print(full_text)\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "    Task: Generate PHQ-8 classifications based on the cumulative summary and full conversation up to this transition point.\n",
    "    \n",
    "    Step-by-Step Instructions:\n",
    "    1. Read the cumulative summary and full conversation carefully and focus ONLY on the participant's sentiments, emotional expressions, and direct statements. Ignore therapist's parts unless they explicitly quote or reflect the participant's sentiment.\n",
    "    2. Identify explicit evidence for each PHQ-8 question from the participant's sentiment-related content.\n",
    "    3. Map only direct evidence to severity levels. Be conservative: If no explicit evidence in the participant's sentiments, use \"Not explored\". Do not infer, hallucinate, or use therapist's interpretations.\n",
    "    4. For depression classification, assess based solely on mapped severities from participant sentiments (e.g., if multiple high severities in sentiments, classify as \"Depressed\").\n",
    "    5. Provide evidence as direct quotes/phrases from the participant's sentiments or \"no evidence\".\n",
    "    \n",
    "    Example:\n",
    "    If conversation says \"Participant: I feel down every day\", map PHQ2 to \"Nearly every day\" with evidence \"I feel down every day\".\n",
    "    \n",
    "    PHQ-8 Questions:\n",
    "    {json.dumps(self.phq8_questions, indent=2)}\n",
    "    \n",
    "    Severity Levels: {self.phq8_severity_levels}\n",
    "    \n",
    "    Cumulative summary (derived from full conversation history): {current_summary}\n",
    "    \n",
    "    Full conversation up to transition: {full_text}\n",
    "    \n",
    "    Output ONLY the JSON object in this EXACT format. No additional text:\n",
    "    {{\n",
    "      \"phq8_scores\": {{\n",
    "        \"PHQ1\": \"severity_level\",\n",
    "        \"PHQ2\": \"severity_level\",\n",
    "        \"PHQ3\": \"severity_level\",\n",
    "        \"PHQ4\": \"severity_level\",\n",
    "        \"PHQ5\": \"severity_level\",\n",
    "        \"PHQ6\": \"severity_level\",\n",
    "        \"PHQ7\": \"severity_level\",\n",
    "        \"PHQ8\": \"severity_level\"\n",
    "      }},\n",
    "      \"depression_classification\": \"Depressed\" or \"Not depressed\",\n",
    "      \"evidence_mapping\": {{\n",
    "        \"PHQ1\": \"evidence from participant's sentiments or 'no evidence'\",\n",
    "        \"PHQ2\": \"evidence from participant's sentiments or 'no evidence'\",\n",
    "        \"PHQ3\": \"evidence from participant's sentiments or 'no evidence'\",\n",
    "        \"PHQ4\": \"evidence from participant's sentiments or 'no evidence'\",\n",
    "        \"PHQ5\": \"evidence from participant's sentiments or 'no evidence'\",\n",
    "        \"PHQ6\": \"evidence from participant's sentiments or 'no evidence'\",\n",
    "        \"PHQ7\": \"evidence from participant's sentiments or 'no evidence'\",\n",
    "        \"PHQ8\": \"evidence from participant's sentiments or 'no evidence'\"\n",
    "      }}\n",
    "    }}\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.api_client.generate_completion(prompt, max_tokens=700)\n",
    "            \n",
    "            json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "            if json_match:\n",
    "                classification_data = json.loads(json_match.group())\n",
    "                classification_data = self.validate_classification_structure(classification_data)\n",
    "                classification_data['transition_turn_id'] = transition_idx\n",
    "                return classification_data\n",
    "            else:\n",
    "                return self.create_default_classification_at_transition({'transition_turn_id': transition_idx})\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating classification for transition at turn {transition_idx}: {e}\")\n",
    "            return self.create_default_classification_at_transition({'transition_turn_id': transition_idx})\n",
    "\n",
    "    def generate_response_strategies_at_transitions(self, turns: List[Dict], transition_points: List[int],\n",
    "                                                    classifications: List[Dict], summaries: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Generate response strategies only at speaker transition points, parallelized\"\"\"\n",
    "        strategies = []\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "            future_to_idx = {executor.submit(self.generate_single_strategy, turns, transition_points[i], classifications, i, summaries): i for i in range(len(transition_points))}\n",
    "            \n",
    "            for future in concurrent.futures.as_completed(future_to_idx):\n",
    "                i = future_to_idx[future]\n",
    "                try:\n",
    "                    strategy = future.result()\n",
    "                    if strategy:\n",
    "                        strategies.append(strategy)\n",
    "                        logger.info(f\"Analyzed strategy for transition at turn {transition_points[i]}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error in parallel strategy generation for turn {transition_points[i]}: {e}\")\n",
    "                    strategies.append(self.create_default_strategy_at_transition(transition_points[i], turns[transition_points[i]-1], turns[transition_points[i]]))\n",
    "        \n",
    "        strategies.sort(key=lambda x: x['transition_turn_id'])\n",
    "        \n",
    "        return strategies\n",
    "\n",
    "    def validate_classification_structure(self, classification_data: Dict) -> Dict:\n",
    "        \"\"\"Validate and fix classification structure\"\"\"\n",
    "        if 'phq8_scores' not in classification_data:\n",
    "            classification_data['phq8_scores'] = {}\n",
    "        \n",
    "        if 'evidence_mapping' not in classification_data:\n",
    "            classification_data['evidence_mapping'] = {key: \"no evidence\" for key in self.phq8_questions.keys()}\n",
    "        \n",
    "        for phq_key in self.phq8_questions.keys():\n",
    "            if phq_key not in classification_data['phq8_scores'] or classification_data['phq8_scores'][phq_key] not in self.phq8_severity_levels:\n",
    "                classification_data['phq8_scores'][phq_key] = \"Not explored\"\n",
    "            \n",
    "            # Post-generation rule: If evidence is \"no evidence\" but severity isn't \"Not explored\", reset to \"Not explored\"\n",
    "            evidence = classification_data['evidence_mapping'].get(phq_key, \"no evidence\").strip().lower()\n",
    "            if evidence == \"no evidence\" and classification_data['phq8_scores'][phq_key] != \"Not explored\":\n",
    "                classification_data['phq8_scores'][phq_key] = \"Not explored\"\n",
    "                classification_data['evidence_mapping'][phq_key] = \"no evidence (reset due to lack of evidence)\"\n",
    "        \n",
    "        if 'depression_classification' not in classification_data or classification_data['depression_classification'] not in self.depression_classifications:\n",
    "            classification_data['depression_classification'] = \"Not depressed\"\n",
    "        \n",
    "        return classification_data\n",
    "\n",
    "    def create_default_classification_at_transition(self, summary_data: Dict) -> Dict:\n",
    "        \"\"\"Create default classification when generation fails at transition\"\"\"\n",
    "        return {\n",
    "            'transition_turn_id': summary_data['transition_turn_id'],\n",
    "            'phq8_scores': {key: \"Not explored\" for key in self.phq8_questions.keys()},\n",
    "            'depression_classification': \"Not depressed\",\n",
    "            'evidence_mapping': {key: \"no evidence\" for key in self.phq8_questions.keys()},\n",
    "            'generation_method': 'default_fallback'\n",
    "        }\n",
    "\n",
    "    def create_default_strategy_at_transition(self, transition_turn_id: int, participant_turn: Dict, \n",
    "                              therapist_turn: Dict) -> Dict:\n",
    "        \"\"\"Create default strategy when analysis fails at transition\"\"\"\n",
    "        return {\n",
    "            'transition_turn_id': transition_turn_id,\n",
    "            'participant_turn_id': transition_turn_id - 1,\n",
    "            'therapist_turn_id': transition_turn_id,\n",
    "            'strategy_used': 'Gather Context',\n",
    "            'emotion_tag': 'Neutral/Supportive',\n",
    "            'response_intent': 'Continue conversation',\n",
    "            'participant_text': participant_turn['text'],\n",
    "            'therapist_response': therapist_turn['text']\n",
    "        }\n",
    "\n",
    "    def align_with_ground_truth_flexible(self, classifications: List[Dict], final_phq_scores: Dict, \n",
    "                                       final_depression_label: str) -> List[Dict]:\n",
    "        \"\"\"Align classifications with ground truth allowing some flexibility/offset\"\"\"\n",
    "        score_to_severity = {0: \"Not at all\", 1: \"Several days\", 2: \"More than half the days\", 3: \"Nearly every day\"}\n",
    "        \n",
    "        for i, classification in enumerate(classifications):\n",
    "            is_final_transition = (i == len(classifications) - 1)\n",
    "            \n",
    "            if is_final_transition:\n",
    "                for phq_key in self.phq8_questions.keys():\n",
    "                    if phq_key in final_phq_scores:\n",
    "                        numeric_score = final_phq_scores[phq_key]\n",
    "                        if isinstance(numeric_score, (int, float)) and 0 <= numeric_score <= 3:\n",
    "                            classification['phq8_scores'][phq_key] = score_to_severity[int(numeric_score)]\n",
    "                \n",
    "                classification['depression_classification'] = final_depression_label\n",
    "                classification['ground_truth_aligned'] = True\n",
    "                classification['alignment_type'] = 'final_exact'\n",
    "            else:\n",
    "                classification['ground_truth_aligned'] = False\n",
    "                classification['alignment_type'] = 'intermediate_flexible'\n",
    "        \n",
    "        return classifications\n",
    "\n",
    "    def create_aligned_training_samples(self, turns: List[Dict], summaries: List[Dict],\n",
    "                                      classifications: List[Dict], strategies: List[Dict], \n",
    "                                      transition_points: List[int]) -> Dict:\n",
    "        \"\"\"Create properly aligned training samples following the pipeline flow\"\"\"\n",
    "        training_samples = {\n",
    "            'summary_module_samples': [],\n",
    "            'classification_module_samples': [],\n",
    "            'response_generation_samples': []\n",
    "        }\n",
    "        \n",
    "        for summary_data in summaries:\n",
    "            transition_turn_id = summary_data['transition_turn_id']\n",
    "            conversation_history = []\n",
    "            \n",
    "            max_turn_for_summary = transition_turn_id - 1  # Exclude target response\n",
    "            \n",
    "            for j in range(max_turn_for_summary + 1):\n",
    "                if j < len(turns):\n",
    "                    conversation_history.append({\n",
    "                        'speaker_role': turns[j]['speaker_role'],\n",
    "                        'text': turns[j]['text']\n",
    "                    })\n",
    "            \n",
    "            sample = {\n",
    "                'task': 'Summary',\n",
    "                'instruction': 'You are a specialized AI assistant for generating cumulative summaries in therapeutic conversations. Based on the full conversation history up to this point, create a concise summary (2-4 sentences) focusing on explicit emotional indicators, all depression symptoms discussed (with direct mapping to PHQ-8 questions if applicable), participant\\'s current state/mood based on evidence, therapist\\'s assessment approach if evident, and key quotes. Cite direct evidence without inference or external knowledge.',\n",
    "                'input': {\n",
    "                    'conversation_history': conversation_history\n",
    "                },\n",
    "                'expected_output': {\n",
    "                    'cumulative_summary': summary_data['cumulative_summary']\n",
    "                },\n",
    "                'metadata': {\n",
    "                    'transition_turn_id': transition_turn_id,\n",
    "                    'conversation_length': len(conversation_history),\n",
    "                    'context_turns_count': len(conversation_history)\n",
    "                }\n",
    "            }\n",
    "            training_samples['summary_module_samples'].append(sample)\n",
    "        \n",
    "        for i, classification_data in enumerate(classifications):\n",
    "            transition_turn_id = classification_data['transition_turn_id']\n",
    "            current_summary = next((s['cumulative_summary'] for s in summaries if s['transition_turn_id'] == transition_turn_id), \"\")\n",
    "            recent_history = []\n",
    "            max_turn_for_context = transition_turn_id - 1\n",
    "            start_idx = max(0, max_turn_for_context - 9)  # Last 10 turns\n",
    "            for j in range(start_idx, max_turn_for_context + 1):\n",
    "                if j < len(turns):\n",
    "                    recent_history.append({\n",
    "                        'speaker_role': turns[j]['speaker_role'],\n",
    "                        'text': turns[j]['text']\n",
    "                    })\n",
    "            sample = {\n",
    "                'task': 'Classification',\n",
    "                'instruction': 'You are a specialized AI assistant for PHQ-8 classification in therapeutic conversations. Using the recent conversation turns (last 10) combined with the cumulative summary (derived from full history), generate severity levels for each PHQ-8 question based only on explicit participant sentiments and evidence. Be conservative: Use \"Not explored\" if no direct evidence. Provide evidence mappings from the inputs. Output only valid JSON in this exact format: {\"phq8_scores\": {\"PHQ1\": \"value\", \"PHQ2\": \"value\", \"PHQ3\": \"value\", \"PHQ4\": \"value\", \"PHQ5\": \"value\", \"PHQ6\": \"value\", \"PHQ7\": \"value\", \"PHQ8\": \"value\"}, \"depression_classification\": \"Depressed\" or \"Not depressed\", \"evidence_mapping\": {\"PHQ1\": \"evidence or \\'no evidence\\'\", ...}}. Do not include any other text, explanations, tags, or tool calls.',\n",
    "                'input': {\n",
    "                    'recent_history': recent_history,\n",
    "                    'cumulative_summary': current_summary\n",
    "                },\n",
    "                'expected_output': {\n",
    "                    'phq8_scores': classification_data['phq8_scores'],\n",
    "                    'depression_classification': classification_data['depression_classification'],\n",
    "                    'evidence_mapping': classification_data.get('evidence_mapping', {})\n",
    "                },\n",
    "                'metadata': {\n",
    "                    'transition_turn_id': classification_data['transition_turn_id'],\n",
    "                    'ground_truth_aligned': classification_data.get('ground_truth_aligned', False),\n",
    "                    'alignment_type': classification_data.get('alignment_type', 'none')\n",
    "                }\n",
    "            }\n",
    "            training_samples['classification_module_samples'].append(sample)\n",
    "        \n",
    "        for strategy_data in strategies:\n",
    "            transition_turn_id = strategy_data['transition_turn_id']\n",
    "            \n",
    "            current_summary = next((s for s in summaries if s['transition_turn_id'] == transition_turn_id), None)\n",
    "            current_classification = next((c for c in classifications if c['transition_turn_id'] == transition_turn_id), None)\n",
    "            \n",
    "            if current_summary and current_classification:\n",
    "                recent_context = []\n",
    "                max_turn_for_context = transition_turn_id - 1\n",
    "                start_idx = max(0, max_turn_for_context - 9)  # Last 10 turns before response\n",
    "                \n",
    "                for j in range(start_idx, max_turn_for_context + 1):\n",
    "                    if j < len(turns):\n",
    "                        recent_context.append({\n",
    "                            'speaker_role': turns[j]['speaker_role'],\n",
    "                            'text': turns[j]['text']\n",
    "                        })\n",
    "                \n",
    "                sample = {\n",
    "                    'task': 'Response',\n",
    "                    'instruction': 'You are a specialized AI assistant for generating therapeutic responses. Based on the recent conversation context (last 10 turns), cumulative summary (from full history), and PHQ-8 classification results, craft a natural therapist response. Incorporate an appropriate emotion tag, strategy, and intent based on context. Output in JSON: {\"therapist_response\": \"response text\", \"emotion_tag\": \"tag\", \"strategy_used\": \"strategy\", \"response_intent\": \"1-sentence intent\"}. Ensure response is empathetic, probing if needed, and advances the assessment.',\n",
    "                    'input': {\n",
    "                        'recent_context': recent_context,\n",
    "                        'cumulative_summary': current_summary['cumulative_summary'],\n",
    "                        'classification_results': {\n",
    "                            'phq8_scores': current_classification['phq8_scores'],\n",
    "                            'depression_classification': current_classification['depression_classification']\n",
    "                        }\n",
    "                    },\n",
    "                    'expected_output': {\n",
    "                        'therapist_response': strategy_data['therapist_response'],\n",
    "                        'emotion_tag': strategy_data['emotion_tag'],\n",
    "                        'strategy_used': strategy_data['strategy_used'],\n",
    "                        'response_intent': strategy_data['response_intent']\n",
    "                    },\n",
    "                    'metadata': {\n",
    "                        'transition_turn_id': transition_turn_id,\n",
    "                        'participant_turn_id': strategy_data['participant_turn_id'],\n",
    "                        'therapist_turn_id': strategy_data['therapist_turn_id'],\n",
    "                        'response_length': len(strategy_data['therapist_response'].split())\n",
    "                    }\n",
    "                }\n",
    "                training_samples['response_generation_samples'].append(sample)\n",
    "                \n",
    "        return training_samples\n",
    "\n",
    "    def process_transcript_with_proper_alignment(self, file_path: str, final_phq_scores: Dict,\n",
    "                                               final_depression_label: str, output_file: str = None) -> Dict:\n",
    "        \"\"\"Process transcript with proper pipeline alignment using API - only at speaker transitions\"\"\"\n",
    "        logger.info(f\"Processing transcript with speaker transition alignment: {file_path}\")\n",
    "        \n",
    "        df = self.load_transcript_data(file_path)\n",
    "        turns = self.extract_conversation_turns(df)\n",
    "        logger.info(f\"Extracted {len(turns)} conversation turns\")\n",
    "        \n",
    "        transition_points = self.identify_transition_points(turns)\n",
    "        logger.info(f\"Identified {len(transition_points)} speaker transition points\")\n",
    "        \n",
    "        logger.info(\"Generating summaries at speaker transitions using API...\")\n",
    "        summaries = self.generate_summaries_at_transitions(turns, transition_points)\n",
    "        \n",
    "        logger.info(\"Generating classifications at speaker transitions using API...\")\n",
    "        classifications = self.generate_classifications_at_transitions(turns, transition_points, final_phq_scores, final_depression_label, summaries)\n",
    "        \n",
    "        logger.info(\"Analyzing response strategies at speaker transitions using API...\")\n",
    "        strategies = self.generate_response_strategies_at_transitions(turns, transition_points, classifications, summaries)\n",
    "        \n",
    "        logger.info(\"Creating aligned training samples...\")\n",
    "        training_samples = self.create_aligned_training_samples(turns, summaries, classifications, strategies, transition_points)\n",
    "        \n",
    "        training_data = {\n",
    "            'metadata': {\n",
    "                'source_file': file_path,\n",
    "                'generation_timestamp': datetime.now().isoformat(),\n",
    "                'total_turns': len(turns),\n",
    "                'transition_points': len(transition_points),\n",
    "                'final_phq_scores': final_phq_scores,\n",
    "                'final_depression_label': final_depression_label,\n",
    "                'api_provider': self.api_client.config.provider,\n",
    "                'api_model': self.api_client.config.model\n",
    "            },\n",
    "            'raw_conversation_turns': turns,\n",
    "            'speaker_transition_points': transition_points,\n",
    "            'transition_summaries': summaries,\n",
    "            'transition_classifications': classifications,\n",
    "            'transition_response_strategies': strategies,\n",
    "            'aligned_training_samples': training_samples\n",
    "        }\n",
    "        \n",
    "        if output_file:\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(training_data, f, indent=2, ensure_ascii=False)\n",
    "            logger.info(f\"Training data saved to: {output_file}\")\n",
    "        \n",
    "        return training_data\n",
    "\n",
    "    def export_training_data(self, training_data: Dict, output_dir: str = \"training_data\"):\n",
    "        \"\"\"Export training data for fine-tuning\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        training_samples = training_data.get('aligned_training_samples', {})\n",
    "        \n",
    "        for module_name, samples in training_samples.items():\n",
    "            if samples:\n",
    "                jsonl_file = os.path.join(output_dir, f\"{module_name}.jsonl\")\n",
    "                with open(jsonl_file, 'w', encoding='utf-8') as f:\n",
    "                    for sample in samples:\n",
    "                        conversation = {\n",
    "                            \"messages\": [\n",
    "                                {\n",
    "                                    \"role\": \"system\",\n",
    "                                    \"content\": f\"You are a specialized AI assistant for {sample['task']}. {sample['instruction']}\"\n",
    "                                },\n",
    "                                {\n",
    "                                    \"role\": \"user\", \n",
    "                                    \"content\": json.dumps(sample['input'], ensure_ascii=False)\n",
    "                                },\n",
    "                                {\n",
    "                                    \"role\": \"assistant\",\n",
    "                                    \"content\": json.dumps(sample['expected_output'], ensure_ascii=False)\n",
    "                                }\n",
    "                            ],\n",
    "                            \"metadata\": sample.get('metadata', {})\n",
    "                        }\n",
    "                        f.write(json.dumps(conversation, ensure_ascii=False) + \"\\n\")\n",
    "                logger.info(f\"Exported {len(samples)} samples to {jsonl_file}\")\n",
    "\n",
    "    def test_csv_format_detection(self, file_path: str):\n",
    "        \"\"\"Test CSV format detection and parsing with speaker-based turn grouping\"\"\"\n",
    "        logger.info(\"Testing CSV format detection, parsing, and speaker transition identification...\")\n",
    "        \n",
    "        df = self.load_transcript_data(file_path)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"CSV FORMAT DETECTION AND PARSING TEST\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Total utterances parsed: {len(df)}\")\n",
    "        print(f\"Speakers found: {df['speaker'].unique()}\")\n",
    "        print(f\"Timestamp range: {df['timestamp'].min():.3f} - {df['timestamp'].max():.3f}\")\n",
    "        \n",
    "        print(\"\\nFirst 10 parsed utterances:\")\n",
    "        for idx, row in df.head(10).iterrows():\n",
    "            print(f\"Utterance {idx}: [{row['timestamp']:.3f}] {row['speaker']}: {row['text'][:60]}...\")\n",
    "        \n",
    "        grouped_df = self.group_by_speaker_turns(df)\n",
    "        \n",
    "        if not grouped_df.empty:\n",
    "            turns = self.extract_conversation_turns(df)\n",
    "            transition_points = self.identify_transition_points(turns)\n",
    "            \n",
    "            print(f\"\\nGrouped {len(df)} utterances into {len(grouped_df)} speaker turns\")\n",
    "            print(f\"Identified {len(transition_points)} speaker transition points\")\n",
    "            print(f\"Transition points: {transition_points}\")\n",
    "            \n",
    "            print(f\"\\nValidation: Ready for training - {'âœ“ YES' if len(transition_points) > 0 else 'âœ— NO'}\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def test_api_connection(self):\n",
    "        \"\"\"Test API connection and basic functionality\"\"\"\n",
    "        test_prompt = \"Say 'test successful' and nothing else.\"\n",
    "        \n",
    "        try:\n",
    "            response = self.api_client.generate_completion(test_prompt, max_tokens=20)\n",
    "            logger.info(f\"API Test Response: {response}\")\n",
    "            # More lenient check - just verify we got a non-empty response\n",
    "            if response and len(response.strip()) > 0:\n",
    "                logger.info(\"API connection validated successfully\")\n",
    "                return True\n",
    "            else:\n",
    "                logger.warning(\"API responded with empty content\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"API Test Failed: {e}\")\n",
    "            return False\n",
    "\n",
    "\n",
    "def process_single_transcript(generator, file_name, transcripts_folder, phq_df, checkpoint_dir):\n",
    "    \"\"\"Helper function to process a single transcript with checkpointing\"\"\"\n",
    "    if not file_name.lower().endswith(\".csv\"):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        participant_id = int(file_name.split(\"_\")[0])\n",
    "    except ValueError:\n",
    "        print(f\"âš  Skipping file with unexpected name format: {file_name}\")\n",
    "        return None\n",
    "    \n",
    "    phq_row = phq_df[phq_df[\"Participant_ID\"] == participant_id]\n",
    "    if phq_row.empty:\n",
    "        print(f\"âš  No PHQ data found for participant {participant_id}, skipping...\")\n",
    "        return None\n",
    "    \n",
    "    final_phq_scores = phq_row.iloc[0].drop(\n",
    "        labels=[\"Participant_ID\", \"PHQ8_Binary\", \"PHQ8_Score\", \"Gender\"]\n",
    "    ).to_dict()\n",
    "    \n",
    "    binary_label = phq_row.iloc[0][\"PHQ8_Binary\"]\n",
    "    final_depression_label = \"Depressed\" if binary_label == 1 else \"Not depressed\"\n",
    "    \n",
    "    file_path = os.path.join(transcripts_folder, file_name)\n",
    "    output_file = f'simplified_training_data_{participant_id}.json'\n",
    "    output_dir = f\"simplified_training_data_{participant_id}\"\n",
    "    checkpoint_file = os.path.join(checkpoint_dir, f\"checkpoint_{participant_id}.pkl\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"PROCESSING TRANSCRIPT: {file_name}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        if os.path.exists(checkpoint_file):\n",
    "            with open(checkpoint_file, 'rb') as f:\n",
    "                checkpoint_data = pickle.load(f)\n",
    "            df = checkpoint_data.get('df')\n",
    "            training_data = checkpoint_data.get('training_data')\n",
    "            if df is not None and training_data is not None:\n",
    "                print(f\"âœ… Loaded checkpoint for participant {participant_id}\")\n",
    "            else:\n",
    "                df = None\n",
    "                training_data = None\n",
    "        else:\n",
    "            df = None\n",
    "            training_data = None\n",
    "        \n",
    "        if df is None:\n",
    "            df = generator.test_csv_format_detection(file_path)\n",
    "        \n",
    "        if training_data is None:\n",
    "            training_data = generator.process_transcript_with_proper_alignment(\n",
    "                file_path=file_path,\n",
    "                final_phq_scores=final_phq_scores,\n",
    "                final_depression_label=final_depression_label,\n",
    "                output_file=output_file\n",
    "            )\n",
    "        \n",
    "        with open(checkpoint_file, 'wb') as f:\n",
    "            pickle.dump({'df': df, 'training_data': training_data}, f)\n",
    "        \n",
    "        generator.export_training_data(\n",
    "            training_data=training_data,\n",
    "            output_dir=output_dir\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Processed participant {participant_id}\")\n",
    "        print(f\"Final depression label: {final_depression_label}\")\n",
    "        print(f\"Total transitions: {len(training_data['speaker_transition_points'])}\")\n",
    "        \n",
    "        return training_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_name}: {e}\")\n",
    "        print(f\"âŒ Error processing {file_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def main_api():\n",
    "    \"\"\"Batch process transcripts from a folder, matching with PHQ scores, parallelized with checkpoints.\"\"\"\n",
    "    \n",
    "    deepseek_config = APIConfig(\n",
    "        provider=\"deepseek\",\n",
    "        api_key=\"sk-\",\n",
    "        base_url=\"https://api.deepseek.com\",\n",
    "        model=\"deepseek-chat\",  # Your specified model\n",
    "        max_tokens=1000,\n",
    "        temperature=0.2,\n",
    "        rate_limit_delay=0.05\n",
    "    )\n",
    "    \n",
    "    generator = ImprovedTrainingDataGenerator(deepseek_config)\n",
    "    \n",
    "    print(\"Testing API connection...\")\n",
    "    if not generator.test_api_connection():\n",
    "        print(\"âŒ API connection failed. Please check your configuration.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"âœ… API connection successful!\")\n",
    "\n",
    "    transcripts_folder = \"Extracted_Text_Transcript_DAIC\"\n",
    "    phq_csv_path = \"train_split_Depression_AVEC2017.csv\"\n",
    "    checkpoint_dir = \"checkpoints-v2\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    phq_df = pd.read_csv(phq_csv_path)\n",
    "    \n",
    "    file_names = [f for f in os.listdir(transcripts_folder) if f.lower().endswith(\".csv\")]\n",
    "    \n",
    "    results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=150) as executor:  # Capped for rate limits\n",
    "        future_to_file = {executor.submit(process_single_transcript, generator, file_name, transcripts_folder, phq_df, checkpoint_dir): file_name for file_name in file_names}\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(future_to_file):\n",
    "            file_name = future_to_file[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    results.append(result)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in parallel processing for {file_name}: {e}\")\n",
    "    \n",
    "    print(\"\\nðŸŽ¯ All transcripts processed.\")\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    training_datas = main_api()\n",
    "    \n",
    "    if training_datas:\n",
    "        for training_data in training_datas:\n",
    "            print(\"\\nSuccessfully generated simplified training data!\")\n",
    "            print(f\"Total turns processed: {training_data['metadata']['total_turns']}\")\n",
    "            print(f\"Total transition points: {len(training_data['speaker_transition_points'])}\")\n",
    "            print(f\"Final depression classification: {training_data['metadata']['final_depression_label']}\")\n",
    "    else:\n",
    "        print(\"\\nProcessing failed. Check logs for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b6dbd3-8c3e-46dc-9143-21a51d12517b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
